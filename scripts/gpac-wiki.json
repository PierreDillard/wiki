[{"date_scraped_timestamp":1720188009730,"host":"wiki.gpac.io","page_title":"360 Video tiled streaming - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC supports streaming HEVC tiled DASH videos. In this page, you will find some helpful information to get started with this feature. In the following, we assume the input video has resolution of 3840x2160 and a frame rate of 30 frames/sec.\nHow to generate tiled streamable video content¶\nThe open-source \"Kvazaar encoder\":https://github.com/ultravideo/kvazaar allows encoding HEVC with motion constrained tiling. To instruct the encoder to encapsulate each tile in a separate slice, the --slices option is used with the value tiles. Motion vectors are then constrained within in each tile using --mv-constraint.\nkvazaar -i input.yuv --input-res 3840x2160 -o output.hvc --tiles 3x3 --slices tiles --mv-constraint frametilemargin --bitrate 128000 --period 30 --input-fps 30\nWe can also set the target bitrate by --bitrate in bps instead of setting QP. It will turn the rate controller on. If you plan on merging tiles with different quality, keep the same QP settings and adjust the bitrate (changing QP would generate incompatible bitstreams). The output of the encoder is a raw bitstream and should be packaged within a container. To do so, MP4Box from GPAC can be used as follows, where the set value for -fps should match that of the encoded video.\nMP4Box -add video_tiled.hvc:split_tiles -fps 30 -new video_tiled.mp4\nThe generated MP4 file includes one base track containing parameter sets and/or SEI messages plus one track for each tile. Now you can generate DASH segments and descriptor for the packaged video as follows.\nMP4Box -dash 1000 -rap -frag-rap -profile live -out dash_tiled.mpd video_tiled.mp4\nMP4Box can also generate an MPD with multiple representations by adding more input files.\nMP4Box -dash 1000 -rap -frag-rap -profile live -out dash_tiled.mpd video_tiled_rep1.mp4 video_tiled_rep2.mp4\nFor more information, please refer this article.\nHow to stream 360 video content¶\nOnce the content is generated and stored on the server side, you can play it from the client side. The only required argument is the URL of the MPD file. In the case of 360/VR videos, it is possible to render only the viewport using #VR right after the URL. For example:\ngpac -gui http://server:port/test.mpd\ngpac -gui http://server:port/test.mpd#VR\nNote that 360 rendering requires usage of the compositor filter (so use gpac -gui or gpac -mp4c).\nCoding Aspects¶\nMain Rendering Loop in Compositor Filter¶\nThe main loop to render the 360 video content (in #VR mode) is roughly as follows.\ngf_sc_draw_frame\n gf_sc_render_frame\n gf_sc_draw_scene\n visual_draw_frame\n visual_3d_draw_frame\n visual_3d_setup\n visual_3d_setup_traversing_state\n visual_3d_setup_clipper\n visual_3d_init_shaders\n for each view\n visual_3d_draw_node // for root\n for each scenegraph\n gf_sc_traverse_subscene\nSince HEVC supports tiling, putting the tiles back to form the whole frame is done inside the decoder. Therefore, the output of the decoder is the projected 2D video (e.g. equirectangular). The decoder puts each decoded frame as a composition unit (CU) inside the composition buffer (CB). The compositor takes out CUs one at a time (by calling gf_sc_texture_update_frame() function) and tries to push the texture to the graphics device (by calling gf_sc_texture_push_image() function).\nThe conversion from YUV (YU12) to RGB is done with the help of a fragment shader. visual_3d_shader_with_flags() function is used to load the shaders.\nThe video output interface (DirectX, SDL, or Raw) is loaded from a module in gf_sc_create() function and signaled to setup in gf_sc_reconfig_task() function.\nScene Graph in #VR Mode¶\nTAG_MPEG4_OrderedGroup\n |\n |--- TAG_MPEG4_Background2D\n |--- TAG_MPEG4_Viewpoint\n |--- TAG_MPEG4_NavigationInfo\n |--- TAG_MPEG4_Sound2D\n | |\n | |--- Source: TAG_MPEG4_AudioClip\n |\n |--- TAG_MPEG4_Transform2D\n |\n |--- TAG_MPEG4_TouchSensor\n |--- TAG_MPEG4_Transform2D\n |\n |--- TAG_MPEG4_Shape\n |\n |--- Appearance: TAG_MPEG4_Appearance\n | |\n | |--- Texture: TAG_MPEG4_MovieTexture\n |\n |--- Geometry: TAG_MPEG4_Sphere\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/Tiled-Streaming/"},{"date_scraped_timestamp":1720187957659,"host":"wiki.gpac.io","page_title":"Android - GPAC wiki","text":"\n \n \n \n \n \n \nThe Android build is a bit complicated. The method described here is the one used for the official Android builds. It is fairly rigid, with some hard-coded paths and versions. It should not be too hard to adapt it to one's own set up, but there is no guarantee that it will work as is on (for example) more recent versions of the ndk/sdk. \nIt was tested on Ubuntu 14 to 18. \nThe process has three main steps: set up the build environment, cross-compile the dependencies, build the GPAC apk. \nIn the following, we'll call the main working directory <GPAC_ROOT_DIR>.\nJVM and tools¶\nWe first install the JAVA runtime and development environment, and some useful libs for later: \nsudo apt-get install openjdk-8-jdk openjdk-8-jre-headless ant libncurses5 libc6:i386 libncurses5:i386 libstdc++6:i386 lib32z1 libbz2-1.0:i386 autoconf automake libtool \nAndroid SDK¶\nWe start by downloading the latest Android Tools from https://developer.android.com/studio#downloads. \nWe then use the sdkmanager included to install the right version of the SDK. \nThen we create a symbolic link to /opt/android-sdk for ease of use later. \n<GPAC_ROOT_DIR>$ mkdir sdk\n<GPAC_ROOT_DIR>$ cd sdk\n<GPAC_ROOT_DIR>/sdk$ wget https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip\n<GPAC_ROOT_DIR>/sdk$ unzip sdk-tools-linux-4333796.zip\n<GPAC_ROOT_DIR>/sdk$ cd tools\n<GPAC_ROOT_DIR>/sdk/tools$ bin/sdkmanager \"platforms;android-23\"\n<GPAC_ROOT_DIR>/sdk/tools$ bin/sdkmanager \"build-tools;23.0.3\"\n<GPAC_ROOT_DIR>/sdk/tools$ bin/sdkmanager \"extras;android;m2repository\"\n<GPAC_ROOT_DIR>/sdk/tools$ cd ../..\n<GPAC_ROOT_DIR>$ sudo ln -sf $(readlink -f ./sdk) /opt/android-sdk\nAndroid NDK¶\nThe version here is important. \n<GPAC_ROOT_DIR>$ mkdir ndk\n<GPAC_ROOT_DIR>$ cd ndk\n<GPAC_ROOT_DIR>/ndk$ wget https://dl.google.com/android/repository/android-ndk-r22b-linux-x86_64.zip\n<GPAC_ROOT_DIR>/ndk$ unzip android-ndk-r22b-linux-x86_64.zip\n<GPAC_ROOT_DIR>/ndk$ sudo ln -sf $(readlink -f ./android-ndk-r22b) /opt/android-ndk\nWe should now have something like this in /opt:\nadavid@ubuntu1810:~/gpac-android$ ll /opt/\ntotal 8\nlrwxrwxrwx 1 root root 41 avril 1 16:14 android-ndk -> /home/adavid/gpac-android/ndk/android-ndk-r13b/\nlrwxrwxrwx 1 root root 24 avril 1 16:15 android-sdk -> /home/adavid/gpac-android/sdk/\nBuild the dependencies¶\nGet the source¶\n###get src\n<GPAC_ROOT_DIR>$ git clone https://github.com/gpac/gpac.git gpac_public\n###get deps\n<GPAC_ROOT_DIR>$ git clone https://github.com/gpac/deps_android\n<GPAC_ROOT_DIR>$ cd deps_android\n<GPAC_ROOT_DIR>/deps_android$ git submodule update --init --recursive --force --checkout\nNote that keeping the name gpac_public for the main source repository is important for scripts to run smoothly. \nBuild deps¶\n<GPAC_ROOT_DIR>/deps_android$ cd build/android/\n<GPAC_ROOT_DIR>/deps_android/build/android$ ./gpac_build_all_extra_libs /opt/android-ndk\nCopy deps¶\n<GPAC_ROOT_DIR>/deps_android$ ./CopyLibs2Public4Android.sh\nIf all went well up to this point, you should see the extra_lib directory of the main repository looking like this: \nadavid@ubuntu1810:~/gpac-android/deps_android$ ll ../gpac_public/extra_lib/lib/android/*\n../gpac_public/extra_lib/lib/android/armeabi:\ntotal 13456\n-rwxr-xr-x 1 adavid adavid 7585444 avril 2 14:12 libavcodec.so*\n-rwxr-xr-x 1 adavid adavid 51108 avril 2 14:12 libavdevice.so*\n-rwxr-xr-x 1 adavid adavid 1504740 avril 2 14:12 libavfilter.so*\n-rwxr-xr-x 1 adavid adavid 1666644 avril 2 14:12 libavformat.so*\n-rwxr-xr-x 1 adavid adavid 255452 avril 2 14:12 libavutil.so*\n-rwxr-xr-x 1 adavid adavid 30544 avril 2 14:12 libeditline.so*\n-rwxr-xr-x 1 adavid adavid 254480 avril 2 14:12 libfaad.so*\n-rwxr-xr-x 1 adavid adavid 349692 avril 2 14:12 libft2.so*\n-rwxr-xr-x 1 adavid adavid 165372 avril 2 14:12 libjpegdroid.so*\n-rwxr-xr-x 1 adavid adavid 589184 avril 2 14:12 libjs_osmo.so*\n-rwxr-xr-x 1 adavid adavid 103932 avril 2 14:12 libmad.so*\n-rwxr-xr-x 1 adavid adavid 105672 avril 2 14:12 libopenjpeg.so*\n-rwxr-xr-x 1 adavid adavid 144944 avril 2 14:12 libpng.so*\n-rwxr-xr-x 1 adavid adavid 444584 avril 2 14:12 libstlport_shared.so*\n-rwxr-xr-x 1 adavid adavid 75148 avril 2 14:12 libswresample.so*\n-rwxr-xr-x 1 adavid adavid 320908 avril 2 14:12 libswscale.so*\n-rwxr-xr-x 1 adavid adavid 87624 avril 2 14:12 libz.so*\n../gpac_public/extra_lib/lib/android/armeabi-v7a:\ntotal 17928\n-rwxr-xr-x 1 adavid adavid 12362152 avril 2 14:12 libavcodec.so*\n-rwxr-xr-x 1 adavid adavid 51108 avril 2 14:12 libavdevice.so*\n-rwxr-xr-x 1 adavid adavid 1504740 avril 2 14:12 libavfilter.so*\n-rwxr-xr-x 1 adavid adavid 1666644 avril 2 14:12 libavformat.so*\n-rwxr-xr-x 1 adavid adavid 255452 avril 2 14:12 libavutil.so*\n-rwxr-xr-x 1 adavid adavid 30552 avril 2 14:12 libeditline.so*\n-rwxr-xr-x 1 adavid adavid 221720 avril 2 14:12 libfaad.so*\n-rwxr-xr-x 1 adavid adavid 345604 avril 2 14:12 libft2.so*\n-rwxr-xr-x 1 adavid adavid 157188 avril 2 14:12 libjpegdroid.so*\n-rwxr-xr-x 1 adavid adavid 523656 avril 2 14:12 libjs_osmo.so*\n-rwxr-xr-x 1 adavid adavid 99844 avril 2 14:12 libmad.so*\n-rwxr-xr-x 1 adavid adavid 89296 avril 2 14:12 libopenjpeg.so*\n-rwxr-xr-x 1 adavid adavid 120376 avril 2 14:12 libpng.so*\n-rwxr-xr-x 1 adavid adavid 399384 avril 2 14:12 libstlport_shared.so*\n-rwxr-xr-x 1 adavid adavid 75148 avril 2 14:12 libswresample.so*\n-rwxr-xr-x 1 adavid adavid 320908 avril 2 14:12 libswscale.so*\n-rwxr-xr-x 1 adavid adavid 87632 avril 2 14:12 libz.so*\n../gpac_public/extra_lib/lib/android/x86:\ntotal 24640\n-rwxr-xr-x 1 adavid adavid 17571708 avril 2 14:12 libavcodec.so*\n-rwxr-xr-x 1 adavid adavid 51200 avril 2 14:12 libavdevice.so*\n-rwxr-xr-x 1 adavid adavid 1829656 avril 2 14:12 libavfilter.so*\n-rwxr-xr-x 1 adavid adavid 2176896 avril 2 14:12 libavformat.so*\n-rwxr-xr-x 1 adavid adavid 292208 avril 2 14:12 libavutil.so*\n-rwxr-xr-x 1 adavid adavid 30444 avril 2 14:12 libeditline.so*\n-rwxr-xr-x 1 adavid adavid 299436 avril 2 14:12 libfaad.so*\n-rwxr-xr-x 1 adavid adavid 374168 avril 2 14:12 libft2.so*\n-rwxr-xr-x 1 adavid adavid 169368 avril 2 14:12 libjpegdroid.so*\n-rwxr-xr-x 1 adavid adavid 900380 avril 2 14:12 libjs_osmo.so*\n-rwxr-xr-x 1 adavid adavid 99736 avril 2 14:12 libmad.so*\n-rwxr-xr-x 1 adavid adavid 130148 avril 2 14:12 libopenjpeg.so*\n-rwxr-xr-x 1 adavid adavid 169420 avril 2 14:12 libpng.so*\n-rwxr-xr-x 1 adavid adavid 558296 avril 2 14:12 libstlport_shared.so*\n-rwxr-xr-x 1 adavid adavid 83236 avril 2 14:12 libswresample.so*\n-rwxr-xr-x 1 adavid adavid 365860 avril 2 14:12 libswscale.so*\n-rwxr-xr-x 1 adavid adavid 83412 avril 2 14:12 libz.so*\n(contents and versions may differ depending on updates and deprecated features)\nBuild GPAC¶\nWe are now ready to build GPAC proper. \n<GPAC_ROOT_DIR>/deps_android$ cd ../gpac_public/build/android/jni\n<GPAC_ROOT_DIR>/gpac_public/build/android/jni$ ./gpac_build_android -ndk=/opt/android-ndk -sdk=/opt/android-sdk -jdk=/usr/lib/jvm/java-8-openjdk-amd64 -force_rebuild\nIf it succeeds, you should see the apk in <GPAC_ROOT_DIR>/gpac_public:\nadavid@ubuntu1810:~/gpac-android/gpac_public$ ll *.apk\n-rw-rw-r-- 1 adavid adavid 30745262 avril 2 14:53 osmo4-0.7.2-DEV-rev1048-g38ab344d4-master.apk\nYou can now install the apk on your device!\nReporting issues¶\nAs mentioned in the introduction, chances are there will be some problems along the way. You can open issues about Android compilation in the issues tracker.\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/build/GPAC-Build-Guide-for-Android/"},{"date_scraped_timestamp":1720187964606,"host":"wiki.gpac.io","page_title":"BIFS Textual Format - GPAC wiki","text":"\n \n \n \n \n \n \nBT Format¶\nBT stands for BIFS Text and is an exact textual representation of the MPEG-4 BIFS scene. Its syntax is the same as the VRML/X3D (.wrl and .x3dv files) ones for the scene description part, and it has been extended for other MPEG-4 tools (OD, OCI, IPMP).\nYou will find plenty of example BT files in the tutorial and regression test suite, and we strongly recommend using these to get more familiar with the BT syntax.\nThe BT language has been originally developed at ENST as the textual format of the late MP4Tool. The format is still enhanced at ENST for GPAC needs, but we try not to mess up too much with it to keep VRML compatibility.\nThere are 3 major parts in a BT file:\nThe root scene, made of a collection of PROTO nodes if desired, a single top level node and a list of routes for interaction if needed. This is the part common to BT and VRML formats.\nThe InitialObjectDescriptor, describing the streams that must be opened when opening the MPEG-4 scene. It usually contains a BIFS stream description and an OD stream description when visual/audio media are present in the scene.\nA succession of modification to the scene and their associated timing.\nMP4Box specific BT syntax¶\nThe syntax of BIFS/OD commands in BT has been enhanced to enable animation stream and scalable description encodings. The new elements are placed at the access unit declaration 'AT TIMING' element:\n'RAP' element: specifies the following access unit is a random access points. This is needed because MP4Box cannot currently compute the scene random access state per stream. Note however that the first access unit of any systems stream is considered as a random access point\n'IN' element: specifies the following access unit happens in the stream of given ID.\nFor example, RAP AT 1000 IN 20 { ... } means that the access unit is a random access point, its timing is 1000 (in stream timescale) and it happens in stream whose ES_ID is 20.\nThe syntax of the 'AT TIMING' element has been extended to support differential timing to enable authors to specify relative time between commands rather than absolute timing. The differential timing is signaled by using a capital D before the timing itself. For example, AT D2000 { ... } means that this access unit occurs 2000 time ticks after the preceding access unit.\nImporting media with MP4Box and BT¶\nMP4Box uses a specific descriptor for stream importing called MuxInfo. This descriptor is not normative (although used by MP4Tool and all tools from MPEG-4 Systems Reference Software). It is never encoded in the file. The modifications made to this descriptor are backward compatible with other tools using this descriptor. \nThe current syntax is:\nMuxInfo {\nfileName\nstreamFormat\nGroupID\nstartTime\nduration\nframeRate\nuseDataReference\nnoFrameDrop\nSBR_Type\ncompactSize\ntextNode\nfontNode\n}\nSemantics :¶\nfileName : specifies location of stream to import. Optional for BIFS/OD streams, required for others. Supported formats are the same as those supported by MP4Box:\nAVI, MPEG: syntax src_filename if only one video in movie, otherwise src_filename#audio, src_filename#video.\nMP3, AAC-ADTS, JPG, PNG, SRT, SUB: syntax src_filename.\n[[NHNT|NHNT Format]]: syntax src_filename where fileName is either the \".nhnt\" or the \".media\" file of the NHNT source.\n[[NHML|NHML Format]]: fileName must be the NHML source file.\nIsoMedia (MP4/3GP): syntax src_filename if only one track in file, src_filename#trackID where TrackID is the ID of the track to import from src_filename.\nstreamFormat : optional, one of \"AVI\", \"MP3\", \"JPEG\", \"PNG\", \"MP4\", \"NHNT\", \"SRT\" depending on input type. This should not be needed as MP4Box uses the file extension to figure out the media format, it is kept for old BT compatibility.\nGroupID : optional integer, specifies the multiplexing order in the final mp4. Media tracks are interleaved by groups, the group with the lowest ID being written first to disk. Using groups may greatly improve http streaming of the file.\nstartTime : optional integer, specifies the DTS/CTS of the first sample - by default the first sample imported has CTS/DTS 0. Expressed in milliseconds.\nduration : optional integer, specifies run-time of media data (from source start) to import. Expressed in milliseconds.\nNote : When creating OCR tracks in MP4, you must use the muxInfo.duration to specify the desired duration of the OCR track, otherwise the OCR track won't have any associated duration and will never stop (looping not possible).\nuseDataReference : optional boolean, specifies that media data shall not be copied in the final MP4 but only referenced. This may not be supported depending on input data framing. cf '-dref' option in MP4Box.\nframeRate : overrides the source media framerate when possible (same as MP4Box -fps).\nnoFrameDrop : optional boolean for AVI import only, specifies that video shall be imported at constant frame rate, e.g. non coded frames in AVI file shall be kept in MP4. cf '-nodrop' option in MP4Box.\nSBR_Type : optional string for AAC-SBR import only, either \"implicit\" or \"explicit\" - cf MP4Box -sbr and -sbrx options.\ncompactSize : optional string, either \"TRUE\" or \"FALSE\", indicating if sample sizes should be stored in a compact way or not.\ntextNode, fontNode : required string identifier for SRT importing, ignored for other streams. SRT importing is done by creating a BIFS animation stream carrying the text and fonts modifications (sample available in regression test suite).\nThe textNode shall be the DEF identifier of the text node to modify (it can be a proto, but then it MUST have an MFString field named \"string\")\nThe fontNode may be ignored. If set, it shall be the DEF identifier of the font node to modify (it can be a proto, but then it MUST have an SFString field named \"style\")\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/scenecoding/MPEG-4-BIFS-Textual-Format/"},{"date_scraped_timestamp":1720188026087,"host":"wiki.gpac.io","page_title":"Custom DASH Algorithms - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to implement your custom DASH rate adaptation logic in JS.\nA custom logic can be defined using a standalone script, or by attaching a JS object to the dashin filter in a JS session.\nIt is recommended to read the associated documentation.\nStandalone script¶\nThe standalone mode works by specifying a JS file to the dashin filter using its -algo option:\ngpac -i source.mpd --algo=mydash.js ...\nThe JS script is loaded with a global object called dashin with 4 callback functions:\nperiod_reset: indicates start or end of a period (optional)\nnew_group: indicates setup of a new adaptation group (optional), i.e. DASH AdaptationSet or HLS Variant Stream. For HEVC tiling, each tile will be declared as a group, as well as the base tile track\nrate_adaptation: performs rate adaptation for a group (mandatory). Return value is the new quality index, or -1 to keep as current, -2 to discard (debug, segments won't be fetched/decoded)\ndownload_monitor: performs download monitoring, potentially resulting in download abort request (optional)\nThe following is a basic JS example performing dummy adaptation (always play lowest quality) :\nimport { Sys as sys } from 'gpaccore'\nprint(\"Hello DASH custom algo !\");\nlet groups = [];\ndashin.period_reset = function (reset_type)\n{\n print(\"Period reset type \" + reset_type);\n if (!reset_type)\n groups = [];\n}\ndashin.new_group = function (group)\n{\n print(\"New group: \" + JSON.stringify(group));\n //remember the group (adaptation set in dash)\n groups.push(group);\n}\ndashin.rate_adaptation = function (group_idx, base_group_idx, force_lower_complexity, stats)\n{\n print(`Getting called in custom algo ! group ${group_idx} base_group ${base_group_idx} force_lower_complexity ${force_lower_complexity}`);\n print('Stats: ' + JSON.stringify(stats));\n //always use lowest quality\n //you could check group.SRD to perform spatial adaptation\n return 0;\n}\ndashin.download_monitor = function (group_idx, stats)\n{\n print(\"Download info \" + JSON.stringify(stats));\n return -1;\n}\nAttaching algo from a JS session¶\nThe first step in your JS is to create a DASH object implementing the callbacks previously indicated:\n//custom rate adaptation object\nlet dashalgo = {};\ndashalgo.groups = [];\ndashalgo.rate_adaptation = function (group, base_group, force_lower_complexity, stats)\n{\n return 0;\n}\ndashalgo.new_group = function (group)\n{\n this.groups.push(group);\n}\ndashalgo.period_reset = function (type)\n{\n if (!type)\n this.groups = [];\n}\nYou will then need to setup a JS session monitoring filter creation process:\nsession.set_new_filter_fun( (f) => {\n print(\"new filter \" + f.name);\n //bind our custom rate adaptation logic\n if (f.name == \"dashin\") {\n f.bind(dashalgo);\n }\n} ); \nAnd you're good to try new rate adaptation algorithm !\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/jsdash/"},{"date_scraped_timestamp":1720188073970,"host":"wiki.gpac.io","page_title":"DASH Low Latency - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nPlease make sure you are familiar with DASH terminology before reading. \nIn this howto, we will study various setups for DASH live streaming in low latency mode, using both MP4Box and gpac. This approach follows standards. It is compatible with any low-latency aware DASH implementation such as dash.js\nAs a reminder, you can always get more info on option `foo in GPAC by using:\ngpac -h foo\nMP4Box -h foo\nYou can also check the filter graph used by your session:\ngpac -graph REST_OF_COMMAND\nMP4Box -fgraph REST_OF_COMMAND \nAnd when using gpac, you can enable real-time reporting of filters activities using gpac -r.\nThe gpac application can be used for dashing whenever MP4Box is used, but the opposite is not true. Especially MP4Box cannot:\nuse complex custom filter chains while dashing, such as transcoding in several qualities\nproduce two DASH sessions at the same time\nThe basic syntax for MP4Box dashing is:\nMP4Box -dash SEGMENT_DUR -profile live -out file.mpd source1 ... sourceN\nThe equivalent syntax for gpac dashing is:\ngpac -i source1 ... -i sourceN -o file.mpd:profile=live:segdur=2\nMP4Box can accept filter options in each of the source URLs as well as in the destination URL.\nMP4Box -dash SEGMENT_DUR -profile live -out file.mpd:OPT1=VAL1[...] source1:OPT2=VAL2 ... sourceN:OPTN=VALN\nThe equivalent syntax for gpac dashing is:\ngpac -i source1:OPT2=VAL2 ... -i sourceN:OPTN=VALN -o file.mpd:profile=live:segdur=2:OPT1=VAL1\nOptions specified on a source or destination filter are inherited by any filter needed to link the source to the destination, as explained here. When dashing, options specified on the destination manifest (file.mpd) are also specified on the target segmented outputs.\nMP4Box can accept filter chains for each source URLs.\nMP4Box -dash SEGMENT_DUR -profile live -out file.mpd source1:@@cecrypt:cfile=drm1.xml ... sourceN:@@cecrypt:cfile=drmN.xml\nThe equivalent syntax for gpac dashing requires setting up filter IDs (FID) and source filtering (SID) on your filters as explained in the encryption howto:\ngpac -i source1:FID=S1 cecrypt:cfile=drm1.xml:FID=C1:SID=S1 ... -i sourceN:FID=SN cecrypt:cfile=drm1.xml:FID=CN:SID=SN -o file.mpd:SID=C1,CN:profile=live:segdur=2\nNote\nMP4Box dashing is a simple wrapper around a filter session running the dasher filter, with all MP4Box DASH options remapped to the filter. For backward compatibility reasons, the old options of GPAC 0.8 or below were kept, but the syntax can also be migrated to GPAC filters:\nMP4Box -dash 1000 -ast-offset 9000 -profile live [...]\nis equivalent to\nMP4Box --segdur=1000 --asto=9000 --profile=live [...]\nDASH Low Latency setup¶\nIn DASH low latency, you must decide the duration of your segment (produced file) and the duration of your fragment or CMAF chunk (sub-part of the produced segment).\nIn GPAC, the dashing process only cares about segments and is unaware of the fragmentation of the segment, as this is a property of the underlying multiplexing format. For example, low latency DASH using MPEG-2 TS mux or raw elementary stream do not need to worry about fragment duration since packets of the mux are generated as soon as input packets are received.\nFor ISOBMFF (fmp4), each segment can be divided in one or more fragments. This is described by the cdur option, which in case of DASH will be set by default to the segment duration (hence one fragment per DASH segment).\nMP4Box -dash 10000 -frag 1000 -profile live -out DST_URL source1 ... source2\ngpac -i source1 -i source2 -o DST_URL:segdur=10:cdur=1:profile=live\nThis will DASH your content trying to use 10s segments, each made of 10 fragments of 1s. \nNote \nThe fragments duration will vary depending on the duration of input frames, but in any case a new fragment will be created as soon as the current fragment duration is equal to or greater than the desired duration.\nYou can now do a live (dynamic) DASH session:\nMP4Box -dash-live 10000 -subdur 10000 -frag 1000 -profile live -out DST_URL source1 ... source2\nThis will generate a live session, dashing 10s (-subdur) of content into 10s (-dash-live) segments, each containing 10 fragments of 1s (-frag). If your sources are live, you don't need to specify the -subduroption since real-time regulation of source is not needed. \nor\ngpac -i source1 -i source2 -o DST_URL:segdur=10:cdur=1:profile=live:dmode=dynamic:sreg\nThis will generate a live session, dashing content into 10s (segdur) segments, each containing 10 fragments of 1s (cdur), performing real-time regulation after each segment generation (sreg). If your sources are live, you don't need to specify the sregoption since real-time regulation of source is not needed. \nThe above commands do not perform fragment regulation, which means that the content of each segment will be written at once by the file multiplexer (if your sources are live, this will not be the case and you can skip this next step) : \nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live -out res/live.mpd source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o res/live.mpd:segdur=10:cdur=1:profile=live:dmode=dynamic\nThe -frag-rt simply injects a reframer in the filter graph performing real-time regulation on media frames. This will ensure that both segments and fragments are written to disk in real-time.\nIf you dash a live TS source for example, the commands will be, without regulation:\nMP4Box -dash-live 10000 -frag 1000 -profile live -out res/live.mpd udp://IP:PORT\ngpac -i udp://IP:PORT -o res/live.mpd:segdur=10:cdur=1:profile=live:dmode=dynamic\nYou now have low-latency producing of your DASH session, however in terms of DASH you may want to indicate to the client that if it is low-latency enabled, it may fetch segments earlier than advertised in the manifest. This is indicated by the availabilityStartOffset attribute which gives the amount of seconds a request can be sent ahead of time:\nMP4Box -ast-offset 9000 -dash-live 10000 -frag 1000 -profile live -out res/live.mpd udp://IP:PORT\ngpac -i udp://IP:PORT -o res/live.mpd:segdur=10:cdur=1:profile=live:dmode=dynamic:asto=9\nIn the above example, we indicate the requests can be issued 9s (asto or -ast-offset) before the segment is fully produced. Your typical setup should have cdur + asto = segdur. \nIf asto is greater than segdur + cdur, this will results in 404; if is is less, this will increase the client delay to the live edge.\nDASH Origin Server setup¶\nYou now will want to distribute your DASH Low Latency over HTTP(S). The difficulty is that regular HTTP severs will usually not understand how to deliver a file while it is being produced without a great deal of tricks - for example, see our experimental node.js low latency server.\nTo simplify this process, GPAC features a simple HTTP server that understands DASH low latency and is able to push segments while they are being produced, without any file system monitoring. Please read the documentation of the HTTP Server for more details.\nThe server will need a local directory where files are stored and can be delivered to clients behind the live edge. This is done by indicating to the server a read directory.\nMP4Box -ast-offset 9000 -frag-rt -dash-live 10000 -frag 1000 -profile live \n -out http://localhost:8080/live.mpd:gpac:rdirs=outdir source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o http://localhost:8080/live.mpd:gpac:segdur=10:cdur=1:profile=live:dmode=dynamic:rdirs=outdir:asto=9\nNote that since this example only uses a single DASH session, all options specified for the dasher can be set globally:\nMP4Box -ast-offset 9000 -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://localhost:8080/live.mpd --rdirs=outdir --wdir=outdir source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o http://localhost:8080/live.mpd\n --segdur=10 --cdur=1 --profile=live --dmode=dynamic --asto=9 --rdirs=outdir\nDASH Origin PUSH setup¶\nYou may rather want to use a regular HTTP server as your origin server, and have GPAC push segments to that server while they are being produced. Again, please read the documentation of the HTTP Server.\nIn this case, the httpout filter does not work as a server but as an HTTP client issuing PUT or POST requests, and does not need any local directory.\nMP4Box -ast-offset 9000 -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://ORIG_SERVER_IP_PORT/live.mpd:gpac:hmode=push source1 source2\ngpac -i source1 -i source2 reframer:rt=on\n -o http://ORIG_SERVER_IP_PORT/live.mpd:gpac:segdur=10:cdur=1:asto=9:profile=live:dmode=dynamic:hmode=push\nNote that since this example only uses a single DASH session, all options specified for the dasher can be set globally:\nMP4Box -ast-offset 9000 -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://ORIG_SERVER_IP_PORT/live.mpd --hmode=push source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o http://ORIG_SERVER_IP_PORT/live.mpd \n --segdur=10 --cdur=1 --profile=live --dmode=dynamic --asto=9 --hmode=push\nDASH Low Latency live encoding¶\nWe will now use a live source (webcam), encode it in two qualities, DASH the result and push it to a remote server. Please check the encoding howto first.\nCompared to what we have seen previously, we only need to modify the input part of the graph:\ntake as a live source the default audio video grabbed by the libavdevice filter\nrescale the video as 1080p and 720p \nencode the rescaled videos at 6 and 3 mbps \nencode the audio in aac at 128 kbps\nfeed the encoders outputs to the dasher\ngpac \n# #specify the source\n -i av://:FID=1\n# #specify first rescaler and encoder for 1080p\n ffsws:osize=1920x1080:SID=1 @ enc:c=avc:fintra=1:FID=EV1:b=6m\n# #specify second rescaler and encoder for 720p\n ffsws:osize=1280x720:SID=1 @ enc:c=avc:fintra=1:FID=EV2:b=3m\n# #specify AAC encoding \n enc:c=aac:SID=1:FID=EA:b=128k\n# #specify http push output in DASH, using only sources from video and audio encoders \n -o http://ORIG_SERVER_IP_PORT/live.mpd:gpac:SID=EV1,EV2,EA\n# #http and dash options specified globally\n --hmode=push --profile=live --segdur=1 --cdur=0.1 --asto=0.9\nThe fintra parameter is used to force the encoders to generate an intra / IDR frame every one second, thus ensuring a proper segmentation of the result.\nIn the above command, we have a ultra-low latency setup where fragments are only 100ms for 1s segments. Note that this assumes that the encoders are able to always encode 100ms of data in less than 100ms, which can be problematic (intra / IDR are heavier to code). To avoid potential 404s, you can either increase the fragment duration or just only reduce the `asto, which will slightly increase the latency but keep pushing at the same rate. \nNote\nThe rescaler filter will automatically move to pass-through mode if the input size and pixel format match the output ones (desired by the encoder).\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/LL-DASH/"},{"date_scraped_timestamp":1720188034753,"host":"wiki.gpac.io","page_title":"DASH Packaging - GPAC wiki","text":"\n \n \n \n \n \nDASH Options¶\nAlso see: \nthe dasher gpac -h dasher filter documentation \n[[DASH wiki|DASH-intro]]. \nSpecifying input files¶\nInput media files to dash can use the following modifiers \ntrackID=N: only use the track ID N from the source file¶\nN: only use the track ID N from the source file (mapped to -tkid)¶\nvideo: only use the first video track from the source file¶\naudio: only use the first audio track from the source file¶\nProp=Value: add PID filtering using the same syntax as SID fragments (cf gpac -h doc)¶\n:id=NAME: set the representation ID to NAME. Reserved value NULL disables representation ID for multiplexed inputs. If not set, a default value is computed and all selected tracks from the source will be in the same output multiplex. \n:dur=VALUE: process VALUE seconds (fraction) from the media. If VALUE is longer than media duration, last sample duration is extended. \n:period=NAME: set the representation's period to NAME. Multiple periods may be used. Periods appear in the MPD in the same order as specified with this option \n:BaseURL=NAME: set the BaseURL. Set multiple times for multiple BaseURLs \nWarning: This does not modify generated files location (see segment template). \n:bandwidth=VALUE: set the representation's bandwidth to a given value \n:pdur=VALUE: sets the duration of the associated period to VALUE seconds (fraction) (alias for period_duration:VALUE). This is only used when no input media is specified (remote period insertion), e.g. :period=X:xlink=Z:pdur=Y \n:ddur=VALUE: override target DASH segment duration to VALUE seconds (fraction) for this input (alias for duration:VALUE) \n:xlink=VALUE: set the xlink value for the period containing this element. Only the xlink declared on the first rep of a period will be used \n:asID=VALUE: set the AdaptationSet ID to VALUE (unsigned int) \n:role=VALUE: set the role of this representation (cf DASH spec). Media with different roles belong to different adaptation sets. \n:desc_p=VALUE: add a descriptor at the Period level. \n:desc_as=VALUE: add a descriptor at the AdaptationSet level. Two input files with different values will be in different AdaptationSet elements. \n:desc_as_c=VALUE: add a descriptor at the AdaptationSet level. Value is ignored while creating AdaptationSet elements. \n:desc_rep=VALUE: add a descriptor at the Representation level. Value is ignored while creating AdaptationSet elements. \n:sscale: force movie timescale to match media timescale of the first track in the segment. \n:trackID=N: same as setting fragment #trackID= \n@f1[:args][@fN:args][@@fK:args]: set a filter chain to insert between the source and the dasher. Each filter in the chain is formatted as a regular filter, see filter doc gpac -h doc. If several filters are set: \nthey will be chained in the given order if separated by a single @ \na new filter chain will be created if separated by a double @@. In this case, no representation ID is assigned to the source. \nExample\nsource.mp4:@c=avc:b=1M@@c=avc:b=500k\nThis will load a filter chain with two encoders connected to the source and to the dasher.\nExample\nsource.mp4:@c=avc:b=1M@c=avc:b=500k\nThis will load a filter chain with the second encoder connected to the output of the first (!!). \nNote: @f must be placed after all other options. \nNote: Descriptors value must be a properly formatted XML element(s), value is not checked. Syntax can use file@FILENAME to load content from file. \nOptions¶\n-dash (number): create DASH from input files with given segment (subsegment for onDemand profile) duration in ms\n-dash-live (number): generate a live DASH session using the given segment duration in ms; using -dash-live=F will also write the live context to F. MP4Box will run the live session until q is pressed or a fatal error occurs\n-ddbg-live (number): same as -dash-live without time regulation for debug purposes\n-frag (number): specify the fragment duration in ms. If not set, this is the DASH duration (one fragment per segment)\n-out (string): specify the output MPD file name\n-profile,-dash-profile (string): specify the target DASH profile, and set default options to ensure conformance to the desired profile. Default profile is full in static mode, live in dynamic mode (old syntax using :live instead of .live as separator still possible). Defined values are onDemand, live, main, simple, full, hbbtv1.5.live, dashavc264.live, dashavc264.onDemand, dashif.ll\n-profile-ext (string): specify a list of profile extensions, as used by DASH-IF and DVB. The string will be colon-concatenated with the profile used\n-rap: ensure that segments begin with random access points, segment durations might vary depending on the source encoding\n-frag-rap: ensure that all fragments begin with random access points (duration might vary depending on the source encoding)\n-segment-name (string): set the segment name for generated segments. If not set (default), segments are concatenated in output file except in live profile where dash_%%s. Supported replacement strings are: \n$Number[%%0Nd]$ is replaced by the segment number, possibly prefixed with 0 \n$RepresentationID$ is replaced by representation name \n$Time$ is replaced by segment start time \n$Bandwidth$ is replaced by representation bandwidth \n$Init=NAME$ is replaced by NAME for init segment, ignored otherwise \n$Index=NAME$ is replaced by NAME for index segments, ignored otherwise \n$Path=PATH$ is replaced by PATH when creating segments, ignored otherwise \n$Segment=NAME$ is replaced by NAME for media segments, ignored for init segments \n-segment-ext (string, default: m4s): set the segment extension, null means no extension\n-init-segment-ext (string, default: mp4): set the segment extension for init, index and bitstream switching segments, null means no extension \n-segment-timeline: use SegmentTimeline when generating segments\n-segment-marker (string): add a box of given type (4CC) at the end of each DASH segment\n-insert-utc: insert UTC clock at the beginning of each ISOBMF segment\n-base-url (string): set Base url at MPD level. Can be used several times. \nWarning: this does not modify generated files location \n-mpd-title (string): set MPD title\n-mpd-source (string): set MPD source\n-mpd-info-url (string): set MPD info url\n-cprt (string): add copyright string to MPD\n-dash-ctx (string): store/restore DASH timing from indicated file\n-dynamic: use dynamic MPD type instead of static\n-last-dynamic: same as -dynamic but close the period (insert lmsg brand if needed and update duration)\n-mpd-duration (number): set the duration in second of a live session (if 0, you must use -mpd-refresh)\n-mpd-refresh (number): specify MPD update time in seconds\n-time-shift (int): specify MPD time shift buffer depth in seconds, -1 to keep all files)\n-subdur (number): specify maximum duration in ms of the input file to be dashed in LIVE or context mode. This does not change the segment duration, but stops dashing once segments produced exceeded the duration. If there is not enough samples to finish a segment, data is looped unless -no-loop is used which triggers a period end\n-run-for (int): run for given ms the dash-live session then exits\n-min-buffer (int): specify MPD min buffer time in ms\n-ast-offset (int): specify MPD AvailabilityStartTime offset in ms if positive, or availabilityTimeOffset of each representation if negative\n-dash-scale (int): specify that timing for -dash, -dash-live, -subdur and -do_frag are expressed in given timescale (units per seconds) rather than ms\n-mem-frags: fragmentation happens in memory rather than on disk before flushing to disk\n-pssh (int): set pssh store mode \nv: initial movie \nf: movie fragments \nm: MPD \nmv, vm: in initial movie and MPD \nmf, fm: in movie fragments and MPD \nn: remove PSSH from MPD, initial movie and movie fragments \n-sample-groups-traf: store sample group descriptions in traf (duplicated for each traf). If not set, sample group descriptions are stored in the initial movie\n-mvex-after-traks: store mvex box after trak boxes within the moov box. If not set, mvex is before\n-sdtp-traf (int): use sdtp box in traf (Smooth-like) \nno: do not use sdtp \nsdtp: use sdtp box to indicate sample dependencies and do not write info in trun sample flags \nboth: use sdtp box to indicate sample dependencies and also write info in trun sample flags \n-no-cache: disable file cache for dash inputs\n-no-loop: disable looping content in live mode and uses period switch instead\n-hlsc: insert UTC in variant playlists for live HLS\n-bound: segmentation will always try to split before or at, but never after, the segment boundary\n-closest: segmentation will use the closest frame to the segment boundary (before or after)\n-subsegs-per-sidx,-frags-per-sidx (int): set the number of subsegments to be written in each SIDX box \n0: a single SIDX box is used per segment \n-1: no SIDX box is used \n-ssix: enable SubsegmentIndexBox describing 2 ranges, first one from moof to end of first I-frame, second one unmapped. This does not work with daisy chaining mode enabled\n-url-template: use SegmentTemplate instead of explicit sources in segments. Ignored if segments are stored in the output file\n-url-template-sim: use SegmentTemplate simulation while converting HLS to MPD\n-daisy-chain: use daisy-chain SIDX instead of hierarchical. Ignored if frags/sidx is 0\n-single-segment: use a single segment for the whole file (OnDemand profile)\n-single-file: use a single file for the whole file (default)\n-bs-switching (string, default: inband, values: inband|merge|multi|no|single): set bitstream switching mode \ninband: use inband param set and a single init segment \nmerge: try to merge param sets in a single sample description, fallback to no \nmulti: use several sample description, one per quality \nno: use one init segment per quality \npps: use out of band VPS,SPS,DCI, inband for PPS,APS and a single init segment \nsingle: to test with single input \n-moof-sn (int): set sequence number of first moof to given value\n-tfdt (int): set TFDT of first traf to given value in SCALE units (cf -dash-scale)\n-no-frags-default: disable default fragments flags in trex (required by some dash-if profiles and CMAF/smooth streaming compatibility)\n-single-traf: use a single track fragment per moof (smooth streaming and derived specs may require this)\n-tfdt-traf: use a tfdt per track fragment (when -single-traf is used)\n-dash-ts-prog (int): program_number to be considered in case of an MPTS input file\n-frag-rt: when using fragments in live mode, flush fragments according to their timing\n-cp-location (string): set ContentProtection element location \nas: sets ContentProtection in AdaptationSet element \nrep: sets ContentProtection in Representation element \nboth: sets ContentProtection in both elements \n-start-date (string): for live mode, set start date (as xs:date, e.g. YYYY-MM-DDTHH:MM:SSZ). Default is current UTC \nWarning: Do not use with multiple periods, nor when DASH duration is not a multiple of GOP size \n-cues (string): ignore dash duration and segment according to cue times in given XML file (tests/media/dash_cues for examples)\n-strict-cues: throw error if something is wrong while parsing cues or applying cue-based segmentation\n-merge-last-seg: merge last segment if shorter than half the target duration \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-dash-opts/"},{"date_scraped_timestamp":1720188030276,"host":"wiki.gpac.io","page_title":"DASH SRD and HEVC tiling","text":"\n \n \n \n \n \n \nWe were at MMSys 2016 talking about new nice features in GPAC: support for MPEG-DASH Spatial Relation Description and HEVC motion-constrained tiling!\nWe had a quick poster presenting our two demos\nTile-based adaptation using independent video encoded in H264¶\nIn this demo, a 4K tears of steel is split in 9 tiles, each of them encoded in various bitrates and resolutions. The resulting composition of tiles is played in GPAC with frame-level accuracy to rebuild the complete video. Of course, all of this is 100% DASH compliant and produced with MP4Box. MP4Box has the ability to add any MPD descriptor at period, adaptation set or representation level. For SRD, we use adaptation set descriptors as specified in MPEG DASH; for example:\nMP4Box -dash 1000 [other dash params]  source.mp4:desc_as=<SupplementalProperty schemeIdUri=\\\"urn:mpeg:dash:srd:2014\\\" value=\\\"0,0,1,1,1,2,2\\\"/>\nindicates that source.mp4 is placed at X=0, Y=1 with width 1 and height 1 on a tiling grid of size 2x2. This information needs to be manually specified at command line for independent videos, but  is automatically inserted if the source file contains an HEVC tile track (see second demo).\nThe full content of the demo can be browsed here, and you can have fun with the HD version or the 4K version of the DASH session.\nYou can use GPAC player with gui to watch the different tiles quality and stats:\nHEVC Motion-constrained Tile-based adaptation¶\nIn this demo, we use HEVC tiling tools with constrained motion to allow replacing a tile of an HEVC bitstream with a tile from another HEVC bitstream with same configuration but different quality. In this use case the different HEVC bitstream represent the same content in order to perform bitrate adaptation at the tile level. The nice thing about this is that the reconstructed bitstream is HEVC compliant and requires only a single decoder for the playback!\nThe full content of the demo can be browsed here, and you can have fun with the HD version of the DASH session.\nYou can use GPAC player with gui to watch the different tiles quality and stats:\nA complete tutorial for HEVC and tiling is available [[here|HEVC Tile-based adaptation guide]].\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/MPEG-DASH-SRD-and-HEVC-tiling-for-VR-videos/"},{"date_scraped_timestamp":1720188120607,"host":"wiki.gpac.io","page_title":"DASH SRD and HEVC tiling","text":"\n \n \n \n \n \n \nWe were at MMSys 2016 talking about new nice features in GPAC: support for MPEG-DASH Spatial Relation Description and HEVC motion-constrained tiling!\nWe had a quick poster presenting our two demos\nTile-based adaptation using independent video encoded in H264¶\nIn this demo, a 4K tears of steel is split in 9 tiles, each of them encoded in various bitrates and resolutions. The resulting composition of tiles is played in GPAC with frame-level accuracy to rebuild the complete video. Of course, all of this is 100% DASH compliant and produced with MP4Box. MP4Box has the ability to add any MPD descriptor at period, adaptation set or representation level. For SRD, we use adaptation set descriptors as specified in MPEG DASH; for example:\nMP4Box -dash 1000 [other dash params]  source.mp4:desc_as=<SupplementalProperty schemeIdUri=\\\"urn:mpeg:dash:srd:2014\\\" value=\\\"0,0,1,1,1,2,2\\\"/>\nindicates that source.mp4 is placed at X=0, Y=1 with width 1 and height 1 on a tiling grid of size 2x2. This information needs to be manually specified at command line for independent videos, but  is automatically inserted if the source file contains an HEVC tile track (see second demo).\nThe full content of the demo can be browsed here, and you can have fun with the HD version or the 4K version of the DASH session.\nYou can use GPAC player with gui to watch the different tiles quality and stats:\nHEVC Motion-constrained Tile-based adaptation¶\nIn this demo, we use HEVC tiling tools with constrained motion to allow replacing a tile of an HEVC bitstream with a tile from another HEVC bitstream with same configuration but different quality. In this use case the different HEVC bitstream represent the same content in order to perform bitrate adaptation at the tile level. The nice thing about this is that the reconstructed bitstream is HEVC compliant and requires only a single decoder for the playback!\nThe full content of the demo can be browsed here, and you can have fun with the HD version of the DASH session.\nYou can use GPAC player with gui to watch the different tiles quality and stats:\nA complete tutorial for HEVC and tiling is available [[here|HEVC Tile-based adaptation guide]].\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/MPEG-DASH-SRD-and-HEVC-tiling-for-VR-videos/?q="},{"date_scraped_timestamp":1720188109836,"host":"wiki.gpac.io","page_title":"DASH Sequences - GPAC wiki","text":"\n \n \n \n \n \n \nTelecom Paris has been generating a set of DASH sequences, and is making them available for DASH conformance testing. \nThese sequences are distributed under the terms of the Creative Common by-nc-nd Licence.\nSequences Links¶\nDASH Conformance (browse directory, including the encoded source sequences and the bash script to regenerate the sequences)\nISOBMF Sequences¶\nlive profile without bitstream switching support: Audio Video Audio+Video\nlive profile with bitstream switching support: Audio Video Audio+Video\nlive profile multiplexed audio+video with bitstream switching and without bitstream switching\nlive profile with two audio codecs (@group attribute) without bitstream switching\nlive profile with five periods without bitstream switching\nmain profile, single file without bitstream switching support: Audio Video Audio+Video\nmain profile, single file with bitstream switching support: Audio Video Audio+Video\nmain profile, multiple files without bitstream switching support: Audio Video Audio+Video\nmain profile, multiple files with bitstream switching support: Audio Video Audio+Video\nmain profile, OpenGOP without bitstream switching support: Video Audio+Video\nmain profile, OpenGOP with bitstream switching support: Video Audio+Video\nonDemand profile: Audio Video Audio+Video\nfull profile, Gradual Decoding Refresh without bitstream switching support: Video Audio+Video\nfull profile, Gradual Decoding Refresh with bitstream switching support: Video Audio+Video\nMPEG-2 TS Sequences¶\nsimple profile in a single file\nsimple profile in multiple files with templates and without templates\nThese sequences consist of a video object in various bitrates and resolutions, as well as an audio object in two different coding systems. All these files have been generated with open-source, freely available software. The sequences duration is 10 minutes.\nAudio¶\nThe audio track consists of beeps. Beeps occur precisely at each second. Beeps are alternatively high tone (even seconds) or low tone (odd seconds). The audio track is available as MP3 or AAC, 64kbps.\nVideo¶\nThe video track consists of an animated pattern showing:\na circle becoming alternately white or grey every second\na time counter and a frame counter\ninformation about the generation (frame rate, resolution, generation information)\nThe video track is a combination of:\na codec: AVC/H264 only;\navailable resolutions: 320x180, 640x360, 1280x720, 1920x1080\nseveral GOP profiles:\nbaseline profile, closed GOP, RAP 1s\nregular 2B-pattern open GOP, RAP 1s\nGDR (Gradual Decoding Refresh) with an 8-picture refresh\n2 bitrates for each of the above combinations\nSPS/PPS ids are unique for a given GOP profile, so as to avoid the switching issue caused by the use of same ids and are described in the table below\nISO File Format Sequences¶\nAll DASH MPDs are generated with:\n10s segments, beginning with RAP\n1s “moof” fragments (which corresponds to the GOP size)\nEach adaptation set (described below) are available in the following ways:\nMultiple segments:\nsegment list conforming Main profile, except for GDR which complies with Full profile; MPD for these sequences are named XXX-**files**.mpd,\nsegment URL Template conforming Live profile, except for GDR which complies with Full profile; MPD for these sequences are named XXX-**url**.mpd.\nOne file:\nindexes (n “sidx”) conforming Main profile, except for GDR which complies with Full profile; MPD for these sequences are named XXX-**indexes**.mpd,\nsingle segment (1 “sidx”) conforming OnDemand profile, except for GDR which complies with Full profile; MPD for these sequences are named XXX-**single**.mpd.\nCurrently the following combinations are provided:\n1 Adaptation Set, with alternate MP3 or AAC audio representations;\n2 Adaptation Sets, one with alternate MP3 or AAC audio representations, the other with baseline AVC video;\n1 Adaptation Set, with audio (AAC) and video (open-GOP), using 2 components. Each segment is made of 2 sub-segments indexed as:\n1 primary “sidx” indexing both subsegments (“reference_type” 1);\neach subsegment has a secondary A/V “sidx” (“reference_type” 0) with 5 entries (one per “moof”).\n1 Adaptation Set, with audio (AAC) and video (GDR with 8 frames roll recovery), using 2 components. Each segment is made of 5 sub-segments indexed with daisy chained SIDX; each SIDX contains two \"reference_type 0\" subsegments (one per moof) and one \"reference type 1\" subsegment pointing to the next SIDX (except for the last SIDX).\nMPEG-2 TS Sequences¶\nAll DASH MPDs are generated with 10s segments, beginning with RAP. Only the baseline AVC|H264 profile is currently used. Each adaptation set (described below) are available in the following ways:\nMultiple segments:\nsegment list conforming Simple@TS profile; MPD for these sequences are named XXX-**files**.mpd,\nsegment URL Template conforming Simple@TS profile; MPD for these sequences are named XXX-**url**.mpd.\nOne file:\nsingle segment (1 “sidx”) conforming Simple@TS profile; MPD for these sequences are named XXX-**single**.mpd.\nUncovered DASH features¶\nNot all features of DASH are tested with the current sequences. Especially, the sequences do not test:\nSubsets\nSegment Timeline\nSub Representations and sub-segment indexing\nMetrics\nBitstream Switching Segments (although bitstream switching is tested)\nSequences Generation¶\nTo generate these DASH sequences, several open-source software tools have been used (in the sequential order of stream generation):\nGPAC/MP4Box to process the BIFS visual data and export it to a raw video track in AVI,\nAudacity [1.3 Beta] to generate the raw audio track,\nx264 [0.118.2085 8a62835] for the AVC|H.264 encoding,\nfaac [as included in FFmpeg build] for AAC encoding,\nLAME [as included in FFmpeg build] for MP3 encoding,\nGPAC/MP42TS [REV3926] for Transport Stream packaging,\nGPAC/MP4Box [REV3926] for ISOBMF packaging, MPD generation and DASH segmentation (TS and ISOBMFF).\nFor more info on DASH generation, see [[DASH Support in MP4Box]].\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/DASH-Sequences/"},{"date_scraped_timestamp":1720188086038,"host":"wiki.gpac.io","page_title":"DASH and HLS Transcoding - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nIn this howto, we will study various setups for DASH transcoding. \nPlease make sure you are familiar with DASH terminology before reading. \nIt is likely that your source media is not properly encoded for DASH or HLS delivery, most likely because:\nopenGOPs are used \nkey-frame position do not match between your different qualities\nkey-frame intervals are not constant\nNote\nNon-constant key frame interval is not problematic for on demand use cases, as most profiles can deal with it. \nYou can use GPAC to transcode your content, and inject the result in a GPAC filter session. \nWe assume GPAC 2.0 or above for command line syntax.\nAs usual in GPAC, the concepts presented here can be reused with different filter chains, for example input being a live source or output being a non-dash file.\nSingle Quality Transcoding¶\nTo transcode a source to DASH using 2s segments to AVC|H264 single quality 1 Mbps and not modifying the audio:\ngpac -i source_av.mp4 c=avc:b=1m -o dash/live.mpd:segdur=2\nThe segment duration will be passed to the encoder to force key frame insertion. If gpac still complains about open-gop present at segment boundaries, force encoder reset:\ngpac -i source_av.mp4 c=avc:b=1m:rc -o dash/live.mpd:segdur=2\nYou can force an intra refresh frequency lower than the segment duration, for example 2s I-frames for 4s segments:\ngpac -i source_av.mp4 c=avc:b=1m:fintra=2 -o dash/live.mpd:segdur=4\nYou can also transcode to HLS:\ngpac -i source_av.mp4 c=avc:b=1m -o dash/live.m3u8:segdur=2\nor to both HLS and DASH:\ngpac -i source_av.mp4 c=avc:b=1m -o dash/live.m3u8:dual:segdur=2\nYou may also transcode the audio:\ngpac -i source_av.mp4 c=avc:b=1m c=aac:b=64k -o dash/live.mpd:segdur=2\nTo transcode only non-aac tracks, you must use explicit PID linking (you may need to single-quot the link string to escape '!'):\ngpac -i source_av.mp4 c=avc:b=1m @1#CodecID=!aac c=aac:b=64k -o dash/live.mpd:segdur=2\nIf no non-AAC audio tracks are present, you will get a warning indicating the AAC encoder was not used:\nFilters not connected:\nffenc (c=aac:b=64k)\nMultiple Quality Transcoding¶\nSame resolution¶\nTo transcode in two bitrates your video, you must add explicit links between your encoders and the source (or desired filter in the source chain).\nFor example, to transcode video source to 1 mbps and 200 kbps streams:\ngpac -i source_av.mp4 @ c=avc:b=1m @@ c=avc:b=200k @ @1 -o dash/live.mpd:segdur=2\nWe use here the @@ link shortcut to get the first filter declared. This is equivalent to:\ngpac -i source_av.mp4 @ c=avc:b=1m @1 c=avc:b=200k @ @1 -o dash/live.mpd:segdur=2\ngpac -i source_av.mp4:FID=S c=avc:b=1m:SID=S:FID=V1 c=avc:b=200k:SID=S:FID=V2 -o dash/live.mpd:segdur=2:SID=V1,V2\nIf you need audio passthrough:\ngpac -i source_av.mp4 @ c=avc:b=1m @@ c=avc:b=200k @ @1 @@#audio -o dash/live.mpd:segdur=2\nThe same command using explicit filter IDs:\ngpac -i source_av.mp4:FID=S c=avc:b=1m:SID=S:FID=V1 c=avc:b=200k:SID=S:FID=V2 -o dash/live.mpd:segdur=2:SID=V1,V2,S#audio\nIf you need audio encoding:\ngpac -i source_av.mp4 @ c=avc:b=1m @@ c=avc:b=200k @@ c=aac:b=64k @ @1 @2 -o dash/live.mpd:segdur=2\nThe same command using explicit filter IDs:\ngpac -i source_av.mp4:FID=S c=avc:b=1m:SID=S:FID=V1 c=avc:b=200k:SID=S:FID=V2 c=aac:b=64k:SID=S:FID=A1 -o dash/live.mpd:segdur=2:SID=V1,V2,A1\nMultiple resolutions¶\nTo transcode in multiple resolutions your video, we will need the rescaler filter.\ngpac -i source_1080p.mp4 ffsws:osize=1280x720 @ c=avc:b=1m @@ c=avc:b=2m @ @1 -o dash/live.mpd:segdur=2\nObviously you can combine with the above approach to provide multiple bitrates for each resolution:\ngpac -i source_1080p.mp4 @ ffsws:osize=1280x720 @ c=avc:b=1m @1 c=avc:b=500k @@ c=avc:b=4m @@ c=avc:b=2m @ @1 @2 @3 -o dash/live.mpd:segdur=2\nIf you specify the rescaler after the HD encoders:\ngpac -i source_1080p.mp4 @ c=avc:b=4m @@ c=avc:b=2m @@ ffsws:osize=1280x720 @ c=avc:b=1m @1 c=avc:b=500k @ @1 @3 @4 -o dash/live.mpd:segdur=2\nIf you add more encoders, it may be simpler to understand the command line using explicit IDs:\ngpac -i source_1080p.mp4:FID=S c=avc:b=4m:SID=S:FID=V1 c=avc:b=2m:SID=S:FID=V2 ffsws:osize=1280x720:SID=RZ c=avc:b=1m:SID=RZ:FID=V3 c=avc:b=500k:SID=RZ:FID=V4 -o dash/live.mpd:segdur=2:SID=V1,V2,V3,V4\nLive Transcoding¶\nThe concepts are the same, except that you may need to add a real-time reframer after your input if it is not realtime\ngpac -i source_1080p.mp4 reframer:rt=on ffsws:osize=1280x720 @ c=avc:b=1m @2 c=avc:b=2m @ @1 -o dash/live.mpd:segdur=2:dmode=dynamic\nUsing explicit IDs\ngpac -i source_1080p.mp4:FID=S reframer:rt=on:SID=S:FID=RT ffsws:osize=1280x720:SID=RT:FID=RZ c=avc:b=1m:SID=RZ:FID=V1 c=avc:b=2m:SID=RT:FID=V2 -o dash/live.mpd:segdur=2:SID=V1,V2:dmode=dynamic\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/dash_transcoding/"},{"date_scraped_timestamp":1720188113477,"host":"wiki.gpac.io","page_title":"Docker - GPAC wiki","text":"\n \n \n \n \n \n \nHOME » Build » Docker\nThis page contains instructions on how to build and use gpac with Docker. \nFor now, only linux containers are available. \nPre-requisite: a working Docker installation.\nLinux¶\nWe provide two types of resources depending on your use case. \nA Dockerfile if you want to build your own image\nA Docker Hub image if you only want to use gpac\nBuild your own image (optional)¶\n## clone gpac repo\ngit clone https://github.com/gpac/gpac.git\ncd gpac\n## make changes or checkout a specific branch or commit you want \n... \n## doubly optional: you can change and rebuild the base system containing the dependencies for gpac with\ndocker build -t gpac/ubuntu-deps -f build/docker/ubuntu-deps.Dockerfile .\n## build the docker image\ndocker build -t myimages/gpac -f build/docker/ubuntu.Dockerfile .\nUse an image¶\nYou can either use your own image if you built it (see section above), or use pre-built images from Docker Hub. \nTo get a pre-build image, use: \nYou can use the gpac image a number of way. \nGet the build artifacts¶\nThe image contains binaries for: \n- a minimal statically linked executable for MP4Box and gpac CLI tools\n- deb packages for the OS used in the image (currently Ubuntu 22.04)\nYou can use these binaries on your host OS if it is relevant.\nYou can extract these binaries with \n$ docker create --name dummy gpac/ubuntu\n$ docker cp dummy:/gpac/binaries .\n$ docker rm dummy\n$ ls -1 binaries/\ngpac\ngpac_2.3-DEV-rev351-gba47abf5a-docker_amd64.deb\nlibgpac-dev_2.3-DEV-rev351-gba47abf5a-docker_amd64.deb\nlibgpac_static.a\nMP4Box\n$ ./binaries/gpac\nRefreshing all options registry, this may take some time ... done\ngpac - GPAC command line filter engine - version 2.3-DEV-rev351-gba47abf5a-docker\n(c) 2000-2023 Telecom Paris distributed under LGPL v2.1+ - http://gpac.io\n MINI build (encoders, decoders, audio and video output disabled)\nPlease cite our work in your research:\n GPAC Filters: https://doi.org/10.1145/3339825.3394929\n GPAC: https://doi.org/10.1145/1291233.1291452\nRun gpac inside of container¶\nOnce you have a gpac image, you can run commands directly inside of a container. \nExamples: \n$ docker run --rm gpac/ubuntu MP4Box -version\n[core] Creating default credential key in /root/.gpac/creds.key, use -cred=PATH/TO_FILE to overwrite\nMP4Box - GPAC version 2.3-DEV-rev345-gc7c404d8f-master\n(c) 2000-2023 Telecom Paris distributed under LGPL v2.1+ - http://gpac.io\nPlease cite our work in your research:\n GPAC Filters: https://doi.org/10.1145/3339825.3394929\n GPAC: https://doi.org/10.1145/1291233.1291452\nGPAC Configuration: --host=x86_64-linux-gnu --prefix=/usr --build=x86_64-linux-gnu --extra-cflags=-Wall -g -fPIC -DPIC --cc=ccache cc CXX=ccache g++\nFeatures: GPAC_CONFIG_LINUX GPAC_64_BITS GPAC_HAS_IPV6 GPAC_HAS_SSL GPAC_HAS_SOCK_UN GPAC_MINIMAL_ODF GPAC_HAS_QJS GPAC_HAS_FAAD GPAC_HAS_MAD GPAC_HAS_LIBA52 GPAC_HAS_JPEG GPAC_HAS_PNG GPAC_HAS_FFMPEG GPAC_HAS_OPENSVC GPAC_HAS_OPENHEVC GPAC_HAS_THEORA GPAC_HAS_VORBIS GPAC_HAS_LINUX_DVB \n$ docker run --rm gpac/ubuntu gpac -i http://download.tsi.telecom-paristech.fr/gpac/gpac_test_suite/mp4/counter_video_360.mp4 inspect\n[core] Creating default credential key in /root/.gpac/creds.key, use -cred=PATH/TO_FILE to overwrite\nRefreshing all options registry, this may take some time ... done\nPID 1 video duration 10:00.000 timescale 25000 640x360 fps 25 SAR 1/1 193 kbps 15000 frames codec avc1.42C01E AVC|H264 PL Baseline@3 YUV 4:2:0 8 bpp\n## map a local folder to the container to read/write\n$ docker run --rm -v ./vids:/vids gpac/ubuntu gpac -i /vids/counter_video_360.mp4 -o /vids/test.mpd\n$ cat vids/test.mpd\n<?xml version=\"1.0\"?>\n<!-- MPD file Generated with GPAC version 2.3-DEV-rev345-gc7c404d8f-master at 2023-06-14T14:22:59.640Z -->\n<MPD xmlns=\"urn:mpeg:dash:schema:mpd:2011\" minBufferTime=\"PT1.000S\" type=\"static\" mediaPresentationDuration=\"PT0H10M0.000S\" maxSegmentDuration=\"PT0H0M1.000S\" profiles=\"urn:mpeg:dash:profile:full:2011\">\n <ProgramInformation moreInformationURL=\"http://gpac.io\">\n <Title>test.mpd generated by GPAC</Title>\n </ProgramInformation>\n <Period duration=\"PT0H10M0.000S\">\n <AdaptationSet segmentAlignment=\"true\" maxWidth=\"640\" maxHeight=\"360\" maxFrameRate=\"25\" par=\"16:9\" startWithSAP=\"1\">\n <SegmentTemplate media=\"counter_video_360_dash$Number$.m4s\" initialization=\"counter_video_360_dashinit.mp4\" timescale=\"25000\" startNumber=\"1\" duration=\"25000\"/>\n <Representation id=\"1\" mimeType=\"video/mp4\" codecs=\"avc1.42C01E\" width=\"640\" height=\"360\" frameRate=\"25\" sar=\"1:1\" bandwidth=\"193112\">\n </Representation>\n </AdaptationSet>\n </Period>\n</MPD>\nUse a container as a dev or runtime environement¶\nFinally you can simply run a container and use it as an environement where you can run gpac commands, modify and build gpac, or develop programs built again libgpac. \n$ docker run -it gpac/ubuntu bash\nroot@6610e278322e:/gpac/gpac_public# make distclean && ./configure --enable-debug && make -j && make install\nroot@6610e278322e:/gpac/gpac_public# MP4Box -version\nMP4Box - GPAC version 2.3-DEV-rev351-gba47abf5a-docker\n(c) 2000-2023 Telecom Paris distributed under LGPL v2.1+ - http://gpac.io\nPlease cite our work in your research:\n GPAC Filters: https://doi.org/10.1145/3339825.3394929\n GPAC: https://doi.org/10.1145/1291233.1291452\nGPAC Configuration: --enable-debug\nFeatures: GPAC_CONFIG_LINUX GPAC_64_BITS GPAC_HAS_IPV6 GPAC_HAS_SSL GPAC_HAS_SOCK_UN GPAC_MINIMAL_ODF GPAC_HAS_QJS GPAC_HAS_FAAD GPAC_HAS_MAD GPAC_HAS_LIBA52 GPAC_HAS_JPEG GPAC_HAS_PNG GPAC_HAS_FFMPEG GPAC_HAS_OPENSVC GPAC_HAS_OPENHEVC GPAC_HAS_THEORA GPAC_HAS_VORBIS GPAC_HAS_LINUX_DVB \n \n \n \n \n ","url":"https://wiki.gpac.io/Build/build/GPAC-Build-with-Docker/"},{"date_scraped_timestamp":1720188124374,"host":"wiki.gpac.io","page_title":"GPAC 10bit support - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC supports 10bit display and 10bit video decoding. \nThe support requires a graphics card capable of driving a 10 bit display; this support is usually enabled via the control application of your GPU. Do not worry, 10-bit video will still show in 8 bit if your graphics card doesn't handle 10 bit output !\nBy default, GPAC always uses 8 bits per component in OpenGL setup. You will have to manually configure the player to use 10 bits per component in OpenGL. \nEdit the GPAC configuration file and set the following keys:\nYou can also enable it from the command line using -gl-bits-comp=10 option (core option, works for any application in GPAC).\n \n \n \n \n ","url":"https://wiki.gpac.io/Player/GPAC-10bit-support/"},{"date_scraped_timestamp":1720187940959,"host":"wiki.gpac.io","page_title":"GPAC and NodeJS - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to use GPAC Filters in NodeJS. \nThis GPAC node-API provides bindings to GPAC filter session. The design is almost identical to the Python bindings, closely inspired from the JS FilterSession API used in GPAC.\nThe GPAC Node API is documented here.\nYou can also have a look at the test script in the GPAC repository.\nWarning GPAC NodeJS bindings are only available starting from GPAC 2.0.\nBefore you begin¶\nThe GPAC NodeJS bindings are using n-api for interfacing with libgpac filter session, while providing an object-oriented wrapper hiding all GPAC C design.\nThe binding is called gpac_napi.c, and is hosted in GPAC source tree. It is NOT installed during default install step.\nYou will need to build the module using node-gyp, potentially editing share/nodejs/binding.gyp as required for your system.\nThe binding.gyp provided is for GPAC:\nbuilt in regular shared library mode for libgpac (i.e. NodeJS module is not compatible with mp4box-only build)\ninstalled on your system (gpac headers available in a standard include directory, libgpac in standard lib directory), typically done with sudo make install after building gpac\nYou can then build using:\n% cd gpac/share/nodejs\n% node-gyp configure\n% node-gyp build\nIf you don't want to install on your system, you will need to modify the binding.gyp file to set the include dir to the root of gpac source tree:\n\"include_dirs\": [\"<(module_root_dir)/../../include\"]\nIf built using configure and make, you will likely have a custom config.h file, and the build tree root must also be indicated together with the GPAC_HAVE_CONFIG_H macro.\nIf you build gpac at the top of the gpac source tree (using e.g. ./configure && make), the build tree root for node will be <(module_root_dir)/../...\nIf you build gpac in gpac/bin/mytest, (using e.g. mkdir bin/mytest && cd bin/mytest && ../../configure && make), the build tree root for node will be <(module_root_dir)/../../bin/mytest.\nYou will also likely need to update the libraries property to add the path to your libgpac shared library (typically in bin/gcc of the build tree root).\n{\n \"targets\": [{\n \"target_name\": \"gpac\",\n \"sources\": [ \"./src/gpac_napi.c\"],\n \"include_dirs\": [\"<(module_root_dir)/../../include\", \"<(module_root_dir)/../..\"],\n \"libraries\": [ '-lgpac',\"-L<(module_root_dir)/../../bin/gcc\"],\n \"defines\":[\"GPAC_HAVE_CONFIG_H\"]\n }]\n}\nThe build is usually located in $GPAC_SRC/share/node/build/Release so you will need to specify the full path to the module in your JS file:\nconst gpac = require('./build/Release/gpac');\nor if you want to use the provided index.js, simply use:\nconst gpac = require(path/to/gpac/share/nodejs);\nCheck everything is in place running the following JS:\nconst gpac = require('./build/Release/gpac');\nconsole.log(\"Welcome to GPAC NodeJS !\\nVersion: \" + gpac.version);\nRunning this should print your current GPAC version. \nA test program gpac.js exercising most of the NodeJS GPAC bindings is available in gpac/share/nodejs/test\nTuning up GPAC¶\nThe first thing to do is to initialize libgpac. This is done by default while importing the bindings with the following settings:\nno memory tracking\ndefault GPAC profile used\nIf you want to change these, you need to re-init libgpac right after import:\ngpac.init(1, \"customprofile\");\nAny call other than init to GPAC will prevent any subsequent call to init to be executed.\nBefore starting any filter session, you may also need to pass some global configuration options (libgpac core or filter options) to GPAC:\n//For example, blacklist some filters, run quiet and set vsync option (for vout)\ngpac.set_args([\"gpacnode\", \"-blacklist=filters_you_do_not_want\", \"-quiet\", \"--vsync=0\"]);\nYou can also pass the command line arguments so that you can specify GPAC options at prompt:\ngpac.set_args(process.argv);\nWARNING: the arguments must all be set together, only the first call to set_args() will be taken into account\nYou may want to adjust the log tools and levels of GPAC:\ngpac.set_logs(\"dash@info\");\nSetting up filter sessions¶\nSimple sessions¶\nTo create a filter session, the simplest way is to use all defaults value, creating a single-threaded blocking session:\nfs = new gpac.FilterSession();\nYou can then add your filters as usual. \nPlayback example:\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load(\"vout\");\nfs.run();\nRemux example:\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load_dst(\"test.ts\");\nfs.run();\nSince the session is blocking, you won't be able to run any other JS code until the end of the session.\nNon-blocking sessions¶\nA non-blocking session will need to be called on regular basis to process pending filter tasks. It is useful if you need to do other tasks while the session is running and do not want to use callbacks from GPAC for that.\nfs = gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load(\"vout\");\nwhile (1) {\n //do things\n //call session\n fs.run();\n //if last task, GPAC session is done\n if (fs.last_task) break;\n}\nThis allows you to do some JS work on the side, however any JS call involving promises will not work here, as promises are resolved inside NodeJS main event loop and we don't go there until the end of the session.\nAsync non-blocking sessions¶\nTo run the session in non-blocking mode while still allowing NodeJS to run its main event loop and resolve other promises, you will need to run the session as a promise. The following example is creating a promise recursively calling itself until the end of the session:\n//create session in non-blocking mode:\nlet fs = new gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\n//setup your filters\n//Run session as Promise\nconst FilterSessionPromise = (fs_run_task) => {\n var fsrun_promise = () => {\n return (fs.last_task==false) ? fs_run_task().then(fsrun_promise) : Promise.resolve();\n }\n return fsrun_promise();\n};\nconst run_task = () => {\n return new Promise((resolve, reject) => {\n resolve( fs.run_step() );\n });\n}\nFilterSessionPromise(run_task).then( ).finally( ()=> { console.log('session is done'); } ) );\nconsole.log('Entering NodeJS EventLoop');\nCallbacks in sessions¶\nRegardless of the way you run the session, you can request for being called back once or on regular basis. This is achieved by posting tasks to the GPAC session scheduler. A task object shall provide an execute method to be called. This function may return:\nfalse to cancel the task, \ntrue to reschedule the task asap\na positive integer giving the time of next task callback in milliseconds\nThe callback function is called with this set to the task object.\n//create a custom task\ntask = {};\ntask.execute = () => {\n console.log('in task'); \n return 1000;\n};\nfs.post(task);\n//run as usual\nTasks can be created at any time, either at the beginning or in a callback function (e.g., another task).\nNOTE When running the session in multi-thread mode, callback tasks are always executed by the main thread (NodeJS main).\nLinking filters¶\nIn order to link filters when desired, you must explicitly do this using set_source of the destination filter. For example, when inserting a reframer in a chain:\nf_src = fs.load_src('source.mp4');\nf_dst = fs.load_dst('remux.mp4');\nf_reframe = fs.load('reframer');\nf_dst.set_source(reframer);\nYou can specify the usual link filtering as an optional argument to set_source:\nf_dst.set_source(reframer, \"#PID=1\");\nThis will instruct that the destination only accepts PIDs coming from the reframer filter, and with ID 1.\nInspecting filters¶\nYou can query the number of input and output PIDs of a filter, the source filter of an input PID, the destination filters of an output PID, their possible options, update options, send events, ...\nPlease check the API documentation and refer to the NodeJS example.\nNote that (as in GPAC JS or Python) properties referring to constant values are not exposed as their native types but as strings. This is the case for these important types:\nStreamType: string containing the stream type name (e.g. 'Visual', 'Audio', ...)\nCodecID: string containing the codec name\nPixelFormat: string containing the pixel format name\nAudioFormat: string containing the audio format name\nCustom Filters¶\nYou can define your own filter(s) to interact with the media pipeline. As usual in GPAC filters, a custom filter can be a source, a sink or any other filter. It can consume packets from input PIDs and produce packets on output PIDs. \nCustom filters are created through the new_filter function of the filter session object. The custom filter can then assign its callbacks functions:\nGF_Err process() method called whenever the filter has some data to process.\nGF_Err configure_pid(pid, is_remove) method called whenever a new PID must be configured, re-configured or removed in the custom filter\nBool process_event(evt) method called whenever an event is passing through the filter or one of its PIDs\nGF_Err reconfigure_output(pid) method called whenever an output PID should be reconfigured\nThese callbacks are all optional, but process should be set if you want your filter to perform any action.\nFilters accepting input PIDs and/or producing output PIDs must configure their capabilities using push_cap function.\nNOTE When running the session in multi-thread mode, custom filter callbacks are always executed by the main thread (NodeJS main).\nCustom Sink example¶\nThe following defines a custom filter doing simple inspection of the pipeline (sink filter) \nlet cust = fs.new_filter('MyFilterJS');\ncust.ipids=[];\n//indicate what we accept and produce - this can be done ether in the constructor or after, but before running the session\n//here we only accept video streams as input, and do not produce any output\ncust.push_cap(\"StreamType\", \"Visual\", gpac.GF_CAPS_INPUT);\n//we accept one or more input video PID, we must configure them\ncust.configure_pid = (pid, is_remove) => {\n if (is_remove) {\n console.log('PID removed !');\n return gpac.GF_OK;\n }\n //PID is already registered with our filter, this is a reconfiguration\n if (this.ipids.indexOf(pid) < 0) {\n this.ipids.push(pid);\n console.log('PID initial configure !');\n } else {\n console.log('PID reconfigure !');\n }\n //enumerate all props \n pid.enum_props( (type, val) => {\n console.log('\\t' + type + ': ' + val);\n });\n //we are a sink, we MUST fire a play event\n evt = new gpac.FilterEvent(gpac.GF_FEVT_PLAY);\n pid.send_event(evt);\n return gpac.GF_OK;\n};\ncust.process() = () => {\n this.ipids.forEach(pid => {\n pck = pid.get_packet();\n if (!pck) break;\n console.log('Got Packet DTS ' + str(pck.dts) + ' CTS ' + str(pck.cts) + ' SAP ' + str(pck.sap) + ' dur ' + str(pck.dur) + ' size ' + str(pck.size) );\n pid.drop_packet();\n }\n return gpac.GF_OK;\n};\n//load a source\nlet my_src = fs.load_src(\"source.mp4\");\n//if needed, setup links between filters (in this example, only 2 filters explicitly loaded, no need for links)\n//run the session\nfs.run();\nCustom Forwarding example¶\nThe following defines a custom filter doing packet forwarding for input AV streams in the middle of the pipeline, exercising all possible packet creation modes (new, clone, copy, forward by ref, forward data by ref).\nlet cust_f = fs.new_filter(\"NodeJS_Test\");\n//we accept any number of input PIDs\ncust_f.set_max_pids(-1);\ncust_f.pids = [];\ncust_f.push_cap('StreamType', 'Visual', gpac.GF_CAPS_INPUT_OUTPUT);\ncust_f.push_cap('StreamType', 'Audio', gpac.GF_CAPS_INPUT_OUTPUT);\ncust_f.configure_pid = function(pid, is_remove)\n{\n if (this.pids.indexOf(pid) < 0) {\n this.pids.push(pid);\n //create output PID\n pid.opid = this.new_pid();\n pid.opid.copy_props(pid);\n } else if (is_remove) {\n console.log('PID remove !');\n } else {\n console.log('PID reconfigure !');\n pid.opid.copy_props(pid);\n }\n return gpac.GF_OK;\n}\ncust_f.pck_clone_cache = null;\ncust_f.process = function() {\n let nb_eos=0;\n this.pids.forEach(pid =>{\n if (pid.eos) {\n nb_eos++;\n return;\n }\n let pck = pid.get_packet();\n if (!pck) return;\n //send by reference\n if (this.fwd_mode==1) {\n let dst = pid.opid.new_pck_ref(pck);\n dst.copy_props(pck);\n dst.send();\n }\n //full copy mode\n else if (this.fwd_mode==2) {\n let dst = pid.opid.new_pck(pck.size);\n dst.copy_props(pck);\n new Uint8Array(dst.data).set(new Uint8Array(pck.data));\n dst.send();\n }\n //keep ref to packet and send new packet using shared data\n else if (this.fwd_mode==3) {\n let ab = new ArrayBuffer(pck.size);\n new Uint8Array(ab).set(new Uint8Array(pck.data));\n //keep ref\n pck.ref();\n let dst = pid.opid.new_pck_shared(ab, () => { pck.unref();});\n dst.copy_props(pck);\n dst.send();\n }\n //pck_clone\n else if (this.fwd_mode==4) {\n let dst = pid.opid.new_pck_clone(pck);\n dst.send();\n }\n //pck_copy\n else if (this.fwd_mode==5) {\n let dst = pid.opid.new_pck_copy(pck);\n dst.send();\n }\n //else (0) direct packet forward\n else {\n pid.opid.forward(pck);\n }\n this.fwd_mode++;\n if (this.fwd_mode==6) this.fwd_mode=0;\n pid.drop_packet();\n });\n if (nb_eos == this.pids.length)\n return gpac.GF_EOS;\n return gpac.GF_OK;\n}\n//load a source filter\nlet src=fs.load_src(\"source.mp4\");\n//load a destination filter\nlet dst=fs.load(\"vout\");\n//we need to indicate that our destination only gets its input from our custom filter !\ndst.set_source(cust_f);\n## and run\nfs.run();\nCustom GPAC callbacks¶\nSome callbacks from libgpac are made available in NodeJS\nRemotery interaction¶\nGPAC is by default compiled with Remotery support for remote profiling. \nYou can interact with Remotery websocket server by sending messages to the remote browser, or receiving messages from it:\ngpac.set_rmt_fun( text => {\n console.log('Remotery got message ' + text);\n gpac.rmt_send('Some response text');\n});\nYou will need to enable Remotery in GPAC by setting the option -rmt, as this cannot be enabled or disabled at run time.\nYou can however enable or disable Remotery profiler using gpac.rmt_enable(true).\nDASH Client¶\nYou can override the default algorithm used by the DASH client with your own algorithm. See the documentation for further details.\nThe principle is as follows:\nthe script can get notification when a period start/end to reset your stats and setup live vs on demand cases\nthe script can get notified of each created group (AdaptationSet in DASH, Variant Stream in HLS) with its various qualities. For HEVC tiling, each tile will be declared as a group, as well as the base tile track\nthe script is notified after each segment download on which quality to pickup next\nthe script can get notified while downloading a segment to decide if the download should be aborted\nlet dash_algo = {};\ndash_algo.on_period_reset = (type) => {\n console.log('period reset type ' + type);\n};\ndash_algo.on_new_group = (group) => {\n console.log('Got new group ' + JSON.stringify(group) );\n};\ndash_algo.on_rate_adaptation = (group, base_group, force_lower_complexity, stats) =>\n{\n console.log('Rate adaptation on group ' + group.idx + ' - stats ' + JSON.stringify(stats) );\n //always use lowest quality in this example\n return 0;\n};\ndash_algo.on_download_monitor = (group, stats) =>\n{\n console.log('Download monitor on group ' + group.idx + ' - stats ' + JSON.stringify(stats) );\n return -1;\n};\nlet fs = new gpac.FilterSession();\nfs.on_filter_new = (f) => {\n if (f.name == \"dashin\")\n f.bind(dash_algo);\n};\n//load a source, here to TelecomParis DASH test sequences\nlet f1 = fs.load_src(\"https://download.tsi.telecom-paristech.fr/gpac/DASH_CONFORMANCE/TelecomParisTech/mp4-live-1s/mp4-live-1s-mpd-AV-BS.mpd\");\n//load a sink, here video out\nlet f2 = fs.load(\"vout\");\n//run the session in blocking mode\nfs.run();\nHTTP Server¶\nYou can override the default behaviour of the httpout filter. See the documentation for further details.\nThe principle is as follows:\nthe script can get notification of each new request being received\nthe script can decide to let GPAC handle the request as usual (typically used for injecting http headers, throttling and monitoring)\nthe script can feed the data to GPAC (GET) or receive the data from GPAC (PUT/POST)\nlet http_req = {\n on_request = (request) => {\n console.log('got request ' + JSON.stringify(request) );\n request.reply=0;\n //throttle the connection, always delaying by 100 us\n request.throttle = function(done, total) {\n return 100;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n request.send();\n };\n};\nlet fs = new gpac.FilterSession();\nfs.on_filter_new = (f) => {\n if (f.name == \"httpout\")\n f.bind(http_req);\n};\n//load the server\nlet f2 = fs.load(\"httpout:port=8080:rdirs=.\");\n//run the session in blocking mode\nfs.run();\nThe following script always serves the same file content using nodejs instead of GPAC for GET, and monitor bytes received for PUT/POST:\nlet http_req = {\n on_request = (request) => {\n console.log('got request ' + JSON.stringify(request) );\n request.reply=200;\n this.file = filesys.openSync(\"source.mp4\", \"r\");\n request.read = function(ab) {\n let nb_bytes = 0;\n try {\n nb_bytes = filesys.readSync(this.file, ab, 0, ab.length);\n } catch (e) {\n console.log('read error: ' + e);\n return 0;\n }\n return nb_bytes;\n }\n request.write = function(ab) {\n console.log(\"got \" + ab.length + \" bytes\");\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n //send reply - this can also be done later on, e.g. in a user tasjk\n request.send();\n };\n};\nFileIO Wrapping¶\nGPAC allows usage of wrappers for file operations (open, close, read, write, seek...), and such wrappers can be constructed from NodeJS.\nA FileIO wrapper is constructed using:\nthe URL you want to wrap\na 'factory' object providing the callbacks for GPAC.\nan optional boolean indicating if direct memory should be used (default), or if array buffers are copied between GPAC and NodeJS.\nLet's define a factory that simply calls NodeJS file system calls:\nconst filesys = require('fs');\nlet fio_factory = {\n open: function(url, mode) {\n this.file = null;\n this.size = 0;\n this.position = 0;\n this.read_mode = false;\n this.is_eof = false;\n this.url = url;\n //NodeJS does not accept 't' or 'b' indicators, always assumes binary\n mode = mode.replace('b', '');\n mode = mode.replace('t', '');\n try {\n this.file = filesys.openSync(url, mode);\n } catch (e) {\n console.log('Fail to open ' + url + ' in mode ' + mode + ': ' + e);\n return false;\n }\n //file is read or append, get the file size\n if (mode.indexOf('w')<0) {\n let stats = filesys.fstatSync(this.file);\n this.size = stats.size;\n if (mode.indexOf('a+')>=0) {\n this.position = this.size;\n }\n this.read_mode = true;\n }\n return true;\n },\n close: function() {\n filesys.closeSync(this.file);\n },\n read: function(buf) {\n let nb_bytes = 0;\n try {\n nb_bytes = filesys.readSync(this.file, buf, 0, buf.length, this.position);\n } catch (e) {\n console.log('read error: ' + e);\n return 0;\n }\n if (!nb_bytes) this.is_eof = true;\n this.position += nb_bytes;\n return nb_bytes;\n },\n write: function(buf) {\n let nb_bytes = filesys.writeSync(this.file, buf, 0, buf.length, this.position);\n if (this.position == this.size) {\n this.size += nb_bytes;\n }\n this.position += nb_bytes;\n return nb_bytes;\n },\n seek: function(pos, whence) {\n this.is_eof = false;\n if (pos<0) return -1;\n //seek set\n if (whence==0) {\n this.position = pos;\n }\n //seek cur\n else if (whence==1) {\n this.position += pos;\n }\n //seek end\n else if (whence==2) {\n if (this.size < pos) return -1;\n this.position = this.size - pos;\n } else {\n return -1;\n }\n return 0;\n },\n tell: function() {\n return this.position;\n },\n eof: function() {\n return this.is_eof;\n },\n exists: function(url) {\n try {\n filesys.accessSync(url);\n } catch (err) {\n return false;\n }\n return true;\n }\n};\nYou can then wrap an input url using:\nsrc_wrap = new gpac.FileIO(\"mysource.hvc\", fio_factory);\ndst_wrap = new gpac.FileIO(\"mydest.mp4\", fio_factory);\nf1 = fs.load_src(src_wrap.url);\nf2 = fs.load_dst(dst_wrap.url+':option');\nThe wrapping can be useful when you don't want to do any IO with the produced content, or if your source content is not a file.\nWhen opening a file, a new empty object is created and the 'open' callback is called with this new object as this.\nThis allows handling, with a single wrapper, cases where a URL resolves in multiple URLs when processing, for example DASH or HLS with manifest file(s) and media segments.\nNote that all FileIO methods must be synchronous.\nNOTE When running the session in multi-thread mode, file IO callbacks are always executed by the main thread (NodeJS main).\nMultithread support¶\nMultithreaded filter sessions can be used with NodeJS, however the binding currently only supports executing callbacks into NodeJS from the main thread (main NodeJS or worker). \nA multithreaded session is created by specifying the -threads=N option to libgpac:\ngpac.set_args(['libgpac', '-threads=2']);\nThis implies a few limitations detailed below.\nRemotery handling¶\nRemotery executes in its own thread, therefore remotery messages are always queued and flushed while running a session.\nIf no filter session is active, remotery messages will not be dispatched.\nIf you want to flush these messages independently from your media sessions, the simplest way is to create a non-blocking session with a user task running until the end of your program.\ngpac.set_args(['libgpac', '-rmt']);\ngpac.set_rmt_fun( (msg) => {\n console.log('RMT got message ' + msg);\n gpac.rmt_send('ACK for ' + msg);\n}); \nlet fs_rmt = new gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\n//create a simple task running forever every 100 ms - you can return false once you are done\nlet rmt_task = {\n execute: function() { return 100; }\n};\nfs_rmt.post_task(rmt_task);\n//run as promise\nCustom filters¶\nIn multi-threaded mode, custom filters are always scheduled on the main thread. \nIf your custom filter is sending packets requiring callbacks into NodeJS, typically shared data with JS packet finalizer, these packets will force all consuming filters to be scheduled on the main thread.\nYou should therefore avoid using shared JS data in your custom filter whenever possible to ensure more efficient thread usage.\nCustom bindings¶\nIn multi-threaded mode, custom filter bindings (dashin filter for now) must be called on the main thread. \nThis implies that the filter bound will be forced to run on the main thread. Packets dispatched by a JS-bound filter may still be processed by other threads, unless they are JS shared data packets.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/nodejs/"},{"date_scraped_timestamp":1720187951339,"host":"wiki.gpac.io","page_title":"GPAC and NodeJS - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to use GPAC Filters in NodeJS. \nThis GPAC node-API provides bindings to GPAC filter session. The design is almost identical to the Python bindings, closely inspired from the JS FilterSession API used in GPAC.\nThe GPAC Node API is documented here.\nYou can also have a look at the test script in the GPAC repository.\nWarning GPAC NodeJS bindings are only available starting from GPAC 2.0.\nBefore you begin¶\nThe GPAC NodeJS bindings are using n-api for interfacing with libgpac filter session, while providing an object-oriented wrapper hiding all GPAC C design.\nThe binding is called gpac_napi.c, and is hosted in GPAC source tree. It is NOT installed during default install step.\nYou will need to build the module using node-gyp, potentially editing share/nodejs/binding.gyp as required for your system.\nThe binding.gyp provided is for GPAC:\nbuilt in regular shared library mode for libgpac (i.e. NodeJS module is not compatible with mp4box-only build)\ninstalled on your system (gpac headers available in a standard include directory, libgpac in standard lib directory), typically done with sudo make install after building gpac\nYou can then build using:\n% cd gpac/share/nodejs\n% node-gyp configure\n% node-gyp build\nIf you don't want to install on your system, you will need to modify the binding.gyp file to set the include dir to the root of gpac source tree:\n\"include_dirs\": [\"<(module_root_dir)/../../include\"]\nIf built using configure and make, you will likely have a custom config.h file, and the build tree root must also be indicated together with the GPAC_HAVE_CONFIG_H macro.\nIf you build gpac at the top of the gpac source tree (using e.g. ./configure && make), the build tree root for node will be <(module_root_dir)/../...\nIf you build gpac in gpac/bin/mytest, (using e.g. mkdir bin/mytest && cd bin/mytest && ../../configure && make), the build tree root for node will be <(module_root_dir)/../../bin/mytest.\nYou will also likely need to update the libraries property to add the path to your libgpac shared library (typically in bin/gcc of the build tree root).\n{\n \"targets\": [{\n \"target_name\": \"gpac\",\n \"sources\": [ \"./src/gpac_napi.c\"],\n \"include_dirs\": [\"<(module_root_dir)/../../include\", \"<(module_root_dir)/../..\"],\n \"libraries\": [ '-lgpac',\"-L<(module_root_dir)/../../bin/gcc\"],\n \"defines\":[\"GPAC_HAVE_CONFIG_H\"]\n }]\n}\nThe build is usually located in $GPAC_SRC/share/node/build/Release so you will need to specify the full path to the module in your JS file:\nconst gpac = require('./build/Release/gpac');\nor if you want to use the provided index.js, simply use:\nconst gpac = require(path/to/gpac/share/nodejs);\nCheck everything is in place running the following JS:\nconst gpac = require('./build/Release/gpac');\nconsole.log(\"Welcome to GPAC NodeJS !\\nVersion: \" + gpac.version);\nRunning this should print your current GPAC version. \nA test program gpac.js exercising most of the NodeJS GPAC bindings is available in gpac/share/nodejs/test\nTuning up GPAC¶\nThe first thing to do is to initialize libgpac. This is done by default while importing the bindings with the following settings:\nno memory tracking\ndefault GPAC profile used\nIf you want to change these, you need to re-init libgpac right after import:\ngpac.init(1, \"customprofile\");\nAny call other than init to GPAC will prevent any subsequent call to init to be executed.\nBefore starting any filter session, you may also need to pass some global configuration options (libgpac core or filter options) to GPAC:\n//For example, blacklist some filters, run quiet and set vsync option (for vout)\ngpac.set_args([\"gpacnode\", \"-blacklist=filters_you_do_not_want\", \"-quiet\", \"--vsync=0\"]);\nYou can also pass the command line arguments so that you can specify GPAC options at prompt:\ngpac.set_args(process.argv);\nWARNING: the arguments must all be set together, only the first call to set_args() will be taken into account\nYou may want to adjust the log tools and levels of GPAC:\ngpac.set_logs(\"dash@info\");\nSetting up filter sessions¶\nSimple sessions¶\nTo create a filter session, the simplest way is to use all defaults value, creating a single-threaded blocking session:\nfs = new gpac.FilterSession();\nYou can then add your filters as usual. \nPlayback example:\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load(\"vout\");\nfs.run();\nRemux example:\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load_dst(\"test.ts\");\nfs.run();\nSince the session is blocking, you won't be able to run any other JS code until the end of the session.\nNon-blocking sessions¶\nA non-blocking session will need to be called on regular basis to process pending filter tasks. It is useful if you need to do other tasks while the session is running and do not want to use callbacks from GPAC for that.\nfs = gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\nf1 = fs.load_src(\"file.mp4\");\nf2 = fs.load(\"vout\");\nwhile (1) {\n //do things\n //call session\n fs.run();\n //if last task, GPAC session is done\n if (fs.last_task) break;\n}\nThis allows you to do some JS work on the side, however any JS call involving promises will not work here, as promises are resolved inside NodeJS main event loop and we don't go there until the end of the session.\nAsync non-blocking sessions¶\nTo run the session in non-blocking mode while still allowing NodeJS to run its main event loop and resolve other promises, you will need to run the session as a promise. The following example is creating a promise recursively calling itself until the end of the session:\n//create session in non-blocking mode:\nlet fs = new gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\n//setup your filters\n//Run session as Promise\nconst FilterSessionPromise = (fs_run_task) => {\n var fsrun_promise = () => {\n return (fs.last_task==false) ? fs_run_task().then(fsrun_promise) : Promise.resolve();\n }\n return fsrun_promise();\n};\nconst run_task = () => {\n return new Promise((resolve, reject) => {\n resolve( fs.run_step() );\n });\n}\nFilterSessionPromise(run_task).then( ).finally( ()=> { console.log('session is done'); } ) );\nconsole.log('Entering NodeJS EventLoop');\nCallbacks in sessions¶\nRegardless of the way you run the session, you can request for being called back once or on regular basis. This is achieved by posting tasks to the GPAC session scheduler. A task object shall provide an execute method to be called. This function may return:\nfalse to cancel the task, \ntrue to reschedule the task asap\na positive integer giving the time of next task callback in milliseconds\nThe callback function is called with this set to the task object.\n//create a custom task\ntask = {};\ntask.execute = () => {\n console.log('in task'); \n return 1000;\n};\nfs.post(task);\n//run as usual\nTasks can be created at any time, either at the beginning or in a callback function (e.g., another task).\nNOTE When running the session in multi-thread mode, callback tasks are always executed by the main thread (NodeJS main).\nLinking filters¶\nIn order to link filters when desired, you must explicitly do this using set_source of the destination filter. For example, when inserting a reframer in a chain:\nf_src = fs.load_src('source.mp4');\nf_dst = fs.load_dst('remux.mp4');\nf_reframe = fs.load('reframer');\nf_dst.set_source(reframer);\nYou can specify the usual link filtering as an optional argument to set_source:\nf_dst.set_source(reframer, \"#PID=1\");\nThis will instruct that the destination only accepts PIDs coming from the reframer filter, and with ID 1.\nInspecting filters¶\nYou can query the number of input and output PIDs of a filter, the source filter of an input PID, the destination filters of an output PID, their possible options, update options, send events, ...\nPlease check the API documentation and refer to the NodeJS example.\nNote that (as in GPAC JS or Python) properties referring to constant values are not exposed as their native types but as strings. This is the case for these important types:\nStreamType: string containing the stream type name (e.g. 'Visual', 'Audio', ...)\nCodecID: string containing the codec name\nPixelFormat: string containing the pixel format name\nAudioFormat: string containing the audio format name\nCustom Filters¶\nYou can define your own filter(s) to interact with the media pipeline. As usual in GPAC filters, a custom filter can be a source, a sink or any other filter. It can consume packets from input PIDs and produce packets on output PIDs. \nCustom filters are created through the new_filter function of the filter session object. The custom filter can then assign its callbacks functions:\nGF_Err process() method called whenever the filter has some data to process.\nGF_Err configure_pid(pid, is_remove) method called whenever a new PID must be configured, re-configured or removed in the custom filter\nBool process_event(evt) method called whenever an event is passing through the filter or one of its PIDs\nGF_Err reconfigure_output(pid) method called whenever an output PID should be reconfigured\nThese callbacks are all optional, but process should be set if you want your filter to perform any action.\nFilters accepting input PIDs and/or producing output PIDs must configure their capabilities using push_cap function.\nNOTE When running the session in multi-thread mode, custom filter callbacks are always executed by the main thread (NodeJS main).\nCustom Sink example¶\nThe following defines a custom filter doing simple inspection of the pipeline (sink filter) \nlet cust = fs.new_filter('MyFilterJS');\ncust.ipids=[];\n//indicate what we accept and produce - this can be done ether in the constructor or after, but before running the session\n//here we only accept video streams as input, and do not produce any output\ncust.push_cap(\"StreamType\", \"Visual\", gpac.GF_CAPS_INPUT);\n//we accept one or more input video PID, we must configure them\ncust.configure_pid = (pid, is_remove) => {\n if (is_remove) {\n console.log('PID removed !');\n return gpac.GF_OK;\n }\n //PID is already registered with our filter, this is a reconfiguration\n if (this.ipids.indexOf(pid) < 0) {\n this.ipids.push(pid);\n console.log('PID initial configure !');\n } else {\n console.log('PID reconfigure !');\n }\n //enumerate all props \n pid.enum_props( (type, val) => {\n console.log('\\t' + type + ': ' + val);\n });\n //we are a sink, we MUST fire a play event\n evt = new gpac.FilterEvent(gpac.GF_FEVT_PLAY);\n pid.send_event(evt);\n return gpac.GF_OK;\n};\ncust.process() = () => {\n this.ipids.forEach(pid => {\n pck = pid.get_packet();\n if (!pck) break;\n console.log('Got Packet DTS ' + str(pck.dts) + ' CTS ' + str(pck.cts) + ' SAP ' + str(pck.sap) + ' dur ' + str(pck.dur) + ' size ' + str(pck.size) );\n pid.drop_packet();\n }\n return gpac.GF_OK;\n};\n//load a source\nlet my_src = fs.load_src(\"source.mp4\");\n//if needed, setup links between filters (in this example, only 2 filters explicitly loaded, no need for links)\n//run the session\nfs.run();\nCustom Forwarding example¶\nThe following defines a custom filter doing packet forwarding for input AV streams in the middle of the pipeline, exercising all possible packet creation modes (new, clone, copy, forward by ref, forward data by ref).\nlet cust_f = fs.new_filter(\"NodeJS_Test\");\n//we accept any number of input PIDs\ncust_f.set_max_pids(-1);\ncust_f.pids = [];\ncust_f.push_cap('StreamType', 'Visual', gpac.GF_CAPS_INPUT_OUTPUT);\ncust_f.push_cap('StreamType', 'Audio', gpac.GF_CAPS_INPUT_OUTPUT);\ncust_f.configure_pid = function(pid, is_remove)\n{\n if (this.pids.indexOf(pid) < 0) {\n this.pids.push(pid);\n //create output PID\n pid.opid = this.new_pid();\n pid.opid.copy_props(pid);\n } else if (is_remove) {\n console.log('PID remove !');\n } else {\n console.log('PID reconfigure !');\n pid.opid.copy_props(pid);\n }\n return gpac.GF_OK;\n}\ncust_f.pck_clone_cache = null;\ncust_f.process = function() {\n let nb_eos=0;\n this.pids.forEach(pid =>{\n if (pid.eos) {\n nb_eos++;\n return;\n }\n let pck = pid.get_packet();\n if (!pck) return;\n //send by reference\n if (this.fwd_mode==1) {\n let dst = pid.opid.new_pck_ref(pck);\n dst.copy_props(pck);\n dst.send();\n }\n //full copy mode\n else if (this.fwd_mode==2) {\n let dst = pid.opid.new_pck(pck.size);\n dst.copy_props(pck);\n new Uint8Array(dst.data).set(new Uint8Array(pck.data));\n dst.send();\n }\n //keep ref to packet and send new packet using shared data\n else if (this.fwd_mode==3) {\n let ab = new ArrayBuffer(pck.size);\n new Uint8Array(ab).set(new Uint8Array(pck.data));\n //keep ref\n pck.ref();\n let dst = pid.opid.new_pck_shared(ab, () => { pck.unref();});\n dst.copy_props(pck);\n dst.send();\n }\n //pck_clone\n else if (this.fwd_mode==4) {\n let dst = pid.opid.new_pck_clone(pck);\n dst.send();\n }\n //pck_copy\n else if (this.fwd_mode==5) {\n let dst = pid.opid.new_pck_copy(pck);\n dst.send();\n }\n //else (0) direct packet forward\n else {\n pid.opid.forward(pck);\n }\n this.fwd_mode++;\n if (this.fwd_mode==6) this.fwd_mode=0;\n pid.drop_packet();\n });\n if (nb_eos == this.pids.length)\n return gpac.GF_EOS;\n return gpac.GF_OK;\n}\n//load a source filter\nlet src=fs.load_src(\"source.mp4\");\n//load a destination filter\nlet dst=fs.load(\"vout\");\n//we need to indicate that our destination only gets its input from our custom filter !\ndst.set_source(cust_f);\n## and run\nfs.run();\nCustom GPAC callbacks¶\nSome callbacks from libgpac are made available in NodeJS\nRemotery interaction¶\nGPAC is by default compiled with Remotery support for remote profiling. \nYou can interact with Remotery websocket server by sending messages to the remote browser, or receiving messages from it:\ngpac.set_rmt_fun( text => {\n console.log('Remotery got message ' + text);\n gpac.rmt_send('Some response text');\n});\nYou will need to enable Remotery in GPAC by setting the option -rmt, as this cannot be enabled or disabled at run time.\nYou can however enable or disable Remotery profiler using gpac.rmt_enable(true).\nDASH Client¶\nYou can override the default algorithm used by the DASH client with your own algorithm. See the documentation for further details.\nThe principle is as follows:\nthe script can get notification when a period start/end to reset your stats and setup live vs on demand cases\nthe script can get notified of each created group (AdaptationSet in DASH, Variant Stream in HLS) with its various qualities. For HEVC tiling, each tile will be declared as a group, as well as the base tile track\nthe script is notified after each segment download on which quality to pickup next\nthe script can get notified while downloading a segment to decide if the download should be aborted\nlet dash_algo = {};\ndash_algo.on_period_reset = (type) => {\n console.log('period reset type ' + type);\n};\ndash_algo.on_new_group = (group) => {\n console.log('Got new group ' + JSON.stringify(group) );\n};\ndash_algo.on_rate_adaptation = (group, base_group, force_lower_complexity, stats) =>\n{\n console.log('Rate adaptation on group ' + group.idx + ' - stats ' + JSON.stringify(stats) );\n //always use lowest quality in this example\n return 0;\n};\ndash_algo.on_download_monitor = (group, stats) =>\n{\n console.log('Download monitor on group ' + group.idx + ' - stats ' + JSON.stringify(stats) );\n return -1;\n};\nlet fs = new gpac.FilterSession();\nfs.on_filter_new = (f) => {\n if (f.name == \"dashin\")\n f.bind(dash_algo);\n};\n//load a source, here to TelecomParis DASH test sequences\nlet f1 = fs.load_src(\"https://download.tsi.telecom-paristech.fr/gpac/DASH_CONFORMANCE/TelecomParisTech/mp4-live-1s/mp4-live-1s-mpd-AV-BS.mpd\");\n//load a sink, here video out\nlet f2 = fs.load(\"vout\");\n//run the session in blocking mode\nfs.run();\nHTTP Server¶\nYou can override the default behaviour of the httpout filter. See the documentation for further details.\nThe principle is as follows:\nthe script can get notification of each new request being received\nthe script can decide to let GPAC handle the request as usual (typically used for injecting http headers, throttling and monitoring)\nthe script can feed the data to GPAC (GET) or receive the data from GPAC (PUT/POST)\nlet http_req = {\n on_request = (request) => {\n console.log('got request ' + JSON.stringify(request) );\n request.reply=0;\n //throttle the connection, always delaying by 100 us\n request.throttle = function(done, total) {\n return 100;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n request.send();\n };\n};\nlet fs = new gpac.FilterSession();\nfs.on_filter_new = (f) => {\n if (f.name == \"httpout\")\n f.bind(http_req);\n};\n//load the server\nlet f2 = fs.load(\"httpout:port=8080:rdirs=.\");\n//run the session in blocking mode\nfs.run();\nThe following script always serves the same file content using nodejs instead of GPAC for GET, and monitor bytes received for PUT/POST:\nlet http_req = {\n on_request = (request) => {\n console.log('got request ' + JSON.stringify(request) );\n request.reply=200;\n this.file = filesys.openSync(\"source.mp4\", \"r\");\n request.read = function(ab) {\n let nb_bytes = 0;\n try {\n nb_bytes = filesys.readSync(this.file, ab, 0, ab.length);\n } catch (e) {\n console.log('read error: ' + e);\n return 0;\n }\n return nb_bytes;\n }\n request.write = function(ab) {\n console.log(\"got \" + ab.length + \" bytes\");\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n //send reply - this can also be done later on, e.g. in a user tasjk\n request.send();\n };\n};\nFileIO Wrapping¶\nGPAC allows usage of wrappers for file operations (open, close, read, write, seek...), and such wrappers can be constructed from NodeJS.\nA FileIO wrapper is constructed using:\nthe URL you want to wrap\na 'factory' object providing the callbacks for GPAC.\nan optional boolean indicating if direct memory should be used (default), or if array buffers are copied between GPAC and NodeJS.\nLet's define a factory that simply calls NodeJS file system calls:\nconst filesys = require('fs');\nlet fio_factory = {\n open: function(url, mode) {\n this.file = null;\n this.size = 0;\n this.position = 0;\n this.read_mode = false;\n this.is_eof = false;\n this.url = url;\n //NodeJS does not accept 't' or 'b' indicators, always assumes binary\n mode = mode.replace('b', '');\n mode = mode.replace('t', '');\n try {\n this.file = filesys.openSync(url, mode);\n } catch (e) {\n console.log('Fail to open ' + url + ' in mode ' + mode + ': ' + e);\n return false;\n }\n //file is read or append, get the file size\n if (mode.indexOf('w')<0) {\n let stats = filesys.fstatSync(this.file);\n this.size = stats.size;\n if (mode.indexOf('a+')>=0) {\n this.position = this.size;\n }\n this.read_mode = true;\n }\n return true;\n },\n close: function() {\n filesys.closeSync(this.file);\n },\n read: function(buf) {\n let nb_bytes = 0;\n try {\n nb_bytes = filesys.readSync(this.file, buf, 0, buf.length, this.position);\n } catch (e) {\n console.log('read error: ' + e);\n return 0;\n }\n if (!nb_bytes) this.is_eof = true;\n this.position += nb_bytes;\n return nb_bytes;\n },\n write: function(buf) {\n let nb_bytes = filesys.writeSync(this.file, buf, 0, buf.length, this.position);\n if (this.position == this.size) {\n this.size += nb_bytes;\n }\n this.position += nb_bytes;\n return nb_bytes;\n },\n seek: function(pos, whence) {\n this.is_eof = false;\n if (pos<0) return -1;\n //seek set\n if (whence==0) {\n this.position = pos;\n }\n //seek cur\n else if (whence==1) {\n this.position += pos;\n }\n //seek end\n else if (whence==2) {\n if (this.size < pos) return -1;\n this.position = this.size - pos;\n } else {\n return -1;\n }\n return 0;\n },\n tell: function() {\n return this.position;\n },\n eof: function() {\n return this.is_eof;\n },\n exists: function(url) {\n try {\n filesys.accessSync(url);\n } catch (err) {\n return false;\n }\n return true;\n }\n};\nYou can then wrap an input url using:\nsrc_wrap = new gpac.FileIO(\"mysource.hvc\", fio_factory);\ndst_wrap = new gpac.FileIO(\"mydest.mp4\", fio_factory);\nf1 = fs.load_src(src_wrap.url);\nf2 = fs.load_dst(dst_wrap.url+':option');\nThe wrapping can be useful when you don't want to do any IO with the produced content, or if your source content is not a file.\nWhen opening a file, a new empty object is created and the 'open' callback is called with this new object as this.\nThis allows handling, with a single wrapper, cases where a URL resolves in multiple URLs when processing, for example DASH or HLS with manifest file(s) and media segments.\nNote that all FileIO methods must be synchronous.\nNOTE When running the session in multi-thread mode, file IO callbacks are always executed by the main thread (NodeJS main).\nMultithread support¶\nMultithreaded filter sessions can be used with NodeJS, however the binding currently only supports executing callbacks into NodeJS from the main thread (main NodeJS or worker). \nA multithreaded session is created by specifying the -threads=N option to libgpac:\ngpac.set_args(['libgpac', '-threads=2']);\nThis implies a few limitations detailed below.\nRemotery handling¶\nRemotery executes in its own thread, therefore remotery messages are always queued and flushed while running a session.\nIf no filter session is active, remotery messages will not be dispatched.\nIf you want to flush these messages independently from your media sessions, the simplest way is to create a non-blocking session with a user task running until the end of your program.\ngpac.set_args(['libgpac', '-rmt']);\ngpac.set_rmt_fun( (msg) => {\n console.log('RMT got message ' + msg);\n gpac.rmt_send('ACK for ' + msg);\n}); \nlet fs_rmt = new gpac.FilterSession(gpac.GF_FS_FLAG_NON_BLOCKING);\n//create a simple task running forever every 100 ms - you can return false once you are done\nlet rmt_task = {\n execute: function() { return 100; }\n};\nfs_rmt.post_task(rmt_task);\n//run as promise\nCustom filters¶\nIn multi-threaded mode, custom filters are always scheduled on the main thread. \nIf your custom filter is sending packets requiring callbacks into NodeJS, typically shared data with JS packet finalizer, these packets will force all consuming filters to be scheduled on the main thread.\nYou should therefore avoid using shared JS data in your custom filter whenever possible to ensure more efficient thread usage.\nCustom bindings¶\nIn multi-threaded mode, custom filter bindings (dashin filter for now) must be called on the main thread. \nThis implies that the filter bound will be forced to run on the main thread. Packets dispatched by a JS-bound filter may still be processed by other threads, unless they are JS shared data packets.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/nodejs/?q="},{"date_scraped_timestamp":1720187960178,"host":"wiki.gpac.io","page_title":"GPAC developer guide - GPAC wiki","text":"\n \n \n \n \n \n \nContributing¶\nA complex project like GPAC wouldn’t exist and persist without the support of its community. Please contribute: a nice message, supporting us in our communication, reporting issues when you see them … any gesture, even the smallest ones, counts. \nGetting support and reporting issues¶\nPlease use github issues for feature requests and bug reports. When filing a request there, please tag it as feature-request.\nAPI documentation¶\nThe API documentation provides information on the GPAC Filter API.\nGPAC's core is writen in C, but it can be easily extended using Javascipt Filters, used in a Python or NodeJS application.\nTutorials¶\nIntro to Filter Session\nWriting a custom Filter\nBuilding¶\nDetailed build Build instructions for MP4Box and GPAC on all supported platforms.\nTesting¶\nLearn how to build and run GPAC's test suite.\nThe testsuite scripts is always a good place to understand GPAC tools usage.\nContinuous integration¶\nGPAC is continuously built and tested through a buildbot server:\nBuild status of GPAC: buildbot.gpac.io\nTests status of GPAC: tests.gpac.io\nArchives¶\ntips and tricks in our github discussions and our old sourceforge forums\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Developers/"},{"date_scraped_timestamp":1720187970158,"host":"wiki.gpac.io","page_title":"GPAC developer guide - GPAC wiki","text":"\n \n \n \n \n \n \nContributing¶\nA complex project like GPAC wouldn’t exist and persist without the support of its community. Please contribute: a nice message, supporting us in our communication, reporting issues when you see them … any gesture, even the smallest ones, counts. \nGetting support and reporting issues¶\nPlease use github issues for feature requests and bug reports. When filing a request there, please tag it as feature-request.\nAPI documentation¶\nThe API documentation provides information on the GPAC Filter API.\nGPAC's core is writen in C, but it can be easily extended using Javascipt Filters, used in a Python or NodeJS application.\nTutorials¶\nIntro to Filter Session\nWriting a custom Filter\nBuilding¶\nDetailed build Build instructions for MP4Box and GPAC on all supported platforms.\nTesting¶\nLearn how to build and run GPAC's test suite.\nThe testsuite scripts is always a good place to understand GPAC tools usage.\nContinuous integration¶\nGPAC is continuously built and tested through a buildbot server:\nBuild status of GPAC: buildbot.gpac.io\nTests status of GPAC: tests.gpac.io\nArchives¶\ntips and tricks in our github discussions and our old sourceforge forums\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Developers/?q="},{"date_scraped_timestamp":1720187892648,"host":"wiki.gpac.io","page_title":"GPAC wiki","text":"\nNote\nThe documentation assumes GPAC 2.0 or above. Some informations relative to MP4Box may still be useful for older versions.\n.","url":"https://wiki.gpac.io"},{"date_scraped_timestamp":1720187991566,"host":"wiki.gpac.io","page_title":"GPAC wiki","text":"\nNote\nThe documentation assumes GPAC 2.0 or above. Some informations relative to MP4Box may still be useful for older versions.\n.","url":"https://wiki.gpac.io?q="},{"date_scraped_timestamp":1720187977870,"host":"wiki.gpac.io","page_title":"Global options - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC Core Options¶\nlibgpac core options:¶\n-noprog: disable progress messages\n-quiet: disable all messages, including errors\n-proglf: use new line at each progress messages\n-strict-error,-se: exit after the first error is reported\n-store-dir (string): set storage directory\n-mod-dirs (string list): set additional module directories as a semi-colon ; separated list\n-js-dirs (string list): set javascript directories\n-no-js-mods (string list): disable javascript module loading\n-ifce (string): set default multicast interface (default is ANY), either an IP address or a device name as listed by gpac -h net. Prefix '+' will force using IPv6 for dual interface\n-lang (string): set preferred language\n-cfg,-opt (string): get or set configuration file value. The string parameter can be formatted as: \nsection:key=val: set the key to a new value \nsection:key=null, section:key: remove the key \nsection=null: remove the section \nno argument: print the entire configuration file \nsection: print the given section \nsection:key: print the given key in section (section can be set to *)- *:key: print the given key in all sections \n-no-save: discard any changes made to the config file upon exit\n-mod-reload: unload / reload module shared libs when no longer used\n-for-test: disable all creation/modification dates and GPAC versions in files\n-old-arch: enable compatibility with pre-filters versions of GPAC\n-ntp-shift (int): shift NTP clock by given amount in seconds\n-bs-cache-size (int, default: 512): cache size for bitstream read and write from file (0 disable cache, slower IOs)\n-no-check: disable compliance tests for inputs (ISOBMFF for now). This will likely result in random crashes\n-unhandled-rejection: dump unhandled promise rejections\n-startup-file (string): startup file of compositor in GUI mode\n-docs-dir (string): default documents directory (for GUI on iOS and Android)\n-last-dir (string): last working directory (for GUI)\n-no-poll: disable poll and use select for socket groups\n-no-tls-rcfg: disable automatic TCP to TLS reconfiguration\n-no-fd: use buffered IO instead of file descriptor for read/write - this can speed up operations on small files \n-no-mx: disable all mutexes, threads and semaphores (do not use if unsure about threading used)\n-xml-max-csize (int, default: 100k): maximum XML content or attribute size\n-netcap (string): set packet capture and filtering rules formatted as [CFG][RULES]. Each -netcap argument will define a configuration\n[CFG] is an optional comma-separated list of: \nid=ID: ID (string) for this configuration. If NULL, configuration will apply to all sockets not specifying a netcap ID \nsrc=F: read packets from F, as produced by GPAC or a pcap or pcapng file \ndst=F: output packets to F (GPAC or pcap/pcapng file), cannot be set if src is set \nloop[=N]: loop capture file N times, or forever if N is not set or negative \nnrt: disable real-time playback \n[RULES] is an optional list of [OPT,OPT2...] with OPT in: \nm=K: set rule mode - K can be r for reception only (default), w for send only or rw for both \ns=K: set packet start range to K \ne=K: set packet end range to K - only used for r and f rules, 0 or not set means rule apply until end \nn=K: set number of packets to drop to K - not set, 0 or 1 means single packet \nr=K: random drop n packet every K \nf=K: drop first n packets every K \nd=K: reorder n packets after the next K packets, can be used with f or r rules \np=K: filter packets on port K only, if not set the rule applies to all packets \no=K: patch packet instead of droping (always true for TCP), replacing byte at offset K (0 is first byte, <0 for random) \nv=K: set patch byte value to K (hexa) or negative value for random (default) \nExample\nThis will record packets to dump.gpc \nExample\n-netcap=src=dump.gpc,id=NC1 -i session1.sdp:NCID=NC1 -i session2.sdp\nThis will read packets from dump.gpc only for session1.sdp and let session2.sdp use regular sockets \nExample\n-netcap=[p=1234,s=100,n=20][r=200,s=500,o=10,v=FE]\nThis will use regular network interface and drop packets 100 to 119 on port 1234 and patch one random packet every 200 starting from packet 500, setting byte 10 to FE \n-cache (string): cache directory location\n-proxy (string): set HTTP proxy server address and port\n-maxrate (int): set max HTTP download rate in bits per sec. 0 means unlimited\n-no-cache: disable HTTP caching\n-offline-cache: enable offline HTTP caching (no re-validation of existing resource in cache)\n-clean-cache: indicate if HTTP cache should be clean upon launch/exit\n-cache-size (int, default: 100M): specify cache size in bytes\n-tcp-timeout (int, default: 5000): time in milliseconds to wait for HTTP/RTSP connect before error\n-req-timeout (int, default: 10000): time in milliseconds to wait on HTTP/RTSP request before error (0 disables timeout)\n-no-timeout: ignore HTTP 1.1 timeout in keep-alive\n-broken-cert: enable accepting broken SSL certificates\n-user-agent,-ua (string): set user agent name for HTTP/RTSP\n-user-profileid (string): set user profile ID (through X-UserProfileID entity header) in HTTP requests\n-user-profile (string): set user profile filename. Content of file is appended as body to HTTP HEAD/GET requests, associated Mime is text/xml\n-query-string (string): insert query string (without ?) to URL on requests\n-dm-threads: force using threads for async download requests rather than session scheduler\n-cte-rate-wnd (int, default: 20): set window analysis length in milliseconds for chunk-transfer encoding rate estimation\n-cred (string): path to 128 bits key for credential storage\n-no-h2: disable HTTP2\n-no-h2c: disable HTTP2 upgrade (i.e. over non-TLS)\n-h2-copy: enable intermediate copy of data in nghttp2 (default is disabled but may report as broken frames in wireshark)\n-curl: use CURL instead of GPAC HTTP stack\n-no-h3: disable HTTP3 (CURL only)\n-dbg-edges: log edges status in filter graph before dijkstra resolution (for debug). Edges are logged as edge_source(status(disable_depth), weight, src_cap_idx -> dst_cap_idx)\n-full-link: throw error if any PID in the filter graph cannot be linked\n-no-dynf: disable dynamically loaded filters\n-no-block (Enum, default: no): disable blocking mode of filters \nno: enable blocking mode \nfanout: disable blocking on fan-out, unblocking the PID as soon as one of its destinations requires a packet \nall: disable blocking \n-no-reg: disable regulation (no sleep) in session\n-no-reassign: disable source filter reassignment in PID graph resolution\n-sched (Enum, default: free): set scheduler mode \nfree: lock-free queues except for task list (default) \nlock: mutexes for queues when several threads \nfreex: lock-free queues including for task lists (experimental) \nflock: mutexes for queues even when no thread (debug mode) \ndirect: no threads and direct dispatch of tasks whenever possible (debug mode) \n-max-chain (int, default: 6): set maximum chain length when resolving filter links. Default value covers for [ in -> ] dmx -> reframe -> decode -> encode -> reframe -> mx [ -> out]. Filter chains loaded for adaptation (e.g. pixel format change) are loaded after the link resolution. Setting the value to 0 disables dynamic link resolution. You will have to specify the entire chain manually\n-max-sleep (int, default: 50): set maximum sleep time slot in milliseconds when regulation is enabled\n-step-link: load filters one by one when solvink a link instead of loading all filters for the solved path\n-threads (int): set N extra thread for the session. -1 means use all available cores\n-no-probe: disable data probing on sources and relies on extension (faster load but more error-prone)\n-no-argchk: disable tracking of argument usage (all arguments will be considered as used)\n-blacklist (string): blacklist the filters listed in the given string (comma-separated list). If first character is '-', this is a whitelist, i.e. only filters listed in the given string will be allowed\n-no-graph-cache: disable internal caching of filter graph connections. If disabled, the graph will be recomputed at each link resolution (lower memory usage but slower)\n-no-reservoir: disable memory recycling for packets and properties. This uses much less memory but stresses the system memory allocator much more\n-buffer-gen (int, default: 1000): default buffer size in microseconds for generic pids\n-buffer-dec (int, default: 1000000): default buffer size in microseconds for decoder input pids\n-buffer-units (int, default: 1): default buffer size in frames when timing is not available\n-gl-bits-comp (int, default: 8): number of bits per color component in OpenGL\n-gl-bits-depth (int, default: 16): number of bits for depth buffer in OpenGL\n-gl-doublebuf: enable OpenGL double buffering\n-glfbo-txid (int): set output texture ID when using glfbo output. The OpenGL context shall be initialized and gf_term_process shall be called with the OpenGL context active\n-video-output (string): indicate the name of the video output module to use (see gpac -h modules). The reserved name glfbo is used in player mode to draw in the OpenGL texture identified by glfbo-txid. In this mode, the application is responsible for sending event to the compositor\n-audio-output (string): indicate the name of the audio output module to use\n-font-reader (string): indicate name of font reader module\n-font-dirs (string): indicate comma-separated list of directories to scan for fonts\n-rescan-fonts: indicate the font directory must be rescanned\n-wait-fonts: wait for SVG fonts to be loaded before displaying frames\n-webvtt-hours: force writing hour when serializing WebVTT\n-charset (string): set charset when not recognized from input. Possible values are: \nutf8: force UTF-8 \nutf16: force UTF-16 little endian \nutf16be: force UTF-16 big endian \nother: attempt to parse anyway \n-rmt: enable profiling through Remotery. A copy of Remotery visualizer is in gpac/share/vis, usually installed in /usr/share/gpac/vis or Program Files/GPAC/vis\n-rmt-port (int, default: 17815): set remotery port\n-rmt-reuse: allow remotery to reuse port\n-rmt-localhost: make remotery only accepts localhost connection\n-rmt-sleep (int, default: 10): set remotery sleep (ms) between server updates\n-rmt-nmsg (int, default: 10): set remotery number of messages per update\n-rmt-qsize (int, default: 131072): set remotery message queue size in bytes\n-rmt-log: redirect logs to remotery (experimental, usually not well handled by browser)\n-rmt-ogl: make remotery sample opengl calls\n-m2ts-vvc-old: hack for old TS streams using 0x32 for VVC instead of 0x33\n-piff-force-subsamples: hack for PIFF PSEC files generated by 0.9.0 and 1.0 MP4Box with wrong subsample_count inserted for audio\n-vvdec-annexb: hack for old vvdec+libavcodec supporting only annexB format\n-heif-hevc-urn: use HEVC URN for alpha and depth in HEIF instead of MPEG-B URN (HEIF first edition)\n-boxdir (string): use box definitions in the given directory for XML dump \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/core_options/"},{"date_scraped_timestamp":1720187962445,"host":"wiki.gpac.io","page_title":"HEIF creation - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to import items to create HEIF file collections. For track import, use the regular tools from GPAC.\nImporting images¶\nYou can use MP4Box to manually import each item, see MP4Box -h meta.\nWhen importing images, the first image imported will be set as primary item if the destination file has no primary item.\nTo import a single key frame src.hvc source:\nMP4Box -add-image src.hvc:primary -new image.heic\nTo import a key frame at a given time from a source video:\nMP4Box -add-image src.hvc:primary:time=11.5 -new image.heic\nMP4Box can also import several images at once.\nTo import all key frames from a source video:\nMP4Box -add-image src.hvc:time=-1 -new image.heic\nTo import all key frames start time 3 sec to end time 22.5 second in a source video:\nMP4Box -add-image src.hvc:primary:time=3-22.5 -new image.heic\nTo import all key frames every 30 seconds in a source video:\nMP4Box -add-image src.hvc:primary:time=-1/30 -new image.heic\nTo import all key frames every 30 seconds from start time 3 sec to end time 3min in a source video:\nMP4Box -add-image src.hvc:primary:time=3-180/30 -new image.heic\nFiltering while importing¶\nMP4Box can be used together with filters in gpac, as discussed here. This section illustrates how this feature can be used in various use cases.\nImporting a subset of frames¶\nThe following example import frames 1, 12 and 15 from a source sequence using the reframer filter:\nMP4Box -add-image src.hvc:primary@@reframer:frames=1,12,15 -new image.heic\nImporting and transcoding¶\nThe following example imports a JPG image and transcodes it to HEVC at 5 mbps:\nMP4Box -add-image src.jpg:primary@@enc:c=avc:b=5m -new image.heic\nCreating images in a file with video¶\nMP4Box can create images from a file with one or more video tracks and save the combined video+items.\nThe syntax used is the same as the examples above except the source file is not set.\nThe video track to import from can be set using tkID option, otherwise the first video track will be used.\nTo import a key frame at a given sample position as item:\nMP4Box -add-image samp=26 source.heif\nTo import all key frames as items position (here renaming the output file from mp4 to heif):\nMP4Box -add-image time=-1 source.mp4 -out images_vid.heic\nIn the above examples, the video data is copied to the new item, which increases the file size. \nYou can change that by specifying that the item is a reference to the sample data using ref:\nMP4Box -add-image ref:time=-1/30 image.heic\nIn this example, the first key frame of every 30s window of the source track will be added as an item sharing the data with the track sample.\nCreating grids¶\nGrids can be created using add-image-grid, as illustrated in these tests.\nThere is also a quick grid creation option called agrid which automatically computes a grid from the items present in the file. \nIt will hide all these items and make the grid a primary item, resulting in a thumbnail-like grid picture.\nAll images in the file must have the same width and height. If the number of images is not even, the last image will be ignored. \nMP4Box -add-image long_video.mp4:time=-1/30 -add-image agrid -new thumbnails.heif\nThis will create a new file containing the first key frame of every 30s window of long_video.mp4 and add a grid representing all these items.\nThe grid aspect ratio can be hinted using agrid=AR , with AR the target aspect ratio. There is no guarantee that the AR will be respected since this depends on the image source sizes and number.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/heif-import/"},{"date_scraped_timestamp":1720188062028,"host":"wiki.gpac.io","page_title":"HEIF creation - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to import items to create HEIF file collections. For track import, use the regular tools from GPAC.\nImporting images¶\nYou can use MP4Box to manually import each item, see MP4Box -h meta.\nWhen importing images, the first image imported will be set as primary item if the destination file has no primary item.\nTo import a single key frame src.hvc source:\nMP4Box -add-image src.hvc:primary -new image.heic\nTo import a key frame at a given time from a source video:\nMP4Box -add-image src.hvc:primary:time=11.5 -new image.heic\nMP4Box can also import several images at once.\nTo import all key frames from a source video:\nMP4Box -add-image src.hvc:time=-1 -new image.heic\nTo import all key frames start time 3 sec to end time 22.5 second in a source video:\nMP4Box -add-image src.hvc:primary:time=3-22.5 -new image.heic\nTo import all key frames every 30 seconds in a source video:\nMP4Box -add-image src.hvc:primary:time=-1/30 -new image.heic\nTo import all key frames every 30 seconds from start time 3 sec to end time 3min in a source video:\nMP4Box -add-image src.hvc:primary:time=3-180/30 -new image.heic\nFiltering while importing¶\nMP4Box can be used together with filters in gpac, as discussed here. This section illustrates how this feature can be used in various use cases.\nImporting a subset of frames¶\nThe following example import frames 1, 12 and 15 from a source sequence using the reframer filter:\nMP4Box -add-image src.hvc:primary@@reframer:frames=1,12,15 -new image.heic\nImporting and transcoding¶\nThe following example imports a JPG image and transcodes it to HEVC at 5 mbps:\nMP4Box -add-image src.jpg:primary@@enc:c=avc:b=5m -new image.heic\nCreating images in a file with video¶\nMP4Box can create images from a file with one or more video tracks and save the combined video+items.\nThe syntax used is the same as the examples above except the source file is not set.\nThe video track to import from can be set using tkID option, otherwise the first video track will be used.\nTo import a key frame at a given sample position as item:\nMP4Box -add-image samp=26 source.heif\nTo import all key frames as items position (here renaming the output file from mp4 to heif):\nMP4Box -add-image time=-1 source.mp4 -out images_vid.heic\nIn the above examples, the video data is copied to the new item, which increases the file size. \nYou can change that by specifying that the item is a reference to the sample data using ref:\nMP4Box -add-image ref:time=-1/30 image.heic\nIn this example, the first key frame of every 30s window of the source track will be added as an item sharing the data with the track sample.\nCreating grids¶\nGrids can be created using add-image-grid, as illustrated in these tests.\nThere is also a quick grid creation option called agrid which automatically computes a grid from the items present in the file. \nIt will hide all these items and make the grid a primary item, resulting in a thumbnail-like grid picture.\nAll images in the file must have the same width and height. If the number of images is not even, the last image will be ignored. \nMP4Box -add-image long_video.mp4:time=-1/30 -add-image agrid -new thumbnails.heif\nThis will create a new file containing the first key frame of every 30s window of long_video.mp4 and add a grid representing all these items.\nThe grid aspect ratio can be hinted using agrid=AR , with AR the target aspect ratio. There is no guarantee that the AR will be respected since this depends on the image source sizes and number.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/heif-import/?q="},{"date_scraped_timestamp":1720187918426,"host":"wiki.gpac.io","page_title":"HEVC Tile Multi-Resolution Adaptation Guide","text":"\n \n \n \n \n \n \nThis page gives a quick walk-through on how to build and play tiled HEVC sequences with multiple resolutions\nForeword¶\nUsing multiple resolutions for tile-based streaming is quite different from tile-track approach covered here.\nAlthough the content preparation is roughly the same, combining tiles of different resolutions requires slice header and SPS/PPS rewrite, hence tile tracks cannot be used here.\nContent Preparation¶\nCheck the HEVC Tile-based adaptation guide for content preparation.\nWe recommend using source videos with \n- the same aspect ration (regular dash requirement)\n- multiple of 64 (or max CU size if you tweek the encoder) for both width and height. Not doing so will likely result in decoding artefacts of the merged bitstream\nPackaging your video¶\nYou will need to package your raw HEVC bitstream, rewriting each tile as a stand-alone HEVC bitstream using the hevcsplit filter:\nMP4Box -add video_tiled.hvc:@hevcsplit -new video_tiled.mp4\nOr using gpac:\ngpac -i video_tiled.hvc hevcsplit -o video_tiled.mp4\nFor a NxM tiling, the resulting file will contain NxM independent HEVC video tracks.\nYou can view the result using:\ngpac -i video_tiled.mp4 hevcmerge vout\nYou can check the motion constrained is well respected by removing a tile track from the file:\nMP4Box -rem 4 video_tiled.mp4 -out test_tile_lost.mp4\ngpac -i test_tile_lost.mp4 hevcmerge vout\nDASHing your video¶\nYour video can be DASHed as any other video with MP4Box, for example\nMP4Box -dash 1000 -profile live -out dash_tiled.mpd video_tiled.mp4\nor with gpac:\ngpac -i video_tiled.mp4 -o dash_tiled.mpd\n##alternative version splitting tiles and tiling from raw bitstream in one pass\ngpac -i video_tiled.hvc hevcsplit -o dash_tiled.mpd\nThe resulting MPD will contain as many adaptation sets are there are tile tracks in the input video(s), and each tile adaptation set will contain representations for each quality specified. \nLive setup¶\nCheck the HEVC Tile-based adaptation guide for live setup, using hevcsplit instead of tilesplit.\nContent Playback¶\nThe logic of content playback is as follows:\nthe MPD indicates SRD information and a GPAC extension for mergeable bitstream\nwhen the compositor is used, the hevcmerge filter is automatically created to reassemble the streams\notherwise (using vout), each PID is declared as an alternative to the other\nA quick way to look at the output of the hevc merger filter is to run:\ngpac -i dash.mpd hevcmerge vout\nYou will see the reconstructed tiles, very likely not in the order you would expect. You can comment out one of the qualities and check the result.\nYou can also set --auto_switch=-1, which will change qualities of tiles one after the other.\nThe reordering of the tiles into a proper texture cannot be done by the merger, due to the different tile sizes.\nThis reordering is done by the compositor filter (3D mode required), remapping texture coordinates to restore the proper layout:\n##2D playabck without GUI\ngpac -mp4c dash.mpd \n##2D playabck with GUI\ngpac -gui dash.mpd\n##VR playabck without GUI\ngpac -mp4c dash.mpd#VR \n##VR playabck with GUI\ngpac -gui dash.mpd#VR\nThe GUI will only indicate a single visual object, as all input streams are merged back to a single HEVC bitstream.\nTile adaptation logic is similar to the one using HEVC tile tracks.\nThe option --skip_lqt is not supported in this mode.\nCustom tiling adaptation¶\nYou can devise your own custom tiling adaptation logic by using the general DASH custom algorithm.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/HEVC-Tile-multi-resolution-adaptation-guide/"},{"date_scraped_timestamp":1720188058934,"host":"wiki.gpac.io","page_title":"HLS Generation - GPAC wiki","text":"\n \n \n \n \n \n \nHLS Generation¶\nGPAC can be used to generate HLS rather than MPEG-DASH manifest formats, and can also be used to generate the two manifests in one pass.\nThe guidelines on segmentation are the same as with general dashing, please refer to the rest of the wiki for more info on this.\nHLS generation is supported by both MP4Box and gpac.\nHLS with one file per segment¶\nGenerating an HLS with one file per media segment is equivalent to dashing under the live profile.\nMP4Box -dash 1000 -profile live -out res/live.m3u8 source1 source2 \ngpac -i source1 -i source2 -o res/live.m3u8:profile=live\nThis will by default generate ISOBMFF / fmp4 files. To generate MPEG-2 TS files, use:\ngpac -i source1 -i source2 -o res/live.m3u8:profile=live:muxtype=ts\nNote that gpac will by default generate non-multiplexed outputs. To generate a multiplexed file, you need to flag inputs as belonging to the same quality , or Representation in DASH terminology:\ngpac -i video:#Representation=main -i audio:#Representation=main -o res/live.m3u8:profile=live:muxtype=ts\nHLS with a single file for all segments¶\nGenerating an HLS with one file per quality is equivalent to dashing under the onDemand profile.\ngpac -i source1 -i source2 -o res/live.m3u8:profile=onDemand\nSpecifying manifest names¶\nThe child playlist names are by default the manifest (master playlist) radical suffixed with _1, _2 ...\nThis can be changed by using the PID property HLSPL :\ngpac -i source1:#HLSPL=video.m3u8 -i source2:#HLSPL=audio.m3u8 -o res/live.m3u8\nThis will generate live.m3u8, video.m3u8 and audio.m3u8\nRenditions¶\nGrouping¶\nWhen several renditions are possible for a set of inputs, the default behavior is as follows:\nif video is present, it is used as the main content\notherwise, audio is used as the main content\nThe main content is repeated for each combination with other content. For example with 1 video and 3 audios, you will have:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"audio1\"\nvideo.m3u8\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"audio2\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio1\",NAME=\"1\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio2\",NAME=\"2\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nThis behavior can be changed by specifying a group for input streams using the PID property HLSGroup :\ngpac -i source1 -i audio1:#HLSGroup=MyAudio -i audio2:#HLSGroup=MyAudio -o res/live.m3u8\nThis will result in:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"MyAudio\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"1\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"2\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nWarning\nGPAC will not check if streams within a group have the same codec properties, you must only put in the same group streams with the same coding configurations, i.e. streams for which the CODECS parameter is the same.\nNaming¶\nThe component names are the DASH representation ID. By default this is an integer value, but it can be set to whatever suits you:\ngpac -i source1 -i audio1:#HLSGroup=MyAudio:#Representation=Soustitres -i audio2:#HLSGroup=MyAudio:#Representation=Subtitles -o res/live.m3u8\nThis will result in:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"MyAudio\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"Soustitres\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"Subtitles\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nDual DASH + HLS Generation¶\nGPAC can generate a manifest for both HLS and DASH by using the dual option.\ngpac -i source1 -i source2 -o res/live.mpd:dual:profile=live\nThis will generate live.mpdand live.m3u8 together with one file per segment produced.\ngpac -i source1 -i source2 -o res/live.m3u8:dual:profile=onDemand\nThis will generate live.m3u8 and live.mpd together with a single file per quality. Since the DASH profile is onDemand, the ISOBMFF file of each quality will also contain a segment index (not used in HLS).\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/hls/?q="},{"date_scraped_timestamp":1720187921464,"host":"wiki.gpac.io","page_title":"HLS Generation - GPAC wiki","text":"\n \n \n \n \n \n \nHLS Generation¶\nGPAC can be used to generate HLS rather than MPEG-DASH manifest formats, and can also be used to generate the two manifests in one pass.\nThe guidelines on segmentation are the same as with general dashing, please refer to the rest of the wiki for more info on this.\nHLS generation is supported by both MP4Box and gpac.\nHLS with one file per segment¶\nGenerating an HLS with one file per media segment is equivalent to dashing under the live profile.\nMP4Box -dash 1000 -profile live -out res/live.m3u8 source1 source2 \ngpac -i source1 -i source2 -o res/live.m3u8:profile=live\nThis will by default generate ISOBMFF / fmp4 files. To generate MPEG-2 TS files, use:\ngpac -i source1 -i source2 -o res/live.m3u8:profile=live:muxtype=ts\nNote that gpac will by default generate non-multiplexed outputs. To generate a multiplexed file, you need to flag inputs as belonging to the same quality , or Representation in DASH terminology:\ngpac -i video:#Representation=main -i audio:#Representation=main -o res/live.m3u8:profile=live:muxtype=ts\nHLS with a single file for all segments¶\nGenerating an HLS with one file per quality is equivalent to dashing under the onDemand profile.\ngpac -i source1 -i source2 -o res/live.m3u8:profile=onDemand\nSpecifying manifest names¶\nThe child playlist names are by default the manifest (master playlist) radical suffixed with _1, _2 ...\nThis can be changed by using the PID property HLSPL :\ngpac -i source1:#HLSPL=video.m3u8 -i source2:#HLSPL=audio.m3u8 -o res/live.m3u8\nThis will generate live.m3u8, video.m3u8 and audio.m3u8\nRenditions¶\nGrouping¶\nWhen several renditions are possible for a set of inputs, the default behavior is as follows:\nif video is present, it is used as the main content\notherwise, audio is used as the main content\nThe main content is repeated for each combination with other content. For example with 1 video and 3 audios, you will have:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"audio1\"\nvideo.m3u8\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"audio2\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio1\",NAME=\"1\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"audio2\",NAME=\"2\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nThis behavior can be changed by specifying a group for input streams using the PID property HLSGroup :\ngpac -i source1 -i audio1:#HLSGroup=MyAudio -i audio2:#HLSGroup=MyAudio -o res/live.m3u8\nThis will result in:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"MyAudio\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"1\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"2\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nWarning\nGPAC will not check if streams within a group have the same codec properties, you must only put in the same group streams with the same coding configurations, i.e. streams for which the CODECS parameter is the same.\nNaming¶\nThe component names are the DASH representation ID. By default this is an integer value, but it can be set to whatever suits you:\ngpac -i source1 -i audio1:#HLSGroup=MyAudio:#Representation=Soustitres -i audio2:#HLSGroup=MyAudio:#Representation=Subtitles -o res/live.m3u8\nThis will result in:\n##EXT-X-STREAM-INF:BANDWIDTH=1000000,CODECS=\"avc1.42C01F,mp4a.40.2\",RESOLUTION=1280x720,FRAME-RATE=\"25\",AUDIO=\"MyAudio\"\nvideo.m3u8\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"Soustitres\",LANGUAGE=\"fra\",AUTOSELECT=YES,URI=\"audio_1.m3u8\",CHANNELS=\"1\"\n##EXT-X-MEDIA:TYPE=AUDIO,GROUP-ID=\"MyAudio\",NAME=\"Subtitles\",LANGUAGE=\"eng\",AUTOSELECT=YES,URI=\"audio_2.m3u8\",CHANNELS=\"1\"\nDual DASH + HLS Generation¶\nGPAC can generate a manifest for both HLS and DASH by using the dual option.\ngpac -i source1 -i source2 -o res/live.mpd:dual:profile=live\nThis will generate live.mpdand live.m3u8 together with one file per segment produced.\ngpac -i source1 -i source2 -o res/live.m3u8:dual:profile=onDemand\nThis will generate live.m3u8 and live.mpd together with a single file per quality. Since the DASH profile is onDemand, the ISOBMFF file of each quality will also contain a segment index (not used in HLS).\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/hls/"},{"date_scraped_timestamp":1720188102836,"host":"wiki.gpac.io","page_title":"HLS Low Latency - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nPlease make sure you have read DASH Low Latency and HLS Generation before reading this. \nIn this howto, we will study various setups for HLS live streaming in low latency mode (LL-HLS), using both MP4Box and gpac.\nHLS Low Latency setup¶\nThe same setup for configuring segments and CMAF chunks is used as the DASH low latency setup.\nWhen you have low-latency producing of your HLS media segments, you need to indicate to the client how to access LL-HLS parts (CMAF chunks) while they are produced. LL-HLS offers two possibilities to describe these parts in the manifest:\nfile mode: advertise the chunks as dedicated files, i.e. each chunk will create its own file. This requires double storage for segments close to the live edge, increases disk IOs and might not be very practical if you setup a PUSH origin (twice the bandwidth is required)\nbyte range mode: advertise the chunks as byte range of a media file. If that media file is the full segment being produced (usually the case), this does not induce bandwidth increase or extra disk IOs.\nGPAC can work in both modes, and always use byte-range in the segment being produced for the second mode.\nThe mode is selected using the llhls option of the dasher.\nIn either mode, each variant playlist will be modified at each new LLHLS part ready.\nIn file mode, each part file is the full segment name appended with .N, with N the 1-based number of the part being generated, e.g. file_1.mp4.1, file_1.mp4.2, file_1.mp4.3 ... \nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live -out res/live.m3u8:llhls=br source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o res/live.m3u8:segdur=10:cdur=1:profile=live:dmode=dynamic:llhls=br\nIn the above example, we indicate the LLHLS parts are byte-ranges in the segment being produced. \nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live -out res/live.m3u8:llhls=sf source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o res/live.m3u8:segdur=10:cdur=1:profile=live:dmode=dynamic:llhls=sf\nIn the above example, we indicate the LLHLS parts are independent files. \nLLHLS Origin Server setup¶\nThe same setup for configuring the server is used as the DASH low latency setup.\nThe server will need a local directory where files are stored.\nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live \n -out http://localhost:8080/live.mpd:gpac:rdirs=outdir:llhls=br source1 source2\ngpac -i source1 -i source2 reframer:rt=on\n -o http://localhost:8080/live.mpd:gpac:segdur=10:cdur=1:profile=live:dmode=dynamic:rdirs=outdir:llhls=br\nNote that since this example only uses a single DASH session, all options specified for the dasher can be set globally:\nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://localhost:8080/live.mpd --rdirs=outdir --llhls=br source1 source2\ngpac -i source1 -i source2 reframer:rt=on -o http://localhost:8080/live.mpd\n --segdur=10 --cdur=1 --profile=live --dmode=dynamic --rdirs=outdir --llhls=br\nLLHLS Origin PUSH setup¶\nYou may want to use a regular HTTP server as your origin server, and have GPAC push segments to that server while they are being produced. Again, please read the documentation of the HTTP Server.\nIn this case, the httpout filter does not work as a server but as an HTTP client issuing PUT or POST requests, and does not need any local directory.\nMP4Box -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://ORIG_SERVER_IP_PORT/live.mpd:gpac:hmode=push:llhls=br source1 source2\ngpac -i source1 -i source2 reframer:rt=on\n -o http://ORIG_SERVER_IP_PORT/live.mpd:gpac:segdur=10:cdur=1:profile=live:dmode=dynamic:hmode=push:llhls=br\nDual DASH and HLS Low Latency¶\nYou can generate at the same time DASH-LL and LL-HLS, by simply setting up both availabilityStartOffset and LL-HLS modes. \nIt is recommended to only use byte-range mode for LL-HLS in that case for bandwidth efficiency reasons, but file mode is also working.\nMP4Box -ast-offset 9000 -frag-rt -dash-live 10000 -frag 1000 -profile live\n -out http://ORIG_SERVER_IP_PORT/live.mpd:gpac:dual:llhls=br:rdirs=outdir source1 source2\ngpac -i source1 -i source2 reframer:rt=on\n -o http://ORIG_SERVER_IP_PORT/live.mpd:gpac:dual:segdur=10:cdur=1:profile=live:dmode=dynamic:hmode=push:llhls=br:asto=9:rdirs=outdir\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/LL-HLS/"},{"date_scraped_timestamp":1720188039958,"host":"wiki.gpac.io","page_title":"Howtos - GPAC wiki","text":"\n \n \n \n \n \n \nThis part of the wiki describes HOWTOs illustrating the various possibilities of GPAC.\nFeel free to contribute your own cookbooks for media processing with GPAC !\nYou can also check gpac test suite scripts: the test suite mostly consists in functional and integration tests which provide good examples of GPAC features.\nNote: most command-line examples are given with local files as input, but they can work with any URL supported by GPAC.\n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/howtos/"},{"date_scraped_timestamp":1720188107028,"host":"wiki.gpac.io","page_title":"Howtos - GPAC wiki","text":"\n \n \n \n \n \n \nThis part of the wiki describes HOWTOs illustrating the various possibilities of GPAC.\nFeel free to contribute your own cookbooks for media processing with GPAC !\nYou can also check gpac test suite scripts: the test suite mostly consists in functional and integration tests which provide good examples of GPAC features.\nNote: most command-line examples are given with local files as input, but they can work with any URL supported by GPAC.\n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/howtos/?q="},{"date_scraped_timestamp":1720188127656,"host":"wiki.gpac.io","page_title":"Image extraction - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to extract items from HEIF file collections. For track extraction, use the regular tools from GPAC.\nA HEIF image collection will contain several items packed in a single meta box in ISOBMFF. These items will usually share the same coding type, although this is not a requirement. \nWe only discuss here HEIF version 1 files, for which each item is an intra picture. We assume the image collection is made of HEVC items.\nYou can use MP4Box to manually extract each item, see MP4Box -h meta: \nMP4Box -dump-item 1:path=dump.hvc source.heic\nThis will dump item 2 into dump.hvc.\nThis however requires the item ID, hence an inspection of the file prior to extracting the item.\nTo avoid this, you can use gpac as follows:\ngpac -i source.heic -o dump_$ItemID$.hvc\nThis will dump each item in dump_$ItemID$.hvc, with $ItemID$ being replaced by the item ID.\nTranscoding as images¶\nTranscoding is not possible using MP4Box. Using gpac, it is a fairly simple process:\ngpac -i source.heic -o dump_$ItemID$.jpg\nThis will transcode each item to JPEG in dump_$ItemID$.jpg, with $ItemID$ being replaced by the item ID. Changing the output extension to png will transcode to PNG.\nEach item will be declared as a media PID. This implies that there will be one decoder instance and one encoder instance created for each media PID.\nTranscoding as a sequence of images¶\nOne way to optimize the previous drawback of high resource usage is by declaring all items as a single track using -itt option of the MP4 demultiplexer:\ngpac -i source.heic:itt -o dump_$ItemID$.jpg\ngpac -i source.heic:itt -o dump_$num$.jpg\nIn this case, a single HEVC PID will be declared, hence a single HEVC decoder and a single JPEG encoder. \nNote: Using $num$ templating allows producing filenames independent from the item ID. \nThe same approach can be used to quickly view all items declared in a file:\ngpac -i source.heic:itt vout\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/heif-extraction/?q="},{"date_scraped_timestamp":1720188110042,"host":"wiki.gpac.io","page_title":"Image extraction - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to extract items from HEIF file collections. For track extraction, use the regular tools from GPAC.\nA HEIF image collection will contain several items packed in a single meta box in ISOBMFF. These items will usually share the same coding type, although this is not a requirement. \nWe only discuss here HEIF version 1 files, for which each item is an intra picture. We assume the image collection is made of HEVC items.\nYou can use MP4Box to manually extract each item, see MP4Box -h meta: \nMP4Box -dump-item 1:path=dump.hvc source.heic\nThis will dump item 2 into dump.hvc.\nThis however requires the item ID, hence an inspection of the file prior to extracting the item.\nTo avoid this, you can use gpac as follows:\ngpac -i source.heic -o dump_$ItemID$.hvc\nThis will dump each item in dump_$ItemID$.hvc, with $ItemID$ being replaced by the item ID.\nTranscoding as images¶\nTranscoding is not possible using MP4Box. Using gpac, it is a fairly simple process:\ngpac -i source.heic -o dump_$ItemID$.jpg\nThis will transcode each item to JPEG in dump_$ItemID$.jpg, with $ItemID$ being replaced by the item ID. Changing the output extension to png will transcode to PNG.\nEach item will be declared as a media PID. This implies that there will be one decoder instance and one encoder instance created for each media PID.\nTranscoding as a sequence of images¶\nOne way to optimize the previous drawback of high resource usage is by declaring all items as a single track using -itt option of the MP4 demultiplexer:\ngpac -i source.heic:itt -o dump_$ItemID$.jpg\ngpac -i source.heic:itt -o dump_$num$.jpg\nIn this case, a single HEVC PID will be declared, hence a single HEVC decoder and a single JPEG encoder. \nNote: Using $num$ templating allows producing filenames independent from the item ID. \nThe same approach can be used to quickly view all items declared in a file:\ngpac -i source.heic:itt vout\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/heif-extraction/"},{"date_scraped_timestamp":1720188124407,"host":"wiki.gpac.io","page_title":"Introduction - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nGPAC has extended support for MPEG-DASH and HLS content generation and playback. \nBasics concepts and terminology of MPEG-DASH are explained here and, and the same terms are usually used in GPAC for both DASH and HLS.\nFor more information on content generation:\nread MP4Box DASH options\nread the dasher filter help\ncheck the dash and HLS scripts in the GPAC test suite\nFor more information on content playback:\nread the dashin filter help, used whenever a DASH or HLS session is read.\ncheck the dash and HLS scripts in the GPAC test suite\nContent Generation¶\nIf you generate your content with an third-party application such as ffmpeg, make sure all your video qualities use closed GOP and have the same positions for their IDR frames.\nWhen using GPAC, this is usually ensure by using the fintra option.\nGPAC can be used to generate both static and live DASH/HLS content. For live cases, GPAC can expose the created files:\ndirectly through disk\nthrough its own HTTP server\nby pushing them to a remote HTTP server\nWe recommend reading the HTTP server filter help, and looking at the DASH and HLS low latency HowTos.\nContent Playback¶\nGPAC comes with a various set of adaptation algorithms:\nBBA0, BOLA, basic throughput (called conventional in the literature)\nCustom throughput-based (gbuf) and buffer-based (grate) algorithms\nThe algorithm can be replaced by your own algo in JS or Python.\nLow-Latency DASH streaming is supported, and HLS is supported starting from GPAC 2.0. \nAs usual in GPAC, accessing a DASH/HLS session is not reserved for playback, it can be used to feed a media pipeline for other tasks such as transcoding, encryption, recording, etc... See this howto for more information.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/DASH-intro/"},{"date_scraped_timestamp":1720187999603,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"Introduction - GPAC wiki","text":"\n \n \n \n \n \n \nContext¶\nHEIF is a new image format defined within MPEG, by companies such as Apple, Nokia, Canon, ... and by the GPAC team and Telecom Paris ! \nHEIF is based on the various constructs of ISOBMFF, supported in GPAC. For example, HEIF enables the storage of single images or image collections through the meta box and so-called items. HEIF also supports the storage of image sequences (like animated GIFs) using usual tracks, but with a new handler type called pict. HEIF was developed as a codec-agnostic container format (like ISOBMFF), but a first derivation of this generic format was standardized for images coded using the High Efficiency Video Codec (HEVC). \nNice examples of HEIF images and of its benefits compared to other formats can also be found on the website of some playback tools developed by Nokia, or on your iOS devices.\nGenerating HEIF images¶\nGPAC supports generating HEIF images using MP4Box, with the following command line:\nMP4Box -add-image file.hvc:primary -ab heic -new image.heic\nThis will take the first image of the HEVC file, create a meta box, add one image item, make it a primary item and add the heic brand to the output file.\nThe following command line will do the same but for the next IDR frame after the given time and the heix brand.\nMP4Box -add-image file.hvc:time=1.2:primary -ab heix -new image.heic\nFinally, the following command line will take a tiled HEVC stream (as described in [[this page|HEVC Tile-based adaptation guide]]) and generate one item per tile and one item for the entire image.\nMP4Box -add-image tiled.hvc:split_tiles:primary -ab heic -new tiled.heic\nFor more options, look at MP4Box -h meta.\nPlayback¶\nPlayback of HEIF (avc and hevc codecs) is also possible using the usual syntax:\ngpac -i heif_url vout\ngpac -play heif_url\ngpac -gui heif_url\nfor HEIF collections, the source can be played as a file sequence:\ngpac -play heif_url:itt\ngpac -gui heif_url:itt\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/GPAC-support-for-HEIF/"},{"date_scraped_timestamp":1720187923121,"host":"wiki.gpac.io","page_title":"Introduction - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC can be used to encrypt or decrypt media streams in a more or less format-agnostic manner, according to the Common Encryption, ISMA E&A and OMA DRM 2.0 specifications. An XML language is used by GPAC to get/set the encryption parameters.\nGPAC supports the ISMA E&A specification, better known as ISMACryp. This specification provides reliable transmission of encrypted media data with key signaling and cryptographic re-synchronization in case of packet loss or random access.\nGPAC partially supports the OMA DRM PDCF specification available here. This specification is derived from the ISMA E&A specification and OMA DRM PDCF files have a structure almost equivalent to ISMA protected files. GPAC does NOT support the ROAP protocol and other tools from the OMA DRM framework, but this could be added through a dedicated OMA DRM filter.\nGPAC supports the complete Common Encryption specification. \nNote that ISMACryp or CENC do not mandate anything regarding how keys are to be distributed (hereafter referred to as KMS - Key Management System) which is up to the content provider/distributor. In other words, CENC and ISMACryp are concerned with cryptographic interoperability only, not rights management.\nThese specifications use for encryption the AES 128 bit algorithm in counter mode (AES-CTR) for ISMA/OMA and CENC, or in CBC mode for CENC. You do not need to know of all these things, the only thing you need to know is that you will need a 128 bit key and a 64 bit salt, which would have to be fetched by the client at some point for decryption. In this document, the key and the salt will simply be referred to as key unless specified otherwise.\nOne interesting feature of the CENC and ISMACryp specifications is that they allow for selective encryption, in other words you may decide to encrypt only specific samples in the media track rather than the whole media. Selective encryption will reduce the complexity of the decryption process, and may also be very nice in demonstrations - for example, encrypting only I-frames in a video can give very nice effects ...\nIf you are familiar with MPEG-4 IPMPX specification, you must be aware that this selective encryption is different from IPMP-X one: in CENC or ISMACryp, selective encryption means whether or not a sample is encrypted while in IPMP-X selective encryption usually means whether specific bitstream syntax elements (motion vectors, DCT, etc) are encrypted or not.\nAnother interesting feature of CENC and ISMACryp is the possibility to roll keys, e.g. have more than one key needed for stream decryption: sample-based synchronization of keys and media are provided in both specifications. GPAC does not currently support usage of multiple keys in ISMACryp, only one key can be used in the stream lifetime.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/encryption/Encryption-Introduction/"},{"date_scraped_timestamp":1720187999603,"host":"wiki.gpac.io","page_title":"Introduction - GPAC wiki","text":"\n \n \n \n \n \n \nContext¶\nHEIF is a new image format defined within MPEG, by companies such as Apple, Nokia, Canon, ... and by the GPAC team and Telecom Paris ! \nHEIF is based on the various constructs of ISOBMFF, supported in GPAC. For example, HEIF enables the storage of single images or image collections through the meta box and so-called items. HEIF also supports the storage of image sequences (like animated GIFs) using usual tracks, but with a new handler type called pict. HEIF was developed as a codec-agnostic container format (like ISOBMFF), but a first derivation of this generic format was standardized for images coded using the High Efficiency Video Codec (HEVC). \nNice examples of HEIF images and of its benefits compared to other formats can also be found on the website of some playback tools developed by Nokia, or on your iOS devices.\nGenerating HEIF images¶\nGPAC supports generating HEIF images using MP4Box, with the following command line:\nMP4Box -add-image file.hvc:primary -ab heic -new image.heic\nThis will take the first image of the HEVC file, create a meta box, add one image item, make it a primary item and add the heic brand to the output file.\nThe following command line will do the same but for the next IDR frame after the given time and the heix brand.\nMP4Box -add-image file.hvc:time=1.2:primary -ab heix -new image.heic\nFinally, the following command line will take a tiled HEVC stream (as described in [[this page|HEVC Tile-based adaptation guide]]) and generate one item per tile and one item for the entire image.\nMP4Box -add-image tiled.hvc:split_tiles:primary -ab heic -new tiled.heic\nFor more options, look at MP4Box -h meta.\nPlayback¶\nPlayback of HEIF (avc and hevc codecs) is also possible using the usual syntax:\ngpac -i heif_url vout\ngpac -play heif_url\ngpac -gui heif_url\nfor HEIF collections, the source can be played as a file sequence:\ngpac -play heif_url:itt\ngpac -gui heif_url:itt\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/heif/GPAC-support-for-HEIF/"},{"date_scraped_timestamp":1720188039990,"host":"wiki.gpac.io","page_title":"JS 3D Graphics - GPAC wiki","text":"\n \n \n \n \n \nOverview¶\nWe discuss here how to use the JavaScript Filter to generate 3D vector graphics in GPAC.\nThe JS scripts in the GPAC test suite are also a good source of examples.\nGPAC provides a media composition engine through its Compositor Filter, capable of rendering BIFS, SVG, VRML and X3D. It requires however a scene description, and is limited to what the scene description syntax allows.\nFor a simpler and/or more customized support of 3D graphics, GPAC includes WebGL JavaScript bindings, allowing 3D graphics generation using the JS filter.\nCheck the documentation of the WebGL APIs for more details. \nSetting up WebGL¶\nYou need to import WebGL JavaScript bindings in your script:\nimport {WebGLContext} from 'webgl'\nYou will also very likely need to use matrices and textures. Helper tools from EVG might help here:\nimport {Texture, Matrix} from 'evg'\nAs explained in the API documentation, WebGL in GPAC is not using DOM and does not use a canvas element to access the context. Rather, the context is explicitly created through a constructor:\nlet width=1280;\nlet height=720;\nlet gl = new WebGLContext(width, height);\nYou can also pass a WebGLContextAttributes object to the constructor:\nlet gl = new WebGLContext(width, height, {depth: false});\nThe second important thing regarding WebGL in GPAC is that a filter is responsible for deciding when to issue GL calls, and this is not always in the process callback. Your therefore must call activate on the context before issuing calls:\n```\ngl.activate(true);\n//draw scene\n...\n//done\ngl.activate(false);\n ```\nSimilarly, the context being not tied to a window or a particular PID, it is the filter's responsibility to resize the underlying framebuffer of the WebGL context:\nYou now have a working WebGL context to draw your content. There is no such thing as requestAnimationFrame in GPAC, so you either need to draw your content upon filter.process callback or post a given for your filter (using filter.post_task).\nSending the framebuffer¶\nOnce your scene is drawn, you will likely want to use the results for later processing or display. \nGPU texture output¶\nIn this mode, the dispatched packet will only contain reference to the GL texture associated with the frame buffer used, typically color. This avoids unnecessary GPU->systems memory transfer if the consuming filter is capable of processing GL textures.\nThis approach is simply performed by creating a new packet from your canvas:\n//draw scene\n...\nlet output_pck = output_pid.new_packet(gl);\n//setup output packet, set DTS/CTS/etc..\n...\n//send packet\noutput_pck.send();\nNote that a single framebuffer is created for each WebGL context. One problem with the above approach is that you will not know when the output frame is consumed, hence you will probably erase the associated framebuffer before it is processed. It is therefore recommended to use a callback to track the packet release:\nif (filter.frame_pending) return;\n//draw scene\n...\n//output canvas\nlet output_pck = output_pid.new_packet(gl, () => { filter.frame_pending=false; } );\n//setup output packet, set DTS/CTS/etc..\n...\n//send packet\nfilter.frame_pending=true;\noutput_pck.send();\nFramebuffer copy output¶\nIn this mode, the dispatched packet will contain a copy of the framebuffer directly in system memory. The simplest way to do this is configuring the output PID to rgb or 'rgba` , allocating the packet and reading back the framebuffer throw gl.ReadPixels:\nlet output_pck = output_pid.new_packet(width*height*3);\ngl.ReadPixels(0, 0, width, height, gl.RGB, gl.UNSIGNED_BYTE, output_pck.data);\ngl.activate(false);\n//setup output packet, set DTS/CTS/etc..\n...\n//send packet\noutput_pck.send();\nNote that in this case you don't need to track when the output packet is consumed since it contains a full copy of the framebuffer. \n# Using textures¶\nGPAC uses regular WebGL textures as well as what we call NamedTexture .\nPlease read the WebGL GPAC doc for more details on textures.\nRegular textures¶\nEVG textures can be used to quickly load JPG or PNG images:\nlet texture = gl.createTexture();\nlet tx = new evg.Texture('source.jpg');\ngl.bindtexture(gl.TEXTURE_2D, texture);\ngl.texImage2D(target, level, internalformat, format, type, tx);\n//at this point the data is uploaded on GPU, the EVG texture is no longer needed and can be GC'ed \nEVG texture combined with EVG Canvas can be used to draw text and 2D shapes:\nlet canvas = new evg.Canvas(200, 200, 'rgba');\n /* draw stuff on 2D canvas\n ...\n */\n let texture = gl.createTexture();\n let tx = new evg.Texture(canvas);\n gl.bindtexture(gl.TEXTURE_2D, texture);\n gl.texImage2D(target, level, internalformat, format, type, tx);\n //at this point the data is uploaded on GPU, the EVG texture and canvas are no longer needed and can be GC'ed \nNamedTextures¶\nNamed textures provide a quick way of setting up texturing in various formats supported by GPAC for later use in GLSL shaders, hiding all the complexity of pixel conversion.\nA named texture is a texture created with a name:\nlet tx = gl.createTexture('myVidTex');\nThe texture data is then associated using upload():\n//source data is in system memory or already in OpenGL textures\nlet pck = input_pid.get_packet();\ntx.upload(pck);\n//or\n//source data is only in system memory\ntx.upload(some_evg_texture);\nRegular bindTexture and texImage2D can also be used if you don't like changing your code too much:\nlet pck = input_pid.get_packet();\ngl.bindTexture(gl.TEXTURE_2D, tx);\n//source data is in system memory or already in OpenGL textures\ngl.texImage2D(target, level, internalformat, format, type, pck);\n//or\ngl.bindTexture(gl.TEXTURE_2D, tx);\n//source data is only in system memory\ngl.texImage2D(target, level, internalformat, format, type, some_evg_texture);\nThe magic comes in when creating your shaders: any call to texture2D on a sampler2D using the same name as the NamedTexture is rewritten before compilation and replaced with GLSL code handling the pixel format conversion for you !\nvarying vec2 vTextureCoord;\nuniform sampler2D myVidTex; //this will get replaced before compilation\nuniform sampler2D imageSampler; //this will NOT get replaced\nvoid main(void) {\n vec2 tx = vTextureCoord;\n vid = texture2D(myVidTex, tx); //this will get replaced before compilation\n img = texture2D(imageSampler, tx); //this will NOT get replaced\n vid.alpha = img.alpha;\n gl_FragColor = vid;\n}\nThe resulting fragment shader may contain one or more sampler2D and a few additional uniforms, but they are managed for you by GPAC!\nThe named texture is then used as usual:\ngl.activeTexture(gl.TEXTURE0);\ngl.bindTexture(gl.TEXTURE_2D, tx);\n//this one is ignored for named textures (the uniformlocation object exists but is deactivated) but you can just keep your code as usual\ngl.uniform1i(myVidTexUniformLocation, 0);\ngl.activeTexture(gl.TEXTURE0 + tx.nb_textures);\ngl.bindTexture(gl.TEXTURE_2D, imageTexture);\ngl.uniform1i(imageSamplerUniformLocation, 0);\nIn the above code, note the usage of tx.nb_textures : this allows fetching the underlying number of texture units used by the named texture, and properly setting up multi-texturing.\nThe core concept for dealing with NamedTexture is that the fragment shader sources must be set AFTER the texture is being setup (upload / texImage2D). Doing it before will result in an unmodified fragment shader and missing uniforms.\nTo summarize, NamedTexture allows you to use existing glsl fragment shaders sources with any pixel format for your source, provided that:\nyou tag the texture with the name of the sampler2D you want to replace\nyou upload data to your texture before creating the program using it\nPrimary framebuffer¶\nYou can setup your WebGL context to work on the main (primary) framebuffer. This can be done by specifying the primary attribute in the WebGLContextAttributes object:\nlet gl = new WebGLContext(width, height, {primary: true});\nYou can then dispatch your output packets as usual:\nif (filter.frame_pending) return;\n//draw scene\n...\n//output canvas\nlet output_pck = output_pid.new_packet(gl, () => { filter.frame_pending=false; } );\n//setup output packet, set DTS/CTS/etc..\n...\n//send packet\nfilter.frame_pending=true;\noutput_pck.send();\nIn this mode, the vout filter consuming the output packets will only perform a backbuffer swap.\nDispatching the depth buffer¶\nYou can setup your WebGL context to allow dispatching of the depth buffer as GPU textures by specifying the depth attribute in the WebGLContextAttributes object:\nlet gl = new WebGLContext(width, height, {depth: texture});\nThe depth buffer can then be dispatched by creating an output packet:\nlet output_pck = output_pid.new_packet(gl, () => { filter.frame_pending=false; }, true );\nThis mode still lets you output the color buffer. This mode is however not compatible with usage of the primary framebuffer.\nYou can also dispatch the depth buffer as a regular system memory packet by reading back the depth buffer using gl.ReadPixels. \n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/webgl/"},{"date_scraped_timestamp":1720188110000,"host":"wiki.gpac.io","page_title":"JS Custom HTTP server - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to implement your custom HTTP server logic in JS.\nA custom logic can be defined using a standalone script, or by attaching a JS object to the httpout filter in a JS session.\nStandalone script¶\nThe standalone mode works by specifying a JS file to the httpin filter using its -js option:\ngpac httpout:port=8080:js=myserver.js ...\nThe JS script is loaded with a global object called httpout with one callback function, called at each new request.\nThe following is a basic JS example performing header injection :\nimport { Sys as sys } from 'gpaccore'\nhttpout.on_request = function (request)\n{\n print(\"Got request \" + JSON.stringify(request));\n request.headers_out.push({name: \"x-gpac\", value: \"foo\"});\n //let gpac handle the request\n request.reply=0;\n //throttle the connection, always delaying by 100 us\n request.throttle = function(done, total) {\n return 100;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n request.send();\n}\nAttaching from a JS session¶\nThe first step in your JS is to create an object implementing the callback previously indicated:\n//custom rate adaptation object\nlet req_handler = {\n on_request: function(request) {\n //same code as above\n }\n};\nYou will then need to setup a JS session monitoring filter creation process:\nsession.set_new_filter_fun( (f) => {\n print(\"new filter \" + f.name);\n //bind our custom rate adaptation logic\n if (f.name == \"httpout\") {\n f.bind(req_handler);\n }\n} ); \nAnd you're good to go !\nHandling requests in JS¶\nYou can handle the GET request in your own code rather han using GPAC httpout logic. You will need to use the read callback for GET and the write callback for PUT/POST:\nimport { Sys as sys } from 'gpaccore'\nhttpout.on_request = function (request)\n{\n print(\"Got request \" + JSON.stringify(request));\n //handle the request ourselves\n request.reply=200;\n //decide what to to - here we always open the same file\n request.src = new File('av1.mp4', 'rb');\n //our reader callback for GET\n request.read = function(buf) {\n if (request.src.eof) return 0;\n let nb = request.src.read(ab);\n return nb;\n }\n //our write callback for PUT/POST\n request.read = function(buf) {\n print('Got new bytes: ' + ab.byteLength);\n return 0;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n request.src.close();\n }\n //send the request - this can also be done later on, e.g. in a callback task\n request.send();\n}\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/jshttp/?q="},{"date_scraped_timestamp":1720188099868,"host":"wiki.gpac.io","page_title":"JS Custom HTTP server - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to implement your custom HTTP server logic in JS.\nA custom logic can be defined using a standalone script, or by attaching a JS object to the httpout filter in a JS session.\nStandalone script¶\nThe standalone mode works by specifying a JS file to the httpin filter using its -js option:\ngpac httpout:port=8080:js=myserver.js ...\nThe JS script is loaded with a global object called httpout with one callback function, called at each new request.\nThe following is a basic JS example performing header injection :\nimport { Sys as sys } from 'gpaccore'\nhttpout.on_request = function (request)\n{\n print(\"Got request \" + JSON.stringify(request));\n request.headers_out.push({name: \"x-gpac\", value: \"foo\"});\n //let gpac handle the request\n request.reply=0;\n //throttle the connection, always delaying by 100 us\n request.throttle = function(done, total) {\n return 100;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n }\n request.send();\n}\nAttaching from a JS session¶\nThe first step in your JS is to create an object implementing the callback previously indicated:\n//custom rate adaptation object\nlet req_handler = {\n on_request: function(request) {\n //same code as above\n }\n};\nYou will then need to setup a JS session monitoring filter creation process:\nsession.set_new_filter_fun( (f) => {\n print(\"new filter \" + f.name);\n //bind our custom rate adaptation logic\n if (f.name == \"httpout\") {\n f.bind(req_handler);\n }\n} ); \nAnd you're good to go !\nHandling requests in JS¶\nYou can handle the GET request in your own code rather han using GPAC httpout logic. You will need to use the read callback for GET and the write callback for PUT/POST:\nimport { Sys as sys } from 'gpaccore'\nhttpout.on_request = function (request)\n{\n print(\"Got request \" + JSON.stringify(request));\n //handle the request ourselves\n request.reply=200;\n //decide what to to - here we always open the same file\n request.src = new File('av1.mp4', 'rb');\n //our reader callback for GET\n request.read = function(buf) {\n if (request.src.eof) return 0;\n let nb = request.src.read(ab);\n return nb;\n }\n //our write callback for PUT/POST\n request.read = function(buf) {\n print('Got new bytes: ' + ab.byteLength);\n return 0;\n }\n //check end of request\n request.close = function(error) {\n print('closed with code ' + error_string(error));\n request.src.close();\n }\n //send the request - this can also be done later on, e.g. in a callback task\n request.send();\n}\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/jshttp/"},{"date_scraped_timestamp":1720187918382,"host":"wiki.gpac.io","page_title":"JavaScript Session - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to use gpac or the JavaScript Filter to query and control from JavaScript the filter session in GPAC.\nThe JS scripts in the gpac test suite are also a good source of examples.\nThe JS FilterSession provides JS bindings to the GPAC filter session object. It is recommended to also check the documentation of the JS APIs for more details. \nThe FilterSession JS bindings can be loaded in three different ways in GPAC.\nA global controller script can be specified as an argument to gpac using -js option:\ngpac -js=myscript.js` [OPTIONAL ARGS]\navailable by default in JavaScript Filters API\navailable by default in JavaScript Compositor API\nThe filter session object is exposed as a global variable named session.\nWARNING\nThe filter session API can only be loaded once per session. The implies that using both -js and the compositor, or -js and a JSFilter using this API will fail.\nDiscussion\nSince the session API is available in a JSFilter, you can load a script directly using gpac script.js. This will however create a JSFilter inside the session, but this filter will be automatically disabled (not used in the graph resolution, leaving it not connected) if the following conditions are met after initialization:\nfilter did not assign any capabilities\nfilter did not create any output PID\nfilter did not post any task using filter.post_task\nListing filters in a session¶\nWARNING\nThe filter object exposed by this API different from the JavaScript Filter one: it cannot be used to create packets, PIDs and so on on a given filter.\nDue to the dynamic nature of a filter session (multi-threading, dynamic filter chain configuration), getting a listing of filters should be done in exclusive mode:\nsession.lock_filters(true);\nlet nb_filters = session.nb_filters;\nfor (let i=0; i<session.nb_filters; i++) {\n let filter = session.get_filter();\n}\nsession.lock_filters(false);\nEach filter JS object is valid for the lifetime of the underlying filter.\nThe filter session can be configured to check for filter creation and deletion through callbacks:\nlet all_filters = [];\nsession.set_new_filter_fun( (f) => {\n print(\"new filter \" + f.name);\n f.iname = \"JS\"+f.name;\n all_filters.push(f);\n} ); \nsession.set_del_filter_fun( (f) => {\n print(\"delete filter \" + f.iname);\n let idx = all_filters.indexOf(f);\n if (idx>=0)\n all_filters.splice (idx, 1);\n}); \nYou can also check if a filter has not been destroyed by using is_destroyed() function. \nThe specific property iname is a string identifier reserved for JS, and used to query a filter by name in the session:\nlet f = session.get_filter('my cool filter');\nNote that in this case, there is no need to lock the session from javascript.\nThe property iname is also shared with the JavaScript Filter.\nGetting notified during session execution¶\nThe filter session is running through an internal task scheduler. You can post tasks to this scheduler for your script:\nsession.post_task( ()=> {\n if (session.last_task) {\n print(\"we are done \");\n //the task will no longer be called \n return false;\n }\n all_filters.forEach( (f) => { print('Name: '+f.name)});\n //call back in 1sec\n return 1000;\n});\nCreating filters¶\nYou can load any filter during the session, connecting it from any existing filter if needed. \n- source filters must be loaded as src=URL:opts\n- destination filters must be loaded as dst=URL:opts\nNote\nInjecting a filter in the middle of a connected chain (i.e. going from A->B to A->newF->B) is currently not supported.\n//load a source\nlet src = session.add_filter(\"video.mp4\");\n//load an inspect filter, getting its input only from src\nlet f = session.add_filter(\"inspect\", src);\nAll filter options valid in GPAC can be used. This means that you can specify complex filter graphs using SID and FID syntax:\n//load 3 sources\nlet src1 = session.add_filter(\"video.mp4:FID=1\");\nlet src2 = session.add_filter(\"video.mp4:FID=2\");\nlet src3 = session.add_filter(\"video.mp4:FID=1\");\n//load an inspect filter, getting its input only from src1 and src2\nlet f = session.add_filter(\"inspect:SID=1,2\");\n//load a ISOBMF mux filter, getting its input only from src2 and src3\nlet f = session.add_filter(\"dst=mux.mp4:SID=2,3\");\nTo insert a filter before or after a specific filter:\nlet f = session.get_filter('my cool filter');\n//insert a TS mux after a given filter\nf.insert(\"dst=mux.ts\");\nTo remove a given filter:\nlet f = session.get_filter('my cool filter');\nf.remove();\nTo update a filter option:\nlet f = session.get_filter('my cool filter');\nf.update(\"opt_name\", \"opt_val\");\nQuerying filters¶\nAll properties of a filter object are enumerable:\nprint(\"Filter properties:\");\nfor(let propertyName in f) {\n print(\"f.\" + propertyName + \" : \" + f[propertyName]);\n}\nYou can query the number of input and output PIDs of a filter, and enumerate their properties:\n//input PID\nprint(\"Filter num input PIDs: \" + f.nb_ipid);\nfor (i=0; i<f.nb_ipid; i++) {\n //enum props\n f.ipid_props(i, (name, type, val) => { print('input pid prop ' + name + ' type ' + type + ' val ' + val);})\n //or direct query\n let st = f.ipid_props(i, 'StreamType');\n}\n//output PID\nprint(\"Filter num output PIDs: \" + f.nb_opid);\nfor (i=0; i<f.nb_opid; i++) {\n f.opid_props(i, (name, type, val) => { print('output pid prop ' + name + ' type ' + type + ' val ' + val);})\n}\nReminder: The list of available properties is gpac -h props.\nYou can query the source filter of a given input PID:\nprint(\"Filter sources: \");\nfor (i=0; i<f.nb_ipid; i++) {\n let s = f.ipid_source(i);\n print(\"PID\"+i+\" source: \" + s.name);\n}\nYou can query the filters connected to an output PID:\nprint(\"Filter destinations: \");\nfor (i=0; i<f.nb_opid; i++) {\n let sinks = f.opid_sinks(i);\n print(\"PID\"+i+\" destinations:\");\n for (j=0; j<sinks.length; j++) {\n print(\" \"+sinks[i].name);\n }\n}\nYou can query the arguments defined on a filter:\nlet args = f.all_args();\nprint(\"\" + args.length + \" arguments: \" + JSON.stringify(args) );\nHandling events¶\nThe filter session can receive UI-related events global to the session.\nsession.set_event_fun( (evt) => {\nprint(\"evt \" + evt.name);\nif (evt.type != GF_FEVT_USER) return;\nif (evt.ui_type == GF_EVENT_SIZE) {\n print('display size is ' + evt.width + 'x' + evt.height);\n return false;\n}\n});\nThe filter session can also be used to fire events events on filters accepting UI events. \nlet f_evt = new FilterEvent(GF_FEVT_USER);\nf_evt.ui_type = GF_EVENT_SET_CAPTION;\nf_evt.caption = \"forced caption\";\nsession.fire_event(f_evt);\nThe filter session can also be used to fire non-UI related events on filters. You must be extra careful when using this, as this might trigger unwanted behavior in the chain. Typically:\nupstream events (towards sink) should only be fired on source filters (nb_ipid = 0)\ndownstream events (towards source) should only be fired on sink filters (nb_opid = 0)\nlet f_evt = new FilterEvent(GF_EVENT_STOP);\nsession.fire_event(f_evt, target_filter);\nRemote interaction¶\nGPAC is by default compiled with Remotery support, and can use the underlying websocket server of remotery to communicate with a web browser.\nYou will need for this:\nto launch GPAC with remotery activate by specifying -rmt\nset a handler function to listen to messages from the web client using session.set_rmt_fun\nsend messsages to the web client using session.rmt_send\nThe following is an example of using remotery in JS:\nsession.set_rmt_fun( (text)=> {\n print(\"rmt message \" + text);\n //do something\n //reply, either at once or later\n session.rmt_send(\"yep !\");\n});\nSince the remotery code in GPAC is not modified, only text messages can currently be sent. We recommend exchanging data through JSON.\nBy default, sampling times collecting is enabled in remotery. You can enable or disable at runtime this:\nif (session.rmt_enabled)\n print(\"disabling rmt sampling!\");\nsession.rmt_enabled = false;\n});\nCreating custom filters¶\nYou can create your own custom filters in a JS session using new_filter. The returned object will be a JavaScript Filter with the following limitations:\nno custom arguments for the filter can be set\nthe initialize function is not called\nthe filter cannot be cloned\nthe filter cannot be used as source of filters loading a source filter graph dynamically, such as the dashin filter.\nthe filter cannot be used as destination of filters loading a destination filter graph dynamically, such as the dasher filter.\nlet my_filter = session.new_filter(\"MyFilter\");\n//let the filter accept any input of type video\nmy_filter.set_cap({id: \"StreamType\", value: \"Visual\"});\n//check input connections\nmy_filter.configure_pid = function(pid)\n{\n}\n//process packets\nmy_filter.process = function()\n{\n}\nSome GPAC core functions are made available through JS for prompt handling, bitstream parsing, file and directory IO, check the documentation.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/jssession/"},{"date_scraped_timestamp":1720188018379,"host":"wiki.gpac.io","page_title":"Linux - GPAC wiki","text":"\n \n \n \n \n \n \nPreliminary notes: the following instructions will be based on Ubuntu and Debian. It should be easily applicable to other distributions, the only changes should be name of the packages to be installed, and the package manager used.\nGPAC is a modular piece of software which depends on third-party libraries. During the build process it will try to detect and leverage the installed third-party libraries on your system. Here are the instructions to:\nbuild GPAC easily (recommended for most users) from what's available on your system,\nbuild a minimal 'MP4Box' and 'gpac' (only contains GPAC core features like muxing and streaming),\nbuild a complete GPAC by rebuilding all the dependencies manually.\nGeneral case for all builds¶\nDevelopment tools¶\nWe first need to install some building tools: \nsudo apt install build-essential pkg-config g++ git cmake yasm\nGet the code¶\ngit clone https://github.com/gpac/gpac.git\ncd gpac\nBuild¶\nTo parallelize the build you can use the -j option of Make.\nUpgrade¶\nIf this fails, do make clean.\nIf this fails again, reconfigure your GPAC with the same option as you previously did: ./configure --xxx .... You can find these options at the top of your config.mak or config.h files (located in the folder where you call the configure script from).\nIf you are upgrading from a previous version (especially going from below 1.0.0 to 1.0.0+) you should run make uninstall ; make distclean before running ./configure.\nUse¶\nYou can either:\nsudo make install to install the binaries,\nor use the MP4Box or gpac binary in gpac_public/bin/gcc/ directly, \nor move/copy it somewhere manually.\nGPAC easy build (recommended for most users)¶\nInstall the development packages for the third-party libraries GPAC is able to leverage: \nsudo apt install zlib1g-dev libfreetype6-dev libjpeg62-dev libpng-dev libmad0-dev libfaad-dev libogg-dev libvorbis-dev libtheora-dev liba52-0.7.4-dev libavcodec-dev libavformat-dev libavutil-dev libswscale-dev libavdevice-dev libnghttp2-dev libopenjp2-7-dev libcaca-dev libxv-dev x11proto-video-dev libgl1-mesa-dev libglu1-mesa-dev x11proto-gl-dev libxvidcore-dev libssl-dev libjack-jackd2-dev libasound2-dev libpulse-dev libsdl2-dev dvb-apps mesa-utils libcurl4-openssl-dev\nThis list should work on Ubuntu from 14.04 (trusty) to at least 22.04 (jammy). \nIf you use Debian instead of Ubuntu, replace libjpeg62-dev with libjpeg62-turbo-dev. It should work at least from Debian 9 (stretch) to 12 (bookworm).\n[Also tested on archlinux-2022.02.01-x86_64 with the following: pacman -S zlib freetype2 libjpeg-turbo libpng libmad faad2 libogg libvorbis libtheora a52dec ffmpeg libxv mesa glu xvidcore openssl jack2 alsa-lib libpulse sdl2]\nOnce the packages are installed, you can follow the general build instructions from the previous section. \nMP4Box & gpac only (minimal static build)¶\nThis build is intended if you only need GPAC core features such as demuxing, muxing and streaming or if you need a static build.\nTo build only the MP4Box and gpac command line utilities, you need to: \nInstall build tools\nsudo apt install build-essential pkg-config git\nInstall zlib\nsudo apt install zlib1g-dev\nGet the code\ngit clone https://github.com/gpac/gpac.git gpac_public\ncd gpac_public\nBuild\nIf you are upgrading from a previous version (especially going from below 1.0.0 to 1.0.0+) you should run make uninstall ; make distclean before running ./configure.\n./configure --static-bin\nmake\nUse\nYou can either:\nsudo make install to install the binaries,\nor use the MP4Box or gpac binary in gpac_public/bin/gcc/ directly, \nor move/copy it somewhere manually.\nFull GPAC build (advanced users)¶\nIn order to do a full build, we'll first have to handle the dependencies. On Linux, we try to rely on system dependencies (i.e. installable packages) as much as possible and only build the rest manually. \nBuilding other dependencies¶\nFrom now on, we'll call the base working directory <GPAC_ROOT_DIR>.\nGet the code\n<GPAC_ROOT_DIR>$ git clone https://github.com/gpac/gpac.git gpac_public\n<GPAC_ROOT_DIR>$ git clone https://github.com/gpac/deps_unix\n<GPAC_ROOT_DIR>$ cd deps_unix\n<GPAC_ROOT_DIR>/deps_unix$ git submodule update --init --recursive --force --checkout\nThe convention of calling the main gpac repository gpac_public here is quite important for some scripts. If you don't you'll have to adapt the scripts in deps_unix. \nBuild the dependencies\n<GPAC_ROOT_DIR>/deps_unix$ ./build_all.sh linux\nIf all went well, you should see some libs have been copied over to the gpac repository. It should look something like this: \n<GPAC_ROOT_DIR>/deps_unix$ ls -l ../gpac_public/extra_lib/lib/gcc/\ntotal 11240\n-rw-r--r-- 1 u g 11820 Aug 23 16:31 libArithCoding.a\n-rw-r--r-- 1 u g 72260 Aug 23 16:31 libcaption.a\n-rw-r--r-- 1 u g 108734 Aug 23 16:31 libDRCdec.a\n-rw-r--r-- 1 u g 129430 Aug 23 16:31 libFDK.a\n-rw-r--r-- 1 u g 119624 Aug 23 16:31 libFormatConverter.a\n-rw-r--r-- 1 u g 298372 Aug 23 16:31 libgVBAPRenderer.a\n-rw-r--r-- 1 u g 52192 Aug 23 16:31 libIGFdec.a\n-rw-r--r-- 1 u g 230002 Aug 23 16:31 libilo.a\n-rw-r--r-- 1 u g 5416016 Aug 23 16:32 libmmtisobmff.a\n-rw-r--r-- 1 u g 218566 Aug 23 16:32 libmmtisobmff_c.a\n-rw-r--r-- 1 u g 270460 Aug 23 16:31 libMpeghDec.a\n-rw-r--r-- 1 u g 22388 Aug 23 16:31 libMpeghUIMan.a\n-rw-r--r-- 1 u g 98452 Aug 23 16:31 libMpegTPDec.a\n-rw-r--r-- 1 u g 3431798 Aug 23 16:31 libopenhevc.a\n-rw-r--r-- 1 u g 888206 Aug 23 16:29 libOpenSVCDec.a\n-rw-r--r-- 1 u g 10514 Aug 23 16:31 libPCMutils.a\n-rw-r--r-- 1 u g 33888 Aug 23 16:31 libSYS.a\n-rw-r--r-- 1 u g 60078 Aug 23 16:31 libUIManager.a\n(contents and versions may differ depending on updates and deprecated features)\nBuilding GPAC¶\nNow that all the set up is done, we can build gpac by simply doing: \n<GPAC_ROOT_DIR>/gpac_public$ ./configure\n<GPAC_ROOT_DIR>/gpac_public$ make\n<GPAC_ROOT_DIR>/gpac_public$ sudo make install\n[on Arch Linux, use ./configure --prefix=/usr since Arch doesn't use /usr/local by default]\nUpgrading from a previous version¶\nIf you already have a gpac version installed, or if you have already built gpac in the same directory, especially going from a version below 1.0.0 to 1.0.0 and after, you should run\n<GPAC_ROOT_DIR>/gpac_public$ make uninstall\n<GPAC_ROOT_DIR>/gpac_public$ make distclean\nbefore running ./configure. \nInstall for developers¶\nIf you want to use GPAC for development (in your own code for example), you can use make install-lib. \nIt will install the necessary libraries and header files. It will also install a gpac.pc file for pkg-config. With it you can easily build projects that use the gpac library with something like:\n$ gcc -o example $(pkg-config --cflags gpac) example.c $(pkg-config --libs gpac)\nPackaging¶\nTo generate a .deb package, you can use make deb. \nYou will probably have to install some packaging tools to run it:\nsudo apt install fakeroot dpkg-dev devscripts debhelper ccache\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/build/GPAC-Build-Guide-for-Linux/"},{"date_scraped_timestamp":1720187931058,"host":"wiki.gpac.io","page_title":"Logging - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC Log System¶\nlibgpac logs options:¶\n-noprog: disable progress messages\n-quiet: disable all messages, including errors\n-log-file,-lf (string): set output log file\n-log-clock,-lc: log time in micro sec since start time of GPAC before each log line except for app tool\n-log-utc,-lu: log UTC time in ms before each log line except for app tool\n-logs (string): set log tools and levels. \nYou can independently log different tools involved in a session. \nlog_args is formatted as a colon (':') separated list of toolX[:toolZ]@levelX \nlevelX can be one of: \nquiet: skip logs \nerror: logs only error messages \nwarning: logs error+warning messages \ninfo: logs error+warning+info messages \ndebug: logs all messages \ntoolX can be one of: \ncore: libgpac core \nmutex: log all mutex calls \nmem: GPAC memory tracker \nmodule: GPAC modules (av out, font engine, 2D rasterizer) \nfilter: filter session debugging \nsched: filter session scheduler debugging \ncodec: codec messages (used by encoder and decoder filters) \ncoding: bitstream formats (audio, video, scene) \ncontainer: container formats (ISO File, MPEG-2 TS, AVI, ...) and multiplexer/demultiplexer filters \nnetwork: TCP/UDP sockets and TLS \nhttp: HTTP traffic \ncache: HTTP cache subsystem \nrtp: RTP traffic \ndash: HTTP streaming logs \nroute: ROUTE (ATSC3) debugging \nmedia: messages from generic filters and reframer/rewriter filters \nparser: textual parsers (svg, xmt, bt, ...) \nmmio: I/O management (AV devices, file, pipes, OpenGL) \naudio: audio renderer/mixer/output \nscript: script engine except console log \nconsole: script console log \nscene: scene graph and scene manager \ncompose: composition engine (2D, 3D, etc) \nctime: media and SMIL timing info from composition engine \ninteract: interaction messages (UI events and triggered DOM events and VRML route) \nrti: run-time stats of compositor \nall: all tools logged - other tools can be specified afterwards. \nThe special keyword ncl can be set to disable color logs. \nThe special keyword strict can be set to exit at first error. \nExample\n-logs=all@info:dash@debug:ncl\nThis moves all log to info level, dash to debug level and disable color logs \n-proglf: use new line at each progress messages\n-log-dual,-ld: output to both file and stderr \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/core_logs/"},{"date_scraped_timestamp":1720187986599,"host":"wiki.gpac.io","page_title":"MP4Box DASH options - GPAC wiki","text":"\n \n \n \n \n \n \nMP4Box can be used to generate content compliant to the MPEG-DASH specification, aka ISO/IEC 23009-1 available in ISO Publicly Available Standards.\nFor more details on what is DASH and HTTP streaming, please refer to [[this post|Fragmentation, segmentation, splitting and interleaving]]. For more help, type MP4Box -h dash\n-dash Duration : enables DASH segmentation of input files with the given segment duration. For onDemand profile, where each media presentation is a single segment, this option sets the duration of a subsegment.\n-dash-live[=File] DUR generates a live DASH session using dur segment duration, optionally writing live context to F. MP4Box will run the live session until q is pressed or a fatal error occurs.\n-ddbg-live[=File] DUR same as -dash-live without time regulation for debug purposes.\n-frag dur_in_ms : specifies the duration of subsegments in ms. This duration is always less than the segment duration. By default (when not set), the subsegment duration is the DASH duration, i.e. there is only one subsegment per segment. For onDemand profile, where each media presentation is a single segment, this option sets the duration of a subsegment.\n-out filename specifies output file name for MPD. May use relative path. All segments will be produced in the same directory as the MPD.\n-tmp dirname specifies a directory for temporary file creation (the default temporary directory is OS-dependent).\n-profile NAME specifies the target DASH profile: onDemand, live, main, simple, full, and two profiles from the DASH-IF: dashavc264:live, dashavc264:onDemand. This will set default option values to ensure conformance to the desired profile.\n-rap forces segments to begin with random access points. Segment duration may not be exactly what asked by -dash switch since encoded video data is not modified.\n-frag-rap All fragments will begin with a random access points. Fragment duration may not be exactly what is asked by -frag since encoded video data is not modified. (ISOBMF only)\n-segment-name name sets the segment name for generated segments. If not set (default), the segments are concatenated in output file, except if live profile is requested, in which case the default template dash_%s is used. The segment names can furthermore be configured by using a subset of the SegmentTemplate identifiers: $RepresentationID$, $Number$, $Bandwidth$ and $Time. Additional items are defined:\n$Init=VALUE$ is replaced by VALUE if the generated file is an initialization segment,\n$Index=VALUE$ is replaced by VALUE if the generated file is an index segment.\n$Path=VALUE$ is replaced by VALUE when creating the file, but ignored when writing the segment template in MPD.\n-segment-ext name sets the segment extension. Default is m4s, null means no extension.\n-segment-timeline uses SegmentTimeline when generating segments. NOT SUPPORTED BY LIVE/CTX MODE YET.\n-segment-marker MARK adds a box of type \\'MARK\\' at the end of each DASH segment. MARK shall be a 4CC identifier.\n-base-url string  sets the base url at MPD level. Can be used several times for multiple URLs.\n-mpd-title string sets MPD title.\n-mpd-source string sets MPD source\n-mpd-info-url string sets MPD info url.\n-cprt string adds copyright string to MPD\n-dash-ctx FILE stores and restore DASH timing from FILE (created if not found). This option stores the current timing of the DASHed presentation, and for all segments except the first (initial call), shifts the timing according to this stored value. By calling MP4Box on a regular basis with new segment to append to the MPD, one can generate a live compatible MPD. All options from the regular mode are allowed in this mode, except the options related to the ISO onDemand profile.\n-dynamic uses dynamic MPD type instead of static (always set for -dash-live)\n-mpd-refresh specifies MPD update time in seconds\n-time-shift specifies MPD time shift buffer depth in seconds (default 0). Specify -1 to keep all files\n-subdur DUR specifies maximum duration in ms of the input file to be dashed in LIVE or context mode. NOTE: This does not change the segment duration: dashing stops once segments produced exceeded the duration.\n-min-buffer TIME specifies MPD min buffer time in milliseconds.\n-dash-scale SCALE specifies that timing for -dash and -frag are expressed in SCALE units per seconds.\n-mem-frags fragments will be produced in memory rather than on disk before flushing to disk.\n-pssh-moof stores PSSH boxes in first moof of each segments. By default PSSH are stored in movie box.\n-sample-groups-traf stores sample group descriptions in traf (duplicated for each traf) rather than in moof. By default sample group descriptions are stored in movie box.\n-subsegs-per-sidx N  sets the number of subsegments to be written in each SIDX box. If 0, a single SIDX box is used per segment. If -1, no SIDX box is used. Otherwise, the segmenter will pack N subsegments in the root SIDX of the segment, with DashDuration/N/fragDuration fragments per subsegments. (ISOBMF only)\n-url-template uses SegmentTemplate instead of explicit sources in segments. Ignored if segments are stored in a single file. Set by default for live profiles.\n-daisy-chain uses daisy-chaining of SIDX (1->2->3->4) instead of hierarchical. Ignored if -subseg-per-sidx is 0. (ISOBMF only)\n-single-segment uses a single segment for each representation. Set by default for onDemand profile.\n-single-file uses a single file for each representation.\n-bs-switching MODE sets the bitstream switching mode to one of the following:\ninband (default): generate initialization segments compatible with each representation in the adaptation set by using inband carriage of video parameter sets (avc3, hev1)\nmerge: generate initialization segments compatible with each representation in the adaptation set by merging video parameter sets in a single configuration, if possible. If not possible, defaults to no\nmulti: generate a single init segment with multiple sample description entries for each track (cf HbbTV specification).\nno: bitstream switching mode not used. Turned on by default if a single input is dashed\nsingle forces inband mode when a single input is used.\nThis option is only used for ISOBMF inputs. The segmenter always assumes that MPEG-2 TS input use bitstream switching.\n-moof-sn N sets sequence number of first moof to N\n-tfdt N sets TFDT of first traf to N in SCALE units (cf -dash-scale)\n-no-frags-default disables default flags in fragments\n-single-traf uses a single track fragment per moof (smooth streaming and derived specs may require this).\n-dash-ts-prog N program_number to be considered in case of an MPTS input file..\nIt is possible to feed MP4Box with a set of ISOBMF files containing different media: MP4Box will generate multiple adaptation sets at once for ISOBMF. The different input files are filtered based on their media type, PAR, language and codec, and gathered in different adaptation sets. Media streams of the same type but with different properties are tagged as belonging to the same group through the @group attribute.\nThe files may be assigned periods, descriptors and other options using the [:OPT] suffix. Specific tracks may be loaded as a single representation using fragments.The following fragments and options are defined:\n#trackID=N only uses the track ID N from the source file\n#video only uses the first video track from the source file\n#audio only uses the first audio track from the source file\n:id=NAME sets the representation ID to NAME\n:period=NAME sets the representation's period to NAME. Multiple periods may be used. Periods appear in the MPD in the same order as specified with this option.\n:BaseURL=NAME sets the BaseURL. Set multiple times for multiple BaseURLs.\n:bandwidth=VALUE sets the representation's bandwidth to a given value.\n:xlink=VALUE sets the xlink value for the period containing this element. Only the xlink declared on the first rep of a period will be used.\n:duration=VALUE Increases the duration of this period by the given duration in seconds. Only used when no input media is specified (remote period insertion), e.g. when using :period=X:xlink=Z:duration=Y as an input. If used on a regular input source, this overrides the target segment duration for this representation to VALUE, expressed in dash timescale (cf -dash-scale); this will potentially create non time aligned segments.\n:role=VALUE sets the role of this representation (cf DASH spec). Media with different roles belong to different adaptation sets.\n:desc_p=VALUE adds a descriptor at the Period level. Value must be a properly formatted XML element.\n:desc_as=VALUE adds a descriptor at the AdaptationSet level. Value must be a properly formatted XML element. Two input files with different values will be in different AdaptationSet elements.\n:desc_as_c=VALUE adds a descriptor at the AdaptationSet level. Value must be a properly formatted XML element. Value is ignored while creating AdaptationSet elements.\n:desc_rep=VALUE adds a descriptor at the Representation level. Value must be a properly formatted XML element. Value is ignored while creating AdaptationSet elements. \n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/DASH-Support-in-MP4Box/?q="},{"date_scraped_timestamp":1720187937786,"host":"wiki.gpac.io","page_title":"MP4Box DASH options - GPAC wiki","text":"\n \n \n \n \n \n \nMP4Box can be used to generate content compliant to the MPEG-DASH specification, aka ISO/IEC 23009-1 available in ISO Publicly Available Standards.\nFor more details on what is DASH and HTTP streaming, please refer to [[this post|Fragmentation, segmentation, splitting and interleaving]]. For more help, type MP4Box -h dash\n-dash Duration : enables DASH segmentation of input files with the given segment duration. For onDemand profile, where each media presentation is a single segment, this option sets the duration of a subsegment.\n-dash-live[=File] DUR generates a live DASH session using dur segment duration, optionally writing live context to F. MP4Box will run the live session until q is pressed or a fatal error occurs.\n-ddbg-live[=File] DUR same as -dash-live without time regulation for debug purposes.\n-frag dur_in_ms : specifies the duration of subsegments in ms. This duration is always less than the segment duration. By default (when not set), the subsegment duration is the DASH duration, i.e. there is only one subsegment per segment. For onDemand profile, where each media presentation is a single segment, this option sets the duration of a subsegment.\n-out filename specifies output file name for MPD. May use relative path. All segments will be produced in the same directory as the MPD.\n-tmp dirname specifies a directory for temporary file creation (the default temporary directory is OS-dependent).\n-profile NAME specifies the target DASH profile: onDemand, live, main, simple, full, and two profiles from the DASH-IF: dashavc264:live, dashavc264:onDemand. This will set default option values to ensure conformance to the desired profile.\n-rap forces segments to begin with random access points. Segment duration may not be exactly what asked by -dash switch since encoded video data is not modified.\n-frag-rap All fragments will begin with a random access points. Fragment duration may not be exactly what is asked by -frag since encoded video data is not modified. (ISOBMF only)\n-segment-name name sets the segment name for generated segments. If not set (default), the segments are concatenated in output file, except if live profile is requested, in which case the default template dash_%s is used. The segment names can furthermore be configured by using a subset of the SegmentTemplate identifiers: $RepresentationID$, $Number$, $Bandwidth$ and $Time. Additional items are defined:\n$Init=VALUE$ is replaced by VALUE if the generated file is an initialization segment,\n$Index=VALUE$ is replaced by VALUE if the generated file is an index segment.\n$Path=VALUE$ is replaced by VALUE when creating the file, but ignored when writing the segment template in MPD.\n-segment-ext name sets the segment extension. Default is m4s, null means no extension.\n-segment-timeline uses SegmentTimeline when generating segments. NOT SUPPORTED BY LIVE/CTX MODE YET.\n-segment-marker MARK adds a box of type \\'MARK\\' at the end of each DASH segment. MARK shall be a 4CC identifier.\n-base-url string  sets the base url at MPD level. Can be used several times for multiple URLs.\n-mpd-title string sets MPD title.\n-mpd-source string sets MPD source\n-mpd-info-url string sets MPD info url.\n-cprt string adds copyright string to MPD\n-dash-ctx FILE stores and restore DASH timing from FILE (created if not found). This option stores the current timing of the DASHed presentation, and for all segments except the first (initial call), shifts the timing according to this stored value. By calling MP4Box on a regular basis with new segment to append to the MPD, one can generate a live compatible MPD. All options from the regular mode are allowed in this mode, except the options related to the ISO onDemand profile.\n-dynamic uses dynamic MPD type instead of static (always set for -dash-live)\n-mpd-refresh specifies MPD update time in seconds\n-time-shift specifies MPD time shift buffer depth in seconds (default 0). Specify -1 to keep all files\n-subdur DUR specifies maximum duration in ms of the input file to be dashed in LIVE or context mode. NOTE: This does not change the segment duration: dashing stops once segments produced exceeded the duration.\n-min-buffer TIME specifies MPD min buffer time in milliseconds.\n-dash-scale SCALE specifies that timing for -dash and -frag are expressed in SCALE units per seconds.\n-mem-frags fragments will be produced in memory rather than on disk before flushing to disk.\n-pssh-moof stores PSSH boxes in first moof of each segments. By default PSSH are stored in movie box.\n-sample-groups-traf stores sample group descriptions in traf (duplicated for each traf) rather than in moof. By default sample group descriptions are stored in movie box.\n-subsegs-per-sidx N  sets the number of subsegments to be written in each SIDX box. If 0, a single SIDX box is used per segment. If -1, no SIDX box is used. Otherwise, the segmenter will pack N subsegments in the root SIDX of the segment, with DashDuration/N/fragDuration fragments per subsegments. (ISOBMF only)\n-url-template uses SegmentTemplate instead of explicit sources in segments. Ignored if segments are stored in a single file. Set by default for live profiles.\n-daisy-chain uses daisy-chaining of SIDX (1->2->3->4) instead of hierarchical. Ignored if -subseg-per-sidx is 0. (ISOBMF only)\n-single-segment uses a single segment for each representation. Set by default for onDemand profile.\n-single-file uses a single file for each representation.\n-bs-switching MODE sets the bitstream switching mode to one of the following:\ninband (default): generate initialization segments compatible with each representation in the adaptation set by using inband carriage of video parameter sets (avc3, hev1)\nmerge: generate initialization segments compatible with each representation in the adaptation set by merging video parameter sets in a single configuration, if possible. If not possible, defaults to no\nmulti: generate a single init segment with multiple sample description entries for each track (cf HbbTV specification).\nno: bitstream switching mode not used. Turned on by default if a single input is dashed\nsingle forces inband mode when a single input is used.\nThis option is only used for ISOBMF inputs. The segmenter always assumes that MPEG-2 TS input use bitstream switching.\n-moof-sn N sets sequence number of first moof to N\n-tfdt N sets TFDT of first traf to N in SCALE units (cf -dash-scale)\n-no-frags-default disables default flags in fragments\n-single-traf uses a single track fragment per moof (smooth streaming and derived specs may require this).\n-dash-ts-prog N program_number to be considered in case of an MPTS input file..\nIt is possible to feed MP4Box with a set of ISOBMF files containing different media: MP4Box will generate multiple adaptation sets at once for ISOBMF. The different input files are filtered based on their media type, PAR, language and codec, and gathered in different adaptation sets. Media streams of the same type but with different properties are tagged as belonging to the same group through the @group attribute.\nThe files may be assigned periods, descriptors and other options using the [:OPT] suffix. Specific tracks may be loaded as a single representation using fragments.The following fragments and options are defined:\n#trackID=N only uses the track ID N from the source file\n#video only uses the first video track from the source file\n#audio only uses the first audio track from the source file\n:id=NAME sets the representation ID to NAME\n:period=NAME sets the representation's period to NAME. Multiple periods may be used. Periods appear in the MPD in the same order as specified with this option.\n:BaseURL=NAME sets the BaseURL. Set multiple times for multiple BaseURLs.\n:bandwidth=VALUE sets the representation's bandwidth to a given value.\n:xlink=VALUE sets the xlink value for the period containing this element. Only the xlink declared on the first rep of a period will be used.\n:duration=VALUE Increases the duration of this period by the given duration in seconds. Only used when no input media is specified (remote period insertion), e.g. when using :period=X:xlink=Z:duration=Y as an input. If used on a regular input source, this overrides the target segment duration for this representation to VALUE, expressed in dash timescale (cf -dash-scale); this will potentially create non time aligned segments.\n:role=VALUE sets the role of this representation (cf DASH spec). Media with different roles belong to different adaptation sets.\n:desc_p=VALUE adds a descriptor at the Period level. Value must be a properly formatted XML element.\n:desc_as=VALUE adds a descriptor at the AdaptationSet level. Value must be a properly formatted XML element. Two input files with different values will be in different AdaptationSet elements.\n:desc_as_c=VALUE adds a descriptor at the AdaptationSet level. Value must be a properly formatted XML element. Value is ignored while creating AdaptationSet elements.\n:desc_rep=VALUE adds a descriptor at the Representation level. Value must be a properly formatted XML element. Value is ignored while creating AdaptationSet elements. \n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/dash/DASH-Support-in-MP4Box/"},{"date_scraped_timestamp":1720188098866,"host":"wiki.gpac.io","page_title":"MP4Box vs gpac - GPAC wiki","text":"\n \n \n \n \n \n \nMP4Box vs gpac¶\nFollowing the introduction of the filter architecture and the gpac application, you may have a hard time choosing between MP4Box and gpac.\nBefore going any further, we assume:\nyou are familiar with MP4Box\nyou understand the principles of GPAC filters and are somehow familiar with using the gpac application\nWe recommend that you quickly read the article on GPAC re-architecture.\nMP4Box, not gpac¶\nThere are many features of libgpac available in MP4Box only, and most of them will probably never be ported to the general filter architecture.\nThe things you can do with both MP4Box and gpac are:\nadding media tracks or image items to a new ISOBMFF file\nextracting media track to raw formats\nfragmenting and DASHing a set of sources (ISOBMFF or not)\nencrypting or decrypting an ISOBMFF file\nsplitting an ISOBMF file\nsome XML dump operations (-dnal option of MP4Box)\nFile concatenation can also be done using MP4Box as well as with gpac, but they do not use the same code base:\nMP4Box only concatenates ISOBMFF files, potentially requiring temporary ISOBMFF import\ngpac can concatenate any source (live or not) using the flist filter.\nAll the rest is MP4Box-specific and unaware of the filter architecture.\nMany operations in MP4Box, such as tagging, track info and timing modifications, are moreover optimized to reduce rewrite time, whereas gpac always completely rewrites the output.\nMP4Box general processing¶\nTo have a better understanding of which one to choose for a given scenario, we first need to have a quick overview of both apps:\nThe gpac application is only in charge of calling a filter session based on the filters passed as arguments, regardless of their numbers/types/etc. \nMP4Box works in a completely different way to allow for ISOBMFF file edition. These are the logical steps in MP4Box processing, in their order of execution:\nIf -add / -cat, then:\nrun a filter session for each import (-add) operation. This may be optimized when creating a new file using -newfs, in which case a single session is used for all import operations.\nstore the result in a temporary file (unless -flat is set )\nThe input file is now either the source file (read-only or edit operations) or the edited file, potentially with new tracks\nIf -split, then:\nrun a filter session on the input file for file splitting using the reframer filter\nIf -raw, then:\nrun a filter session on the input file for each track to dump (usually involving the writegen filter)\nIf -add-image, then:\nrun a filter session with the target source adding the track to the input file, convert desired samples to items and remove added track\nIf -dash, then:\nrun a filter session on each input file names using the dasher filter\nexit\nIf -crypt or -decrypt , then:\nrun a filter session for file encryption/decryption (potentially using fragmented mode)\nexit\nIf -frag, then:\nrun a filter session for fragmentation\nexit\nThese sessions are separated, hence combining them will result in a longer processing time than doing the same operation with gpac.\nLet's take the following example: add one video, two audios and encrypt the result. Using MP4Box:\nMP4Box -add video.264:options -add audio_en.264:options -add audio_fr.264:options -crypt DRM.xml -new result.mp4\nThis will use 3 filter sessions for the import operations (one if you use -newfs instead of -new), creating a temporary edition file containing all media data, then use a filter session for the encryption.\nThe intermediate file is only deleted once the encryption session is done.\nThis can be slightly faster with less IOs and no temporary edit file using gpac:\ngpac -i video.264:options -i audio_en.264:options -i audio_fr.264:options cecrypt:cfile=DRM.xml -o result.mp4\nIn both cases, we still use temporary storage for the final file interleaving. So let's add fragmentation to avoid this:\nMP4Box -add video.264:options -add audio_en.264:options -add audio_fr.264:options -frag 100 -crypt DRM.xml -new result.mp4\nWe got rid of the temporary storage due do file interleaving, but we still need an intermediate file to store the import result.\ngpac -i video.264:options -i audio_en.264:options -i audio_fr.264:options cecrypt:cfile=DRM.xml -o result.mp4:frag:cdur=100\nWe use no longer use temporary storage.\nNote that these two are strictly equivalent in terms of processing:\nMP4Box -add video.264:options -add audio_en.264:options -add audio_fr.264:options -newfs result.mp4\ngpac -i video.264:options -i audio_en.264:options -i audio_fr.264:options -o result.mp4\nMP4Box and filters¶\nThe track import syntax and the dashing syntax may be combined with filter declarations, as discussed here.\nThey are however restricted as follows:\nfor importing, the destination format is always ISOBMFF\nthe filter chain described is fairly simple, going from source to destination (mp4mx or dasher filters) without any possible branch in-between.\nFor example:\nsource_vid -> rescale -> encode1 \\\n -> encode2 -> dasher\n -> encode3 /\nThis cannot be described using MP4Box, this must be converted into something like:\nsource_vid -> rescale -> encode1 -> dasher\nsource_vid -> rescale -> encode2 /\nsource_vid -> rescale -> encode3 /\nThe syntax will be:\nMP4Box -dash 2000 video.264:@ffsws:osize=1280x720:@enc:c=avc:fintra=2:b=4m:@@ffsws:osize=1280x720:@enc:c=avc:fintra=2:b=2m:@@ffsws:osize=1280x720:@enc:c=avc:fintra=2:b=1m -out result.mp4\nThis will work as expected, but with three rescaler filters doing all the same thing (and a heavy syntax).\nUsing gpac, this is much simpler\ngpac -i video.264 ffsws:osize=1280x720 --fintra=2 enc:c=avc:b=4m @@1 enc:c=avc:b=2m @@1 enc:c=avc:b=1m -o result.mp4\nMP4Box and live sources¶\nMP4Box is not designed to deal with live sources and its processing cannot be interrupted.\nOn the other hand, gpac can be interrupted using ctrl+c and the current session flushed to save the results of what has been done so far.\nSubtle differences in gpac¶\nThere are some subtle differences between MP4Box and gpac that may result in different behavior. Some of these are:\nDefault interleaving in MP4Box is 500ms, while it is 1000ms in gpac\ndeps option is set by default on mp4mx, you must set in on sources in gpac\nHandler name is not set by -for-test in gpac, for that you need to set #HandlerName=... property on the source\ngpac, not MP4Box¶\nYou should use gpac rather than MP4Box in the following cases:\nsources are either live or simulate live (running forever)\noutputs are not local files: HTTP output, RTSP server, ROUTE output, etc...\nthere are many filters manually specified in the pipeline\nmonitoring of the filter session (gpac -r)\nthe MP4Box pipeline combines several filter-session related aspects (-dash, -split, -crypt, -add, -frag)\nneed for complex filter graph connections, reusing non-source filters in the graph\nthe pipeline has multiple outputs (e.g. dash/hls and live TS multicast)\nthe pipeline processes an unknown number of streams that must be redirected to different outputs, requiring URL templating.\npipeline involves playback or composition, using vout, aout or compositor filters\ninput (live or not) inspection before ISOBMFF packaging\nremux input(s) without ISOBMFF conversion (e.g. audio + video to MPEG2 TS)\ndistributed processing (using GSF or other formats through pipes, sockets or other means)\nany other complex use case we never thought of :)\nAnother good reason to use gpac to test a pipeline is when you plan on using GPAC bindings in Python or NodeJS scripts.\n- these bindings are indeed very close to gpac logic, and moving from a gpac command line to the bindings is trivial\n- rewriting an MP4Box pipeline to the bindings will first require rewriting the MP4Box command line as a gpac one.\nCheck the gpac one-liner page for more examples !\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/gpac-mp4box/"},{"date_scraped_timestamp":1720187962672,"host":"wiki.gpac.io","page_title":"Media Export and Dump - GPAC wiki","text":"\n \n \n \n \n \n \nMP4Box can be used to extract media tracks from MP4 files. If you need to convert these tracks however, please check the filters doc. \nOptions:\n-raw (string): extract given track in raw format when supported. Use tkID:output=FileName to set output file name\n-raws (string): extract each sample of the given track to a file. Use tkID:N to extract the Nth sample\n-nhnt (int): extract given track to NHNT format\n-nhml (string): extract given track to NHML format. Use tkID:full for full NHML dump with all packet properties\n-webvtt-raw (string): extract given track as raw media in WebVTT as metadata. Use tkID:embedded to include media data in the WebVTT file\n-single (int): extract given track to a new mp4 file\n-six (int): extract given track as raw media in experimental XML streaming instructions\n-mux (string): multiplex input file to given destination\n-qcp (int): same as -raw but defaults to QCP file for EVRC/SMV\n-saf: multiplex input file to SAF multiplex\n-dvbhdemux: demultiplex DVB-H file into IP Datagrams sent on the network\n-raw-layer (int): same as -raw but skips SVC/MVC/LHVC extractors when extracting\n-diod: extract file IOD in raw format\n-mpd (string): convert given HLS or smooth manifest (local or remote http) to MPD - options -url-template and -segment-timelinecan be used in this mode. \nNote: This only provides basic conversion, for more advanced conversions, see gpac -h dasher \nWarning: This is not compatible with other DASH options and does not convert associated segments \nFile Dumping¶\nMP4Box has many dump functionalities, from simple track listing to more complete reporting of special tracks. \nOptions:\n-std: dump/write to stdout and assume stdout is opened in binary mode\n-stdb: dump/write to stdout and try to reopen stdout in binary mode\n-tracks: print the number of tracks on stdout\n-info (string): print movie info (no parameter) or track extended info with specified ID\n-infon (string): print track info for given track number, 1 being the first track in the file\n-infox: print movie and track extended info (same as -info Nfor each track)\n-diso,-dmp4: dump IsoMedia file boxes in XML output\n-dxml: dump IsoMedia file boxes and known track samples in XML output\n-disox: dump IsoMedia file boxes except sample tables in XML output\n-keep-ods: do not translate ISOM ODs and ESDs tags (debug purpose only)\n-bt: dump scene to BT format\n-xmt: dump scene to XMT format\n-wrl: dump scene to VRML format\n-x3d: dump scene to X3D XML format\n-x3dv: dump scene to X3D VRML format\n-lsr: dump scene to LASeR XML (XSR) format\n-svg: dump scene to SVG\n-drtp: dump rtp hint samples structure to XML output\n-dts: print sample timing, size and position in file to text output\n-dtsx: same as -dts but does not print offset\n-dtsc: same as -dts but analyses each sample for duplicated dts/cts (slow !)\n-dtsxc: same as -dtsc but does not print offset (slow !)\n-dnal (int): print NAL sample info of given track\n-dnalc (int): print NAL sample info of given track, adding CRC for each nal\n-dnald (int): print NAL sample info of given track without DTS and CTS info\n-dnalx (int): print NAL sample info of given track without DTS and CTS info and adding CRC for each nal\n-sdp: dump SDP description of hinted file\n-dsap (int): dump DASH SAP cues (see -cues) for a given track\n-dsaps (int): same as -dsap but only print sample number\n-dsapc (int): same as -dsap but only print CTS\n-dsapd (int): same as -dsap but only print DTS\n-dsapp (int): same as -dsap but only print presentation time\n-dcr: dump ISMACryp samples structure to XML output\n-dchunk: dump chunk info\n-dump-cover: extract cover art\n-dump-chap: extract chapter file as TTXT format\n-dump-chap-ogg: extract chapter file as OGG format\n-dump-chap-zoom: extract chapter file as zoom format\n-dump-udta [tkID:]4cc: extract user data for the given 4CC. If tkID is given, dumps from UDTA of the given track ID, otherwise moov is used\n-mergevtt: merge vtt cues while dumping\n-ttxt (int): convert input subtitle to GPAC TTXT format if no parameter. Otherwise, dump given text track to GPAC TTXT format\n-srt (int): convert input subtitle to SRT format if no parameter. Otherwise, dump given text track to SRT format\n-nstat: generate node/field statistics for scene\n-nstats: generate node/field statistics per Access Unit\n-nstatx: generate node/field statistics for scene after each AU\n-hash: generate SHA-1 Hash of the input file\n-comp (string): replace with compressed version all top level box types given as parameter, formatted as orig_4cc_1=comp_4cc_1[,orig_4cc_2=comp_4cc_2]\n-topcount (string): print to stdout the number of top-level boxes matching box types given as parameter, formatted as 4cc_1,4cc_2N\n-topsize (string): print to stdout the number of bytes of top-level boxes matching types given as parameter, formatted as 4cc_1,4cc_2N or all for all boxes\n-bin: convert input XML file using NHML bitstream syntax to binary\n-mpd-rip: fetch MPD and segment to disk\n-udp-write (string, default: IP[:port]): write input name to UDP (default port 2345)\n-raw-cat (string): raw concatenation of given file with input file\n-wget (string): fetch resource from http(s) URL\n-dm2ts: dump timing of an input MPEG-2 TS stream sample timing\n-check-xml: check XML output format for -dnal, -diso and -dxml options\n-fuzz-chk: open file without probing and exit (for fuzz tests) \nFile splitting¶\nMP4Box can split input files by size, duration or extract a given part of the file to new IsoMedia file(s).\nThis requires that at most one track in the input file has non random-access points (typically one video track at most).\nSplitting will ignore all MPEG-4 Systems tracks and hint tracks, but will try to split private media tracks.\nThe input file must have enough random access points in order to be split. If this is not the case, you will have to re-encode the content.\nYou can add media to a file and split it in the same pass. In this case, the destination file (the one which would be obtained without splitting) will not be stored. \nTime ranges are specified as follows: \nS-E: S start and E end times, formatted as HH:MM:SS.ms, MM:SS.ms or time in seconds (int, double, fraction) \nS:E: S start time and E end times in seconds (int, double, fraction). If E is prefixed with D, this sets E = S + time \nS:end or S:end-N: S start time in seconds (int, double), N number of seconds (int, double) before the end \nMP4Box splitting runs a filter session using the reframer filter as follows: \nsplitrange option of the reframer is always set \nsource is demultiplexed with alltk option set \nstart and end ranges are passed to xs and xe options of the reframer \nfor -splitz, options xadjust and xround=after are enforced \nfor -splitg, options xadjust and xround=before are enforced \nfor -splitf, option xround=seek is enforced and propbe_refset if not specified at prompt \nfor -splitx, option xround=closest and propbe_ref are enforced if not specified at prompt \nThe default output storage mode is to full interleave and will require a temp file for each output. This behavior can be modified using -flat, -newfs, -inter and -frag.\nThe output file name(s) can be specified using -out and templates (e.g. -out split$num%04d$.mp4 produces split0001.mp4, split0002.mp4, ...).\nMultiple time ranges can be specified as a comma-separated list for -splitx, -splitz and -splitg. \n-split (string): split in files of given max duration (float number) in seconds. A trailing unit can be specified: \nM, m: duration is in minutes \nH, h: size is in hours \n-split-rap,-splitr (string): split in files at each new RAP\n-split-size,-splits (string): split in files of given max size (integer number) in kilobytes. A trailing unit can be specified: \nM, m: size is in megabytes \nG, g: size is in gigabytes \n-split-chunk,-splitx (string): extract the specified time range as follows: \nthe start time is moved to the RAP sample closest to the specified start time \nthe end time is kept as requested \n-splitz (string): extract the specified time range so that ranges A:B and B:C share exactly the same boundary B: \nthe start time is moved to the RAP sample at or after the specified start time \nthe end time is moved to the frame preceding the RAP sample at or following the specified end time \n-splitg (string): extract the specified time range as follows: \nthe start time is moved to the RAP sample at or before the specified start time \nthe end time is moved to the frame preceding the RAP sample at or following the specified end time \n-splitf (string): extract the specified time range and insert edits such that the extracted output is exactly the specified range \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-dump-opts/"},{"date_scraped_timestamp":1720187964109,"host":"wiki.gpac.io","page_title":"Multiview rendering - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC can support various output type and view number, but requires OpenGL to do so. \nWarning, multi-view rendering is a costly operation. The scene gets rendered multiple times, and you will need a decent graphic card to try that, especially support for VBOs to avoid sending the geometry data repeatedly, and OpenGL shaders for most screen configurations. \nThe output type is selected in GPAC configuration file by the following keys\n[filter@compositor]\nnbviews=2\nrview=no\nstereo=custom\nmvshader=/path/to/fragmentshader_sourcecode.glsl\ndispdist=180\ncamlay=circular\nSee gpac -hh compositor for documentation of these options.\nWhen shaders are used, each view X in [1, nbviews] is exposed to the shader as:\n uniform sampler2D gfViewX\nFor example, the column interleaving in GPAC is done with a built-in shader whose code is:\nuniform sampler2D gfView1;\nuniform sampler2D gfView2;\nvoid main(void) {\n if ( int( mod(gl_FragCoord.x, 2.0) ) == 0)\n gl_FragColor = texture2D(gfView1, gl_TexCoord[0].st);\n else\n gl_FragColor = texture2D(gfView2, gl_TexCoord[0].st);\n}\ncamlay defines how the camera is positioned in the 3D world. It can take the value offaxis (default), circular or linear - you can get pretty good overviews of this on the web. dispdist specifies the nominal viewing distance of the auto-stereoscopic display. GPAC will use that to compute each view's camera position in the virtual world. \nWe started some experiments on automatic calibration of 3D models, but it is not yet fully tested and may results in too high disparity. Don't worry, this can be adjusted at run-time: use Alt+shift+UP/Down to modify the inter-ocular distance (by default 6.8 cm) and Alt+shift+UP/Down to modify the view distance.   \nHere are some videos showing some of these modes. \nhttp://www.youtube.com/watch?v=_8sZc3dL9ds\nOriginal scene designed during the Triscope project, in which we built an auto-stereoscopic mobile prototype.\nhttp://www.youtube.com/watch?v=7bNHkuFBg0c\nAnaglyph version of the Triscope demo.\nhttp://www.youtube.com/watch?v=6kzK54XiRXw\nSide-by-Side version of the Triscope Demo.\nCustom interleaving version of the Triscope Demo for a 5 views display.\nThe demo is written in BIFS, and uses a hardcoded proto (e.g. gpac-specific) node called DepthGroup for the 2D menu. This node allows to specify depth translation and scaling factors to a sub-tree, which are then accumulated when going down the sub-tree. We could also have used Transform3D nodes, but we didn't want to have too many 3D matrices to handle (the original prototype platform was not that powerful). To declare such a node in a BT file, you need to insert the following code before your scene:\nEXTERNPROTO DepthGroup [\nexposedField MFNode children []\nexposedField SFInt32 _3d_type 4\nexposedField SFFloat depth_gain 1.0\nexposedField SFFloat depth_offset 0.0\n ] \"urn:inet:gpac:builtin:DepthGroup\"\nYou can ignore the _3d_type field, which was specific to the hardware part of the prototype.\nThe resulting depth value is scaled by a constant defined in GPAC configuration file:\n[Compositor]\nDepthScale=100\nThis is still very preliminary work, automatic scaling is planed in the near future.\nSupport for SVG and depth has also been investigated, but this work is not yet integrated in GPAC.\nFor images, we defined a new image format called PNGD. These are regular PNG files where the alpha channel is interpreted as the depth map of the RGB channels. These files can be played by GPAC whether in stereo-mode or not. \nFor video, we defined a new (not standard yet) MPEG-4 descriptor defining a video stream as carrying the depth map of another video stream. This is in-sync with approaches based on MPEG-C Part 3. You can find a sample BT file showing this here, then use MP4Box to produce the mp4 file. \nThe playback of video+depth or image+depth is in its early stage in GPAC, you will need a BIG machine and a recent graphics card to get good results. Don't expect too much from high resolution video yet, they will likely overkill the player. The reason for this ? The video data is sent as as a point sprite, triangle strip elevation grid or vertex buffer, and this costs a lot (this will have to be improved in the future). \nThe way depth video is rendered is defined by the following option:\n[filter@compositor]\ndepth_gl_type=none\ndepth_gl_type take the following value:\nnone: video is rendered as a flat rectangle\npoint: video is rendered as a point sprite if GL_ARB_point_parameters extension is supported\nstrip: video is rendered as a triangle strip. You may specify \"Strips=V\" as an option; this will perform two-pass rendering and V the cut-off value for the depth (expressed between 0 and 255)\nThe height of each vertex is taken from the depth map and multiplied by the DepthScale value. Note that you don't need stereo-rendering to play with this: http://www.youtube.com/watch?v=-AuaU1Eq5Xg \nWhen playing depth+video or depth+image, GPAC will automatically switch to OpenGL rendering if the depth_gl_type option is not none. \n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Some-fun-with-multiview-rendering-on-3D-screens/?q="},{"date_scraped_timestamp":1720187939772,"host":"wiki.gpac.io","page_title":"Multiview rendering - GPAC wiki","text":"\n \n \n \n \n \n \nGPAC can support various output type and view number, but requires OpenGL to do so. \nWarning, multi-view rendering is a costly operation. The scene gets rendered multiple times, and you will need a decent graphic card to try that, especially support for VBOs to avoid sending the geometry data repeatedly, and OpenGL shaders for most screen configurations. \nThe output type is selected in GPAC configuration file by the following keys\n[filter@compositor]\nnbviews=2\nrview=no\nstereo=custom\nmvshader=/path/to/fragmentshader_sourcecode.glsl\ndispdist=180\ncamlay=circular\nSee gpac -hh compositor for documentation of these options.\nWhen shaders are used, each view X in [1, nbviews] is exposed to the shader as:\n uniform sampler2D gfViewX\nFor example, the column interleaving in GPAC is done with a built-in shader whose code is:\nuniform sampler2D gfView1;\nuniform sampler2D gfView2;\nvoid main(void) {\n if ( int( mod(gl_FragCoord.x, 2.0) ) == 0)\n gl_FragColor = texture2D(gfView1, gl_TexCoord[0].st);\n else\n gl_FragColor = texture2D(gfView2, gl_TexCoord[0].st);\n}\ncamlay defines how the camera is positioned in the 3D world. It can take the value offaxis (default), circular or linear - you can get pretty good overviews of this on the web. dispdist specifies the nominal viewing distance of the auto-stereoscopic display. GPAC will use that to compute each view's camera position in the virtual world. \nWe started some experiments on automatic calibration of 3D models, but it is not yet fully tested and may results in too high disparity. Don't worry, this can be adjusted at run-time: use Alt+shift+UP/Down to modify the inter-ocular distance (by default 6.8 cm) and Alt+shift+UP/Down to modify the view distance.   \nHere are some videos showing some of these modes. \nhttp://www.youtube.com/watch?v=_8sZc3dL9ds\nOriginal scene designed during the Triscope project, in which we built an auto-stereoscopic mobile prototype.\nhttp://www.youtube.com/watch?v=7bNHkuFBg0c\nAnaglyph version of the Triscope demo.\nhttp://www.youtube.com/watch?v=6kzK54XiRXw\nSide-by-Side version of the Triscope Demo.\nCustom interleaving version of the Triscope Demo for a 5 views display.\nThe demo is written in BIFS, and uses a hardcoded proto (e.g. gpac-specific) node called DepthGroup for the 2D menu. This node allows to specify depth translation and scaling factors to a sub-tree, which are then accumulated when going down the sub-tree. We could also have used Transform3D nodes, but we didn't want to have too many 3D matrices to handle (the original prototype platform was not that powerful). To declare such a node in a BT file, you need to insert the following code before your scene:\nEXTERNPROTO DepthGroup [\nexposedField MFNode children []\nexposedField SFInt32 _3d_type 4\nexposedField SFFloat depth_gain 1.0\nexposedField SFFloat depth_offset 0.0\n ] \"urn:inet:gpac:builtin:DepthGroup\"\nYou can ignore the _3d_type field, which was specific to the hardware part of the prototype.\nThe resulting depth value is scaled by a constant defined in GPAC configuration file:\n[Compositor]\nDepthScale=100\nThis is still very preliminary work, automatic scaling is planed in the near future.\nSupport for SVG and depth has also been investigated, but this work is not yet integrated in GPAC.\nFor images, we defined a new image format called PNGD. These are regular PNG files where the alpha channel is interpreted as the depth map of the RGB channels. These files can be played by GPAC whether in stereo-mode or not. \nFor video, we defined a new (not standard yet) MPEG-4 descriptor defining a video stream as carrying the depth map of another video stream. This is in-sync with approaches based on MPEG-C Part 3. You can find a sample BT file showing this here, then use MP4Box to produce the mp4 file. \nThe playback of video+depth or image+depth is in its early stage in GPAC, you will need a BIG machine and a recent graphics card to get good results. Don't expect too much from high resolution video yet, they will likely overkill the player. The reason for this ? The video data is sent as as a point sprite, triangle strip elevation grid or vertex buffer, and this costs a lot (this will have to be improved in the future). \nThe way depth video is rendered is defined by the following option:\n[filter@compositor]\ndepth_gl_type=none\ndepth_gl_type take the following value:\nnone: video is rendered as a flat rectangle\npoint: video is rendered as a point sprite if GL_ARB_point_parameters extension is supported\nstrip: video is rendered as a triangle strip. You may specify \"Strips=V\" as an option; this will perform two-pass rendering and V the cut-off value for the depth (expressed between 0 and 255)\nThe height of each vertex is taken from the depth map and multiplied by the DepthScale value. Note that you don't need stereo-rendering to play with this: http://www.youtube.com/watch?v=-AuaU1Eq5Xg \nWhen playing depth+video or depth+image, GPAC will automatically switch to OpenGL rendering if the depth_gl_type option is not none. \n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Some-fun-with-multiview-rendering-on-3D-screens/"},{"date_scraped_timestamp":1720188066188,"host":"wiki.gpac.io","page_title":"NHML format - GPAC wiki","text":"\n \n \n \n \n \n \nNHML Overview¶\nThe NHML (Network Hint Markup Language) format has been developed at Telecom Paris in order to provide import support on media formats not natively supported by GPAC, typically for ISOBMFF file constructions. It is an XML translation and extension of a binary format called [[NHNT Format]] used during the development of MPEG-4 Systems.\nThe NHML format is an XML-based description of a single media stream, allowing to specify media stream properties and media samples in GPAC.\nTo obtain some sample NHML files, simply use MP4Box -nhml trackID srcFile\nJust like any XML file, the file must begin with the usual xml header. The file encoding SHALL BE UTF-8.\nThe root element of an NHML file is the NHNTStream element, containing zero or one DecoderSpecificInfo element and zero or more NHNTSample elements.\nThe NHNTStream, NHNTSample,SAI and P elements may use child bitstream constructors, allow assembling bytes and files as needed to construct the desired data.\nNHML Syntax¶\nThe following shows the different elements defined in NHML.\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<NHNTStream ... >\n <!-- decoder config from XML binary -->\n <DecoderSpecificInfo>\n <BS/>\n </DecoderSpecificInfo>\n <!-- regular sample -->\n <NHNTSample ... /> \n <!-- sample building its data from XML binary -->\n <NHNTSample ... >\n <BS ... />\n </NHNTSample>\n <!-- sample with subsample definitions -->\n <NHNTSample ...>\n <SubSamples>\n <SubSample ... />\n <SubSample ... />\n </SubSamples>\n </NHNTSample>\n <!-- sample using XML binary with subsample definitions-->\n <NHNTSample ...>\n <BS dataFile=\"file1\"/>\n <BS dataFile=\"file2\"/>\n <BS value=\"30\" bits=\"16\"/>\n <SubSamples>\n <SubSample sfile=\"file1\" />\n <SubSample size=\"2\" sfile=\"file2\" />\n </SubSamples>\n </NHNTSample>\n <!-- sample with auxiliary info definitions using XML binary -->\n <NHNTSample ...>\n <SAI type=\"fooX\">\n <BS value=\"0\" bits=\"8\"/>\n </SAI>\n </NHNTSample>\n <!-- stream reconfiguration -->\n <NHNTReconfig />\n <!-- sample with sample group description using XML binary -->\n <NHNTSample ...>\n <SAI id=\"g2\" group=\"true\" type=\"barZ\">\n <BS value=\"0\" bits=\"8\"/>\n </SAI>\n </NHNTSample>\n <!-- sample with sample group description reference -->\n <NHNTSample ...>\n <SAI ref=\"g2\"/>\n </NHNTSample>\n <!-- sample using XML binary -->\n <NHNTSample ...>\n <BS value=\"0\" bits=\"8\"/>\n <BS value=\"2\" bits=\"32\"/>\n </NHNTSample>\n <!-- sample with sample group description and using XML binary construction -->\n <NHNTSample ...>\n <Media>\n <BS value=\"24\" bits=\"32\"/>\n </Media> \n <SAI group=\"true\" type=\"barZ\">\n <BS value=\"3\" bits=\"8\"/>\n </SAI>\n </NHNTSample>\n</NHNTStream>\nNHNTStream¶\nOverview¶\nThe NHNTStream element describes stream properties applying throughout the stream.\nDefined Attributes¶\nstreamType : identifies the media streamType as specified in MPEG-4, either the integer value or GPAC name.\nmediaType : indicates the 4CC media type (handler) as used in IsoMedia. Not needed if streamType is specified. Value Type: 4 byte string. Officially supported handler types are listed here.\nmediaSubType : indicates the 4CC media subtype (codec) to use in IsoMedia. This subtype will identify the sample description used (stsd table). Not needed if streamType is specified. Value Type: 4 byte string. Officially supported codec types are listed here.\nobjectTypeIndication : identifies the media type as specified in MPEG-4. For example, 0x40 for MPEG-4 AAC. Officially supported object types are listed here.\ncodecID : codec identifier (needed if neither mediaSubType nor objectTypeIndication is set). Can be a 4CC or GPAC codec name.\ntimeScale : indicates the time scale in which the time stamps are given. Value type: unsigned integer. Default Value: 1000 or sample rate if specified.\nwidth, height : indicates the dimension of a visual media. Ignored if the media is not video (streamType 0x04 or mediaType \"vide\"). Value Type: unsigned integer.\nparNum, parDen : indicates the pixel aspect ratio of a visual media. Ignored if the media is not video (streamType 0x04 or mediaType \"vide\"). Value Type: unsigned integer.\nsampleRate : indicates the sample rate of an audio media. Ignored if the media is not audio (streamType 0x05 or mediaType \"soun\"). Value Type: unsigned integer.\nnumChannels : indicates the number of channels of an audio media. Ignored if the media is not audio (streamType 0x05 or mediaType \"soun\"). Value Type: unsigned integer.\nbaseMediaFile : indicates the default location of the stream data. If not set, the file with the same name and extension .media is assumed to be the source.\nspecificInfoFile : indicates the location of the decoder configuration data if any.\ntrackID : indicates a desired trackID for this media when importing to IsoMedia. Value type: unsigned integer. Default Value: 0.\ninRootOD : indicates if the imported stream is present in the InitialObjectDescriptor. Value type: \"yes\", \"no\". Default Value: \"no\".\nDTS_increment : indicates a default time increment between two consecutive samples. Value type: unsigned integer. Default Value: 0.\nheaderEnd : Number of bytes to copy from specificInfoFile (if not set, use the entire file). Value type: unsigned integer. Default Value: 0.\ngzipSamples : compress samples using gzip, deflate or none. Default Value: none.\ngzipDictionary : gzip dictionary to use, either self or path to dict file. Value type: string. Default Value: null.\nThe following attributes are used when creating custom sample description in IsoMedia (i.e. not natively supported by GPAC). Their semantics are given in the QT (and IsoMedia) file format specification.\ncompressorName : compressor name. Value type: string. Default Value: null.\ncodecVersion : codec version. Value type: unsigned integer. Default Value: 0.\ncodecRevision : codec revision. Value type: unsigned integer. Default Value: 0.\ncodecVendor : 4CC of codec vendor. Value type: 4 byte string. Default Value: null.\ntemporalQuality : temporal quality. Value type: unsigned integer. Default Value: 0.\nspatialQuality : spatial quality. Value type: unsigned integer. Default Value: 0.\nhorizontalResolution : horizontal resolution. Value type: unsigned integer. Default Value: 0.\nverticalResolution : vertical resolution. Value type: unsigned integer. Default Value: 0.\nbitDepth : vertical resolution. Value type: unsigned integer. Default Value: 0.\nbitsPerSample : indicates the number of bits per audio sample for an audio media. Ignored if the media is not audio (streamType 0x05 or mediaType \"soun\"). Value Type: unsigned integer.\nThe following attributes are used when creating custom XML-base sample descriptions:\nxml_namespace: XML namespace to advertise for XML-base sample entries. Type: string. Default Value: unspecified.\nxml_schema_location: XML schema location to advertise for XML-base sample entries. Type: string. Default Value: unspecified.\nxmlHeaderEnd: Name of element at the end of the header. The XML source will be copied from first byte to end of this element to create the decoder specific info. Type: string. Default Value: unspecified.\ntext_encoding, encoding: text encoding. Type: string. Default Value: null.\ncontent_encoding: content encoding (gzip, deflate or none accepted). This indicates the compression of the source (no compression is done by importer). Type: string. Default Value: null.\nauxiliaryMimeTypes : auxiliary data mime types (for TTML images). Value type: string. Default Value: null.\nThe following attributes are used when creating 3GPP DIMS sample descriptions:\nprofile : DIMS profile. Type: unsigned integer. Default Value: 0.\nlevel : DIMS level. Type: unsigned integer. Default Value: 0.\npathComponents : DIMS/LASeR path components. Type: unsigned integer. Default Value: 0.\nuseFullRequestHost: DIMS full request host flag. Type: boolean. Default Value: no.\ncontains_redundant: DIMS redundant, either main , redundant or main+redundant. Default Value: unspecified.\ncontent_script_types: DIMS mime type for scripts. Type: string. Default Value: unspecified.\ntext_encoding, encoding: text encoding. Type: string. Default Value: null.\ncontent_encoding: content encoding (gzip, deflate or none accepted). This indicates the compression of the source (no compression is done by importer). Type: string. Default Value: null.\nThe decoder config of an NHNTStream can be specified using XML bitstream constructors. To do this, the BS elements shall be encapsulated in a DecoderSpecificInfo element present in the children of the NHNTStream element. The content of the DecoderSpecificInfo element is then inserted:\nin the ESD (MPEG-4 Systems)\nor after the base sampleDescription (ISOBMFF generic), in which case the data should likely be formatted as a box (4 byte size, 4 byte type then payload).\nNHNTSample¶\nOverview¶\nThe NHNTSample element describes an access unit, or a fragment of an access unit.\nEach element may contain zero or one SubSamples element.\nEach element may contain zero or more SAI elements.\nIf an NHNTSample has the same decode time than the previous NHNTSample, it is a continuation fragment of the frame started at that time. In this case, the fragment packet has no timing info, no SAP info and no dependency info. Subsamples and side data should not be used in this case.\nDefined Attributes¶\nDTS or time : decoding time stamp of the sample. If not set, the previous sample DTS (or 0) plus the specified DTS_increment or the previous sample duration is used. Value type: unsigned integer or HH:MM:SS.ms. Default Value: 0.\nduration : sets the duration of the sample. The duration set on the last sample will change the track duration. Default Value: 0.\nCTSOffset : offset between the decoding and the composition time stamp of the sample. Value type: unsigned integer. Default Value: 0.\nisRAP : indicates if the sample is a random access point or not. Value type: \"yes\", \"no\" or SAP type value (0 to 5). Default Value: \"no\".\nisSyncShadow : indicates if the sample is a sync shadow sample (IsoMedia storage only). Value type: \"yes\", \"no\". Default Value: \"no\".\nmediaOffset : indicates the position of the first byte of this sample in the media source file. Value type: unsigned integer. Default Value: 0.\ndataLength : indicates the size of this sample. Value type: unsigned integer. Default Value: 0.\nmediaFile : indicates the media source file to use. If not set, the baseMediaFile is used. If base64, prefix is present in the name, data is loaded from the attribute value. Value type: string. Default Value: null.\nxmlFrom : if the source file is XML data, indicates the location of the first element to copy fom the XML document. The location can be \"doc.start\", \"elt_id.start\" or \"elt_id.end\". Elements are identified through their \"id\", \"xml:id\" or \"DEF\" attributes.\nxmlTo : if the source file is XML data, indicates the location of the last element to copy fom the XML document. The location can be \"doc.end\", \"elt_id.start\" or \"elt_id.end\". Elements are identified through their \"id\", \"xml:id\" or \"DEF\" attributes.\nis-Scene : DIMS is-scene flag. Value type: boolean. Default Value: no.\nis-RAP : DIMS is-rap flag. Value type: boolean. Default Value: no.\nis-redundant : DIMS is-redundant flag. Value type: boolean. Default Value: no.\nredundant-exit : DIMS redundant-exit flag. Value type: boolean. Default Value: no.\npriority : DIMS priority flag. Value type: boolean. Default Value: no.\ncompress : DIMS compress flag. Value type: boolean. Default Value: no.\nA NHNTSample can use XML bitstream constructors in its children. If it does, the sample data (if any) is extended with the result of the binarized XML of the children if no Media child is present, or with the result of the binarized XML of the Media child.\nThe Media wrapping is required when combining both XML construction for sample data and SAI children.\nWARNING Support for Media wrapping requires GPAC 2.0 or above.\nSubSamples¶\nOverview¶\nThe SubSamples element is used to give subsample information for the parent sample.\nA SubSamples element contains one or more SubSample element.\nWARNING Only the first SubSamples element in an NHNTSample is parsed at the current time.\nWARNING Support for SubSamples requires GPAC 2.0 or above.\nDefined Attributes for SubSamples element¶\nflags : flags for the subsample box, default is 0\nDefined Attributes for SubSample element¶\nsize : size in bytes of the subsample, 0 by default\nfsize : add to size the size of the file indicated by this attribute, default is NULL\ntextmode : if fsize is set, open the file in text mode if this attribute is set to yes, true or 1 , default is no\ndiscardable : indicates if the subsample data is discardable (value yes, true or 1), default is no\npriority : indicates the subsample data priority , default is 0\ncodec_info : indicates the codec specific parameters (default is 0) for the subsample, as unsigned int or as hexadecimal data (prefixed with 0x)\nSample Auxiliary Information¶\nDefinition¶\nThe SAI element is used to associate auxiliary information to the parent sample. The children of this element must use bitstream constructors to describe the data.\nAuxiliary information will be tanslated by the ISOBMFF multiplexer as:\nsample group description with grouping_type value of type if group is set\nsample auxiliary information with aux_info_type value of type if group is not set\nAn SAI element can be a reference to another SAI element, in which case the data is generated from the reference element. This avoids repeating the structure when reused (sample groups).\nSeveral SAI can be specified for different auxiliary types.\nWARNING Support for SAI requires GPAC 2.0 or above.\nDefined Attributes¶\nid : string assigning ID to this SAI for later reuse\nref : string indicating a defined SAI to use. The first SAI found (starting from first defined sample) with matching ID will be used\ngroup : boolean. If set to yes, true or 1, indicates that this auxiliary info must be signaled as sample grouping in ISOBMFF, otherwise auxiliary sample info is used\ntype : 4CC of the auxiliary data or of the sample group\naux_info : 32bit integer for the auxiliary data (aux_info_type_parameterin ISOBMFF) or for the sample grouping (grouping_type_parameter in ISOBMFF)\nExample¶\n<NHNTSample>\n <SAI type=\"fooZ\" aux_info=\"0\" group=\"1\">\n <BS value=\"0\" bits=\"8\"/>\n </SAI>\n</NHNTSample>\nThis defines a sample grouping with type GPAC and a value of '0' on 8 bits.\n<NHNTSample>\n <SAI type=\"barZ\" aux_info=\"0\">\n <BS value=\"1\" bits=\"8\"/>\n </SAI>\n</NHNTSample>\nThis defines a sample auxiliary data with type GPAC and a value of '1' on 8 bits.\nProperties¶\nThe Properties element can be used to set packet or PID properties as used in GPAC filters.\nThe element is a list of P elements. Each P element can be a regular property (built-in or user defined), or can use bitstream constructors to construct the property data.\nFor NHNTStream element, the first Properties child element declared before the first NHNTSample element is used, if any.\nFor NHNTSample element, the first Properties child element is used, if any.\nWARNING Support for Properties requires GPAC 2.0 or above.\nDefined Attributes for Properties¶\nid: assigns an ID of the element \nref: uses the first Properties element with given ID found in the document.\nDefined Attributes for P¶\ntype: property type. For non built-in properties, defaults to string or to data if bitstream constructors are used. Ignored for built-in properties.\nname: property name\nvalue: property value\nid: assigns an ID of the element \nref: uses the first P element with given ID found in the document.\nIf name attribute is not set, the first non-recognized attribute name is the property name and its value the property value.\nExamples¶\n<Properties>\n <P name=\"foo\" value=\"32\" type=\"uint\"/>\n <P myProp=\"Toto\">\n <BS value=\"3\" bits=\"8\"/>\n </P>\n</Properties>\nNHNTReconfig¶\nOverview¶\nThe NHNTReconfig element reconfigures the media stream properties between samples. It may happen as often as desired, and has exactly the same syntax as the NHNTStream element. \nIf a DecoderSpecificInfo element is needed, it must be set as a child of the NHNTReconfig element.\n<NHNTReconfig ...>\n <DecoderSpecificInfo>\n <BS .../>\n </DecoderSpecificInfo>\n</NHNTReconfig>\nIf Propertieselement is needed, it must be set as a child of the NHNTReconfig element.\n<NHNTReconfig ...>\n <Properties>\n <P .../>\n </Properties>\n</NHNTReconfig>\nWARNING Support for NHNTReconfig requires GPAC 2.0 or above.\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/NHML-Format/"},{"date_scraped_timestamp":1720187981000,"host":"wiki.gpac.io","page_title":"NHNT format - GPAC wiki","text":"\n \n \n \n \n \n \nNHNT Overview¶\nThe NHNT format has been developed during the MPEG-4 Systems implementation phase, as a way to easily mux unknown media formats to an MP4 file or an MPEG-4 multiplex. The goal was to have the media encoder produce a description of the media time fragmentation (access units and timestamps) that could be reused by a media-unaware MPEG-4 multiplexer.\nA NHNT source is composed of 2 or 3 parts:\nthe media file: This file contains all the media data as written by the encoder. The file extension must be .media.\nthe NHNT (meta) file: This file contains all the information needed by the MPEG-4 multiplexer to use the media data. The file extension must be .nhnt.\nthe decoder initialization file: If the media format requires decoder configuration data (MPEG-4 Visual, AAC, AVC/H264, ...), the binary data is put in this third file in order for the MPEG-4 multiplexer to correctly signal decoder configuration. This is required by the fact that in MPEG-4 Systems, configuration data is never sent in-band of the media stream, but through the object descriptor stream. The file extension must be .info.\nThe NHNT file format¶\nA NHNT file is made of a header, and a set of access units descriptors. All integers are written in network-byte order.\nchar Signature[4];\nbit(8) version;\nbit(8) streamType;\nbit(8) objectTypeIndication;\nbit(16) reserved = 0;\nbit(24) bufferSizeDB;\nbit(32) avgBitRate;\nbit(32) maxBitRate;\nbit(32) timeStampResolution;\nSemantics¶\nSignature : identifies the file as an NHNT file. The signature must be 'NHnt' or 'NHnl' for large files (using 64 bits offsets and timestamps).\nversion : identifies the NHNT version used to produce the file. Default version is 0.\nstreamType : identifies the media streamType as specified in MPEG-4 (0x04: Visual, 0x05: audio, ...). Officially supported stream types are listed here.\nobjectTypeIndication : identifies the media type as specified in MPEG-4. For example, 0x40 for MPEG-4 AAC. Officially supported object types are listed here.\nbufferSizeDB : indicates the size of the decoding buffer for this stream in byte.\navgBitRate : indicates the average bitrate in bits per second of this elementary stream. For streams with variable bitrate this value shall be set to zero.\nmaxBitRate : indicates the maximum bitrate in bits per second of this elementary stream in any time window of one second duration.\ntimeStampResolution : indicates the unit in which the media timestamps are expressed in the file (timeStampResolution ticks = 1 second).\nAfter the header, the file is just a succession of access unit (sample) info until the end of the file.\nbit(24) data_size;\nbit(1) random_access_point;\nbit(1) au_start_flag;\nbit(1) au_end_flag;\nbit(3) reserved = 0;\nbit(2) frame_type;\nbit(32) file_offset;\nbit(32) compositionTimeStamp;\nbit(32) decodingTimeStamp;\nbit(24) data_size;\nbit(1) random_access_point;\nbit(1) au_start_flag;\nbit(1) au_end_flag;\nbit(3) reserved = 0;\nbit(2) frame_type;\nbit(64) file_offset;\nbit(64) compositionTimeStamp;\nbit(64) decodingTimeStamp;\nSemantics¶\ndata_size : indicates the amount of data to fetch from the source file for this access unit.\nrandom_access_point : indicates if the access unit is a random access point.\nau_start_flag : indicates if this is the start of an access unit or not.\nau_end_flag : indicates if this is the end of an access unit or not.\nframe_type : Used for bidirectional video coding sources only, 0 otherwise.\nframe_type=2: access unit is a B-frame\nframe_type=1: access unit is a P-frame\nframe_type=0: access unit is an I-frame\nfile_offset : indicates the position in the source file of the first byte to fetch for this data chunk.\ncompositionTimeStamp : indicates the composition (presentation) time stamp of this access unit.\ndecodingTimeStamp : indicates the decoding time stamp of this access unit.\nNote : Samples must be described in decoding order in the nhnt file when using sample fragmentation. Otherwise, sample may be described out of order.\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/NHNT-Format/"},{"date_scraped_timestamp":1720188054272,"host":"wiki.gpac.io","page_title":"OMA DRM - GPAC wiki","text":"\n \n \n \n \n \n \nIn order to encrypt a 3GP/MP4 file into a PDCF file, MP4Box uses the same process as CENC or ISMA encryption, only the drm file syntax changes.\nThis page documents OMA-specific options of the DRM file, see Common Encryption for more details on generic options.\nAn CryptTrack element may have children describing the optional textual headers defined in OMA DRM 2.0. Each textual header is inserted as is during OMA encryption, so be careful not to specify twice the same header. More information on textual headers can be found in the OMA (P)DCF specification.\nXML Syntax¶\n<GPACDRM type=\"OMA\">\n <CryptTrack trackID=\"...\" key=\"...\" selectiveType=\"...\" rightsIssuerURL=\"...\" contentID=\"...\" transactionID=\"...\" >\n <OMATextHeader>textual header</OMATextHeader>\n </CryptTrack>\n</GPACDRM>\nCrypTrack OMA-specific Semantics¶\ntrackID : specifies the track ID to encrypt (mandatory, see Common Encryption ).\nkey : the AES-128 bit key to use. The key must be specified as an 32 bytes hex string, like 0x2b7e151628aed2a6abf7158809cf4f3c. This is a mandatory field, not specifying it or using an improper length will result in an error.\nrightsIssuerURL : the URL of the OMA DRM licence server. This is the URL to which an OMA client will request the keys using the ROAP protocol.\ncontentID : a string identifier for the content, passed during ROAP exchanges.\ntransactionID : a string identifier for the transaction, passed during ROAP exchanges.\nselectiveType : specifies how selective encryption is to be used, see Common Encryption for more details.\nSample GPAC OMA drm files¶\nThe following example shows how to encrypt a file with one track, without selective encryption, using OMA DRM.\n<?xml version=\"1.0\" encoding=\"UTF-8\" >\n<OMADRM>\n <OMATrack trackID=\"1\" key=\"0x2b7e151628aed2a6abf7158809cf4f3c\" selectiveType=\"None\" rightsIssuerURL=\"https://gpac.sourceforge.net/kms\" contentID=\"WatchMe1984\" transactionID=\"14fd12zd3q\" >\n <OMATextHeader>Preview=instant;http://other.content.com/sonaive</OMATextHeader>\n </OMATrack>\n</OMADRM>\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/OMA-DRM/"},{"date_scraped_timestamp":1720188012060,"host":"wiki.gpac.io","page_title":"OSX - GPAC wiki","text":"\n \n \n \n \n \n \nThis page describes how to setup a complete build environment for GPAC using Macports (update: Brew command-line below tested on Mac OS 10.11) in order to generate DMG installer images for both 10.5 and 10.6 versions of MacOS X.\nGetting GPAC source code¶\nUsing a git client, checkout GPAC from the repository:\ngit clone https://github.com/gpac/gpac.git\nConfiguring macports¶\n(Re-)Installing¶\nIf macports is installed, uninstall it:\n```\n sudo port -f uninstall installed\nsudo rm -rf /opt/local /Applications/DarwinPorts /Applications/MacPorts /Library/LaunchDaemons/org.macports. /Library/Receipts/DarwinPorts.pkg /Library/Receipts/MacPorts*.pkg Library/StartupItems/DarwinPortsStartup /Library/Tcl/darwinports1.0 /Library/Tcl/macports1.0 ~/.macports\n[Install macport](http://distfiles.macports.org/MacPorts/). You MUST install a version >=1.9.x\n### Custom flags for OSX 10.5 and 10.6 compatibility\nIf you plan to generate GPAC binaries compatible with 10.5, add the following lines at the end of `/opt/local/etc/macports/macports.conf`:\nmacosx_deployment_target 10.4\nsdkroot /Developer/SDKs/MacOSX10.5.sdk\nThis allows compiling a 10.5 binary with a 10.4 compatible dyld (dyn. loader). Furthermore, you will need to install the 10.5 development SDK. Such binaries should work on any 32 bits x86 versions of OS X.\n### Custom flags for OSX 10.6 32 bits only\nIf you are on a 10.6 platform with default targeting to x86\\_64 (i.e. x86\\_64 compatible cpu, whatever is your kernel configuration), you still may want to generate i386 binaries. Uncomment the `build_arch i386` line of `/opt/local/etc/macports/macports.conf`. Also comment the `universal_archs` feature as MacPorts may end up in an unpredictable state and you won't be able to compile GPAC with most features:\nbuild_arch i386\nCPU architectures to use for Universal Binaries (+universal variant)¶\nuniversal_archs i386¶\n## Installing GPAC extra libs\n### MacPorts packages\nTo install a package, type:\nsudo port install my_package_name\nThe command-line to install all the packages (compulsory and optional) can be found below in this chapter.\nThe zlib package is required to build GPAC.\nTo install all required packages, type:\nsudo port install pkgconfig freetype libpng jpeg spidermonkey185 libsdl-devel ffmpeg faad2 libmad xvid libogg libvorbis libtheora a52dec openjpeg\n**WARNING:** it is possible that some ports are not working for your system. In this case, you should get  the latest version of the source code of the package, and recompile it locally.\nIf you have configured macports to generate binaries compatible with 10.5, you must edit `/opt/local/bin/sdl-config` and replace the line below ` --cflags` with the following:\n `echo -I${prefix}/include/SDL -D_THREAD_SAFE -arch i386` \nIf you want to cross-compile from a 64 bits environment to a 32 bits target, you may encounter [this bug](https://trac.macports.org/ticket/28935) with the ffmpeg package (old versions), or [this bug](https://trac.macports.org/ticket/30137) with newer ffmpeg versions.\n### Brew packages\nAlternately to Ports, you may want to use Brew:\nbrew install jpeg libpng faad2 sdl pkgconfig freetype libvorbis theora openjpeg libmad xvid libogg spidermonkey ffmpeg\n### _Deprecated_ Setting up UPnP (Optional)\nInstall [Java developer package](http://developer.apple.com/java/download/) if your OSX version is > 10.6.3.\nInstall the Scons build system:\n `sudo port install scons` \nGet the platinum source code patched for gpac here: gpac.sourceforge.net/downloads/platinum\\_sdk\\_0.4.5.zip, and extract it.\nEdit file `Platinum/Platinum/Build/Targets/universal-apple-macosx/Config.scons` as follows:\nIf building for 10.6 64bits, keep the following flags:\nuniversal_flags = [('-arch', 'x86_64'), ('-isysroot', '/Developer/SDKs/MacOSX10.6.sdk'), '-mmacosx-version-min=10.6']\nOtherwise, set the flags to:\nuniversal_flags = [('-arch', 'i386'), ('-isysroot', '/Developer/SDKs/MacOSX10.5.sdk'), '-mmacosx-version-min=10.5']\nDepending on your GCC version, you may need to edit the file `Platinum/Platinum/Build/Boot.scons` and replace the line\nBoolVariable?('stop_on_warning', 'Stop the build on warnings', True),\nBoolVariable?('stop_on_warning', 'Stop the build on warnings', False),\nthen go to `Platinum/Platinum` and type:\nscons\ncp Build/Targets/universal-apple-macosx/Debug/*.a gpac/extra_lib/lib/gcc\n### Setting up OpenSVCDecoder (Optional)\nGet the latest GPAC [extra libs package](http://gpac.svn.sourceforge.net/viewvc/gpac/trunk/gpac_extra_libs/gpac_extra_libs.zip).\nUnzip and go to `./opensvcdecoder`. Build using\ncmake .\nthen copy the library to `/opt/local/lib` (or any place included in your link settings):\nsudo cp ./CommonFiles/src/libOpenSVCDec.dylib /opt/local/lib\nThe OpenSVCDecoder in gpac\\_extra\\_libs package is version 1.11 (latest) patched for MSVC and OS X compilation.\n## Building gpac\n### Configure gpac\nIf you plan to generate GPAC binaries compatible with 10.5, type the following configure:\n./configure --extra-cflags=\"-arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk\" --extra-ldflags=\"-mmacosx-version-min=10.4 -arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk\"\n```\nThis allows compiling a 10.5 binary with a 10.4 compatible dyld (dynamic loader).\nIf you don't plan to generate GPAC binaries compatible with 10.5, type the following configure:\n./configure \nBuild GPAC by typing\nmake \nIf you have setup UPnP, type\nmake -C modules/platinum \nIf you have setup OpenSVC decoder, type\nmake -C modules/opensvc_dec \nInstalling gpac¶\nIf you want to generate a DMG package for GPAC, regardless of 10.5 compatibility, type\n./mkdmg \nor\nmake dmg \nThis will produce a file called GPAC-$VERSION-r$REVISION.dmg\nIf you want to install GPAC directly on your system, type\nsudo make install \nNOTE: DO NOT RUN ./mkdmg AFTER make install, THE BINARIES WON'T BE USEABLE THROUGH THE DMG ARCHIVE. INSTEAD USE make dmg\nNow you deserve a coffee, enjoy it :)\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/old/Compiling-GPAC-for-MacOS-X/?q="},{"date_scraped_timestamp":1720187977532,"host":"wiki.gpac.io","page_title":"OSX - GPAC wiki","text":"\n \n \n \n \n \n \nThis page describes how to setup a complete build environment for GPAC using Macports (update: Brew command-line below tested on Mac OS 10.11) in order to generate DMG installer images for both 10.5 and 10.6 versions of MacOS X.\nGetting GPAC source code¶\nUsing a git client, checkout GPAC from the repository:\ngit clone https://github.com/gpac/gpac.git\nConfiguring macports¶\n(Re-)Installing¶\nIf macports is installed, uninstall it:\n```\n sudo port -f uninstall installed\nsudo rm -rf /opt/local /Applications/DarwinPorts /Applications/MacPorts /Library/LaunchDaemons/org.macports. /Library/Receipts/DarwinPorts.pkg /Library/Receipts/MacPorts*.pkg Library/StartupItems/DarwinPortsStartup /Library/Tcl/darwinports1.0 /Library/Tcl/macports1.0 ~/.macports\n[Install macport](http://distfiles.macports.org/MacPorts/). You MUST install a version >=1.9.x\n### Custom flags for OSX 10.5 and 10.6 compatibility\nIf you plan to generate GPAC binaries compatible with 10.5, add the following lines at the end of `/opt/local/etc/macports/macports.conf`:\nmacosx_deployment_target 10.4\nsdkroot /Developer/SDKs/MacOSX10.5.sdk\nThis allows compiling a 10.5 binary with a 10.4 compatible dyld (dyn. loader). Furthermore, you will need to install the 10.5 development SDK. Such binaries should work on any 32 bits x86 versions of OS X.\n### Custom flags for OSX 10.6 32 bits only\nIf you are on a 10.6 platform with default targeting to x86\\_64 (i.e. x86\\_64 compatible cpu, whatever is your kernel configuration), you still may want to generate i386 binaries. Uncomment the `build_arch i386` line of `/opt/local/etc/macports/macports.conf`. Also comment the `universal_archs` feature as MacPorts may end up in an unpredictable state and you won't be able to compile GPAC with most features:\nbuild_arch i386\nCPU architectures to use for Universal Binaries (+universal variant)¶\nuniversal_archs i386¶\n## Installing GPAC extra libs\n### MacPorts packages\nTo install a package, type:\nsudo port install my_package_name\nThe command-line to install all the packages (compulsory and optional) can be found below in this chapter.\nThe zlib package is required to build GPAC.\nTo install all required packages, type:\nsudo port install pkgconfig freetype libpng jpeg spidermonkey185 libsdl-devel ffmpeg faad2 libmad xvid libogg libvorbis libtheora a52dec openjpeg\n**WARNING:** it is possible that some ports are not working for your system. In this case, you should get  the latest version of the source code of the package, and recompile it locally.\nIf you have configured macports to generate binaries compatible with 10.5, you must edit `/opt/local/bin/sdl-config` and replace the line below ` --cflags` with the following:\n `echo -I${prefix}/include/SDL -D_THREAD_SAFE -arch i386` \nIf you want to cross-compile from a 64 bits environment to a 32 bits target, you may encounter [this bug](https://trac.macports.org/ticket/28935) with the ffmpeg package (old versions), or [this bug](https://trac.macports.org/ticket/30137) with newer ffmpeg versions.\n### Brew packages\nAlternately to Ports, you may want to use Brew:\nbrew install jpeg libpng faad2 sdl pkgconfig freetype libvorbis theora openjpeg libmad xvid libogg spidermonkey ffmpeg\n### _Deprecated_ Setting up UPnP (Optional)\nInstall [Java developer package](http://developer.apple.com/java/download/) if your OSX version is > 10.6.3.\nInstall the Scons build system:\n `sudo port install scons` \nGet the platinum source code patched for gpac here: gpac.sourceforge.net/downloads/platinum\\_sdk\\_0.4.5.zip, and extract it.\nEdit file `Platinum/Platinum/Build/Targets/universal-apple-macosx/Config.scons` as follows:\nIf building for 10.6 64bits, keep the following flags:\nuniversal_flags = [('-arch', 'x86_64'), ('-isysroot', '/Developer/SDKs/MacOSX10.6.sdk'), '-mmacosx-version-min=10.6']\nOtherwise, set the flags to:\nuniversal_flags = [('-arch', 'i386'), ('-isysroot', '/Developer/SDKs/MacOSX10.5.sdk'), '-mmacosx-version-min=10.5']\nDepending on your GCC version, you may need to edit the file `Platinum/Platinum/Build/Boot.scons` and replace the line\nBoolVariable?('stop_on_warning', 'Stop the build on warnings', True),\nBoolVariable?('stop_on_warning', 'Stop the build on warnings', False),\nthen go to `Platinum/Platinum` and type:\nscons\ncp Build/Targets/universal-apple-macosx/Debug/*.a gpac/extra_lib/lib/gcc\n### Setting up OpenSVCDecoder (Optional)\nGet the latest GPAC [extra libs package](http://gpac.svn.sourceforge.net/viewvc/gpac/trunk/gpac_extra_libs/gpac_extra_libs.zip).\nUnzip and go to `./opensvcdecoder`. Build using\ncmake .\nthen copy the library to `/opt/local/lib` (or any place included in your link settings):\nsudo cp ./CommonFiles/src/libOpenSVCDec.dylib /opt/local/lib\nThe OpenSVCDecoder in gpac\\_extra\\_libs package is version 1.11 (latest) patched for MSVC and OS X compilation.\n## Building gpac\n### Configure gpac\nIf you plan to generate GPAC binaries compatible with 10.5, type the following configure:\n./configure --extra-cflags=\"-arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk\" --extra-ldflags=\"-mmacosx-version-min=10.4 -arch i386 -isysroot /Developer/SDKs/MacOSX10.5.sdk\"\n```\nThis allows compiling a 10.5 binary with a 10.4 compatible dyld (dynamic loader).\nIf you don't plan to generate GPAC binaries compatible with 10.5, type the following configure:\n./configure \nBuild GPAC by typing\nmake \nIf you have setup UPnP, type\nmake -C modules/platinum \nIf you have setup OpenSVC decoder, type\nmake -C modules/opensvc_dec \nInstalling gpac¶\nIf you want to generate a DMG package for GPAC, regardless of 10.5 compatibility, type\n./mkdmg \nor\nmake dmg \nThis will produce a file called GPAC-$VERSION-r$REVISION.dmg\nIf you want to install GPAC directly on your system, type\nsudo make install \nNOTE: DO NOT RUN ./mkdmg AFTER make install, THE BINARIES WON'T BE USEABLE THROUGH THE DMG ARCHIVE. INSTEAD USE make dmg\nNow you deserve a coffee, enjoy it :)\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/old/Compiling-GPAC-for-MacOS-X/"},{"date_scraped_timestamp":1720188058514,"host":"wiki.gpac.io","page_title":"Other Features - GPAC wiki","text":"\n \n \n \n \n \n \nHinting Options¶\nIsoMedia hinting consists in creating special tracks in the file that contain transport protocol specific information and optionally multiplexing information. These tracks are then used by the server to create the actual packets being sent over the network, in other words they provide the server with hints on how to build packets, hence their names hint tracks.\nMP4Box supports creation of hint tracks for RTSP servers supporting these such as QuickTime Streaming Server, DarwinStreaming Server or 3GPP-compliant RTSP servers.\nNote: GPAC streaming tools rtp output and rtsp server do not use hint tracks, they use on-the-fly packetization from any media sources, not just MP4 \nOptions:\n-hint: hint the file for RTP/RTSP\n-mtu (int, default: 1450): specify RTP MTU (max size) in bytes (this includes 12 bytes RTP header)\n-copy: copy media data to hint track rather than reference (speeds up server but takes much more space)\n-multi [maxptime]: enable frame concatenation in RTP packets if possible (with max duration 100 ms or maxptime ms if given)\n-rate (int, default: 90000): specify rtp rate in Hz when no default for payload\n-mpeg4: force MPEG-4 generic payload whenever possible\n-latm: force MPG4-LATM transport for AAC streams\n-static: enable static RTP payload IDs whenever possible (by default, dynamic payloads are always used)\n-add-sdp (string): add given SDP string to movie (string) or track (tkID:string), tkID being the track ID or the hint track ID\n-no-offset: signal no random offset for sequence number and timestamp (support will depend on server)\n-unhint: remove all hinting information from file\n-group-single: put all tracks in a single hint group\n-ocr: force all MPEG-4 streams to be synchronized (MPEG-4 Systems only)\n-rap: signal random access points in RTP packets (MPEG-4 Systems)\n-ts: signal AU Time Stamps in RTP packets (MPEG-4 Systems)\n-size: signal AU size in RTP packets (MPEG-4 Systems)\n-idx: signal AU sequence numbers in RTP packets (MPEG-4 Systems)\n-iod: prevent systems tracks embedding in IOD (MPEG-4 Systems), not compatible with -isma \nTagging support¶\nTags are specified as a colon-separated list tag_name=tag_value[:tag2=val2]\nSetting a tag with no value or value NULL removes the tag.\nSpecial tag value clear (or reset) removes all tags.\nUnsupported tags can be added using their four character code as a tag name, and string value will be assumed.\nIf the tag name length is 3, the prefix 0xA9 is used to create the four character code. \nTags can also be loaded from a text file using -itags filename. The file must be in UTF8 with: \nlines starting with tag_name=value specify the start of a tag \nother lines specify the remainder of the last declared tag \nIf tag name starts with WM/, the tag is added to Xtra box (WMA tag, string only). \nQT metadata key¶\nThe tag is added as a QT metadata key if: \ntag_name starts with QT/ \nor tag_name is not recognized and longer than 4 characters \nThe tag_name can optionally be prefixed with HDLR@, indicating the tag namespace 4CC, the default namespace being mdta.\nThe tag_value can be prefixed with: \nS: force string encoding (must be placed first) instead of parsing the tag value \nb: use 8-bit encoding for signed or unsigned int \ns: use 16-bit encoding for signed or unsigned int \nl: use 32-bit encoding for signed or unsigned int \nL: use 64-bit encoding for signed or unsigned int \nf: force float encoding for numbers \nNumbers are converted by default and stored in variable-size mode.\nTo force a positive integer to use signed storage, add + in front of the number.\nExample\n-tags io.gpac.some_tag=s+32\nThis will force storing value 32 in signed 16 bit format.\nThe tag_value can also be formatted as: \nXxY@WxH: a rectangle type \nXxY: a point type \nW@H: a size type \nA,B,C,D,E,F,G,H,I: a 3x3 matrix \nFNAME: data is loaded from FNAME, type set to jpeg or png if needed \nSupported tag names (name, value, type, aliases)¶\ntitle (A9nam) string (alias name)\nartist (A9ART) string\nalbum_artist (aART) string (alias albumArtist)\nalbum (A9alb) string\ngroup (A9grp) string (alias grouping)\ncomposer (A9com) string\nwriter (A9wrt) string\nconductor (A9con) string\ncomment (A9cmt) string (alias comments)\ngenre (gnre) string (ID3 genre tag)\ncreated (A9day) string (alias releaseDate)\ntrack (A9trk) string\ntracknum (trkn) fraction (syntax: A/B or A, B will be 0)\ndisk (disk) fraction (syntax: A/B or A, B will be 0)\ntempo (tmpo) integer\ncompilation (cpil) bool (yes or no)\nshow (tvsh) string (alias tvShow)\nepisode_id (tven) string (alias tvEpisodeID)\nseason (tvsn) integer (alias tvSeason)\nepisode (tves) integer (alias tvEPisode)\nnetwork (tvnn) string (alias tvNetwork)\nsdesc (desc) string (alias description)\nldesc (ldes) string (alias longDescription)\nlyrics (A9lyr) string\nsort_name (sonm) string (alias sortName)\nsort_artist (soar) string (alias sortArtist)\nsort_album_artist (soaa) string (alias sortAlbumArtist)\nsort_album (soal) string (alias sortAlbum)\nsort_composer (soco) string (alias sortComposer)\nsort_show (sosn) string (alias sortShow)\ncover (covr) file path (alias artwork)\ncopyright (cprt) string\ntool (A9too) string (alias encodingTool)\nencoder (A9enc) string (alias encodedBy)\npdate (purd) string (alias purchaseDate)\npodcast (pcst) bool (yes or no)\nurl (purl) string (alias podcastURL)\nkeywords (kyyw) string\ncategory (catg) string\nhdvideo (hdvd) integer\nmedia (stik) integer (alias mediaType)\nrating (rtng) integer (alias contentRating)\ngapless (pgap) bool (yes or no)\nart_director (A9ard) string\narranger (A9arg) string\nlyricist (A9aut) string\nacknowledgement (A9cak) string\nsong_description (A9des) string\ndirector (A9dir) string\nequalizer (A9equ) string\nliner (A9lnt) string\nrecord_company (A9mak) string\noriginal_artist (A9ope) string\nphono_rights (A9phg) string\nproducer (A9prd) string\nperformer (A9prf) string\npublisher (A9pub) string\nsound_engineer (A9sne) string\nsoloist (A9sol) string\ncredits (A9src) string\nthanks (A9thx) string\nonline_info (A9url) string\nexec_producer (A9xpd) string\ngenre (A9gen) string (ID3 genre tag)\nlocation (A9xyz) string \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-other-opts/?q="},{"date_scraped_timestamp":1720188042707,"host":"wiki.gpac.io","page_title":"Other Features - GPAC wiki","text":"\n \n \n \n \n \n \nHinting Options¶\nIsoMedia hinting consists in creating special tracks in the file that contain transport protocol specific information and optionally multiplexing information. These tracks are then used by the server to create the actual packets being sent over the network, in other words they provide the server with hints on how to build packets, hence their names hint tracks.\nMP4Box supports creation of hint tracks for RTSP servers supporting these such as QuickTime Streaming Server, DarwinStreaming Server or 3GPP-compliant RTSP servers.\nNote: GPAC streaming tools rtp output and rtsp server do not use hint tracks, they use on-the-fly packetization from any media sources, not just MP4 \nOptions:\n-hint: hint the file for RTP/RTSP\n-mtu (int, default: 1450): specify RTP MTU (max size) in bytes (this includes 12 bytes RTP header)\n-copy: copy media data to hint track rather than reference (speeds up server but takes much more space)\n-multi [maxptime]: enable frame concatenation in RTP packets if possible (with max duration 100 ms or maxptime ms if given)\n-rate (int, default: 90000): specify rtp rate in Hz when no default for payload\n-mpeg4: force MPEG-4 generic payload whenever possible\n-latm: force MPG4-LATM transport for AAC streams\n-static: enable static RTP payload IDs whenever possible (by default, dynamic payloads are always used)\n-add-sdp (string): add given SDP string to movie (string) or track (tkID:string), tkID being the track ID or the hint track ID\n-no-offset: signal no random offset for sequence number and timestamp (support will depend on server)\n-unhint: remove all hinting information from file\n-group-single: put all tracks in a single hint group\n-ocr: force all MPEG-4 streams to be synchronized (MPEG-4 Systems only)\n-rap: signal random access points in RTP packets (MPEG-4 Systems)\n-ts: signal AU Time Stamps in RTP packets (MPEG-4 Systems)\n-size: signal AU size in RTP packets (MPEG-4 Systems)\n-idx: signal AU sequence numbers in RTP packets (MPEG-4 Systems)\n-iod: prevent systems tracks embedding in IOD (MPEG-4 Systems), not compatible with -isma \nTagging support¶\nTags are specified as a colon-separated list tag_name=tag_value[:tag2=val2]\nSetting a tag with no value or value NULL removes the tag.\nSpecial tag value clear (or reset) removes all tags.\nUnsupported tags can be added using their four character code as a tag name, and string value will be assumed.\nIf the tag name length is 3, the prefix 0xA9 is used to create the four character code. \nTags can also be loaded from a text file using -itags filename. The file must be in UTF8 with: \nlines starting with tag_name=value specify the start of a tag \nother lines specify the remainder of the last declared tag \nIf tag name starts with WM/, the tag is added to Xtra box (WMA tag, string only). \nQT metadata key¶\nThe tag is added as a QT metadata key if: \ntag_name starts with QT/ \nor tag_name is not recognized and longer than 4 characters \nThe tag_name can optionally be prefixed with HDLR@, indicating the tag namespace 4CC, the default namespace being mdta.\nThe tag_value can be prefixed with: \nS: force string encoding (must be placed first) instead of parsing the tag value \nb: use 8-bit encoding for signed or unsigned int \ns: use 16-bit encoding for signed or unsigned int \nl: use 32-bit encoding for signed or unsigned int \nL: use 64-bit encoding for signed or unsigned int \nf: force float encoding for numbers \nNumbers are converted by default and stored in variable-size mode.\nTo force a positive integer to use signed storage, add + in front of the number.\nExample\n-tags io.gpac.some_tag=s+32\nThis will force storing value 32 in signed 16 bit format.\nThe tag_value can also be formatted as: \nXxY@WxH: a rectangle type \nXxY: a point type \nW@H: a size type \nA,B,C,D,E,F,G,H,I: a 3x3 matrix \nFNAME: data is loaded from FNAME, type set to jpeg or png if needed \nSupported tag names (name, value, type, aliases)¶\ntitle (A9nam) string (alias name)\nartist (A9ART) string\nalbum_artist (aART) string (alias albumArtist)\nalbum (A9alb) string\ngroup (A9grp) string (alias grouping)\ncomposer (A9com) string\nwriter (A9wrt) string\nconductor (A9con) string\ncomment (A9cmt) string (alias comments)\ngenre (gnre) string (ID3 genre tag)\ncreated (A9day) string (alias releaseDate)\ntrack (A9trk) string\ntracknum (trkn) fraction (syntax: A/B or A, B will be 0)\ndisk (disk) fraction (syntax: A/B or A, B will be 0)\ntempo (tmpo) integer\ncompilation (cpil) bool (yes or no)\nshow (tvsh) string (alias tvShow)\nepisode_id (tven) string (alias tvEpisodeID)\nseason (tvsn) integer (alias tvSeason)\nepisode (tves) integer (alias tvEPisode)\nnetwork (tvnn) string (alias tvNetwork)\nsdesc (desc) string (alias description)\nldesc (ldes) string (alias longDescription)\nlyrics (A9lyr) string\nsort_name (sonm) string (alias sortName)\nsort_artist (soar) string (alias sortArtist)\nsort_album_artist (soaa) string (alias sortAlbumArtist)\nsort_album (soal) string (alias sortAlbum)\nsort_composer (soco) string (alias sortComposer)\nsort_show (sosn) string (alias sortShow)\ncover (covr) file path (alias artwork)\ncopyright (cprt) string\ntool (A9too) string (alias encodingTool)\nencoder (A9enc) string (alias encodedBy)\npdate (purd) string (alias purchaseDate)\npodcast (pcst) bool (yes or no)\nurl (purl) string (alias podcastURL)\nkeywords (kyyw) string\ncategory (catg) string\nhdvideo (hdvd) integer\nmedia (stik) integer (alias mediaType)\nrating (rtng) integer (alias contentRating)\ngapless (pgap) bool (yes or no)\nart_director (A9ard) string\narranger (A9arg) string\nlyricist (A9aut) string\nacknowledgement (A9cak) string\nsong_description (A9des) string\ndirector (A9dir) string\nequalizer (A9equ) string\nliner (A9lnt) string\nrecord_company (A9mak) string\noriginal_artist (A9ope) string\nphono_rights (A9phg) string\nproducer (A9prd) string\nperformer (A9prf) string\npublisher (A9pub) string\nsound_engineer (A9sne) string\nsoloist (A9sol) string\ncredits (A9src) string\nthanks (A9thx) string\nonline_info (A9url) string\nexec_producer (A9xpd) string\ngenre (A9gen) string (ID3 genre tag)\nlocation (A9xyz) string \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-other-opts/"},{"date_scraped_timestamp":1720187974835,"host":"wiki.gpac.io","page_title":"Overview - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to use the JavaScript Filter to write JavaScript-based custom filters in GPAC.\nThe JS scripts in the gpac test suite are also a good source of examples.\nThe JS filter provides JS bindings to the GPAC filter architecture. JavaScript support in GPAC is powered by QuickJS. Check the documentation of the JS APIs for more details. \nPrinciples¶\nIn order to load, a JS filter requires a source JS, specified in the filter js option. A short-cut syntax is to directly specify the js script. In other words, the following syntaxes are equivalent:\ngpac -h jsf:js=script.js\ngpac -h script.js\nA JS filter behaves like any other filters in GPAC, and can be a source, sink, filter or server.\nYour script will be called with the JS API already loaded with a global object called filter implementing the JSFilter API.\nWe will assume in the rest of this article that the script file is called script.js \nJS filter life cycle can be described as follows:\ncreation of JS context\nloading of the JS file (load or setup phase)\nfilter initializing, executed in the callback function filter.initialize\nif the filter accepts inputs, configuration of input PIDs in the callback function filter.configure_pid\nfilter process task, executed in the callback function filter.process. This function should not block and typically consumes/produces one packet at a time.\nfilter termination, executed in the callback function filter.finalize\nfinal destruction of JS context\nWhile a filter is active, it can get the following notifications:\nif the filter accepts inputs, (re)configuration of input PIDs in the callback function filter.configure_pid\nif the filter accepts inputs, removal of input PIDs through the callback function filter.remove_pid\nevents sent by the pipeline through the callback function filter.process_event\nupdate of arguments values through the callback function filter.update_arg\nCallback functions defined for filter object do not use exception error handling, they shall either return nothing (success) or a GF_Err code. \nIt is possible to use several JS filters in a given chain, but each JS filter will create its own JavaScript context, and JS objects cannot be shared between JS filters.\nIf you need to pass JS data across filters, you will have to serialize to JSON your data and either:\nsend it as PID information on a PID of your choice\nsend it as JSON-only packets through a dedicated JS PID\nsend it as associated property on existing packets\nDeclaring a filter (optional)¶\nThe first thing to do when creating a filter is to setup a few things about your filter. \nGive a name to your filter (optional): \nfilter.set_name(\"MyFilter\");\nThis name will be used when logging messages and inspecting the filter graph. If not set, the filter name will be the script name.\nGive a description to your filter (optional): \n filter.set_desc(\"A demonstration JS filter\");\n This description should provide a quick hint as to what the purpose of the filter is. It will be shown by the command gpac -h script.js.\nYou can also setup version and author information (optional):\n filter.set_version(\"0.1beta\");\nfilter.set_author(\"GPAC team\");\nYou should finally set some help for your filter (optional):\nfilter.set_help(\"This filter provides a very simple javascript filter\");\nThis will help other users understand what your filter does and how to use it. It will be shown by the command gpac -h script.js.\nYou can specify arguments to your filter (optional):\nfilter.set_arg({ name: \"raw\", desc: \"if set, accept non-demultiplexed input PIDs\", type: GF_PROP_BOOL, def: \"false\"} );\n...\nfilter.set_arg({name: \"str\", desc: \"string to send\", type: GF_PROP_STRING, def: \"GPAC JS Filter Packet\"} );\nArguments simplify script configuration and usage, and will be shown by the command gpac -h script.js.\nDefined arguments will be parsed from command line. Each defined argument results in a JS property in the filter object with the given value. \nFor the above command:\ngpac script.js will result in the filter object having raw: false and str: \"GPAC JS Filter Packet\" properties defined in the filter object.\ngpac script.js:raw=true will result in the filter object having raw: true and str: \"GPAC JS Filter Packet\" properties defined in the filter object.\nFilter arguments are parsed once the script is loaded. If you need to test the arguments during the initialization phase, do this in the filter.initialize callback: \nfilter.initialize = function() {\n if (this.raw) {\n //do something\n } else {\n //do something else\n }\n }\nSetting up filter capabilities¶\nEach filter in GPAC uses a set of input and/or output capabilities in order to solve connections between filters, and JS filters are no exception. \nCapabilities shall be set during load or initialization phase, it is not possible to modify capabilities after initialization.\nThe following will add an output capability indicating the property StreamType with a value Visual is requested on input PIDs.\nfilter.set_cap({id: \"StreamType\", value: \"Visual\"});\nThe following will add an input capability indicating the property StreamType with a value File is excluded on input PIDs, forcing a demultiplexing of the source.\nfilter.set_cap({id: \"StreamType\", value: \"File\", excluded: true} );\nThe following will add an output capability indicating the property StreamType with a value Visual is always present on output PIDs.\nfilter.set_cap({id: \"StreamType\", value: \"Visual\", output: true});\nThe following will add an input and output capability indicating the property CodecID with a value raw is required on input PIDs and present on output PIDs (typically, raw audio or video processing filter).\nfilter.set_cap({id: \"CodecID\", value: \"raw\", inout: true} );\nDiscussion\nGPAC uses a concept of capabilities bundles for complex filters, allowing to describe characteristics of different classes of input or output PIDs. This is also possible using JS filters, by adding an empty cap to your filter:\nfilter.set_cap({id: \"StreamType\", value: \"Visual\", inout: true});\nfilter.set_cap({id: \"CodecID\", value: \"raw\", inout: true} );\n//video specific PID characteristics for input and output\n...\n//start a new capability bundle\nfilter.set_cap();\nfilter.set_cap({id: \"StreamType\", value: \"Audio\", inout: true});\nfilter.set_cap({id: \"CodecID\", value: \"raw\", inout: true} );\n//audio specific PID characteristics for input and output\n...\nYou can check your JS filter sources and sinks links by using gpac -h links script.js.\nAccepting input connections¶\nOnce your capabilities are setup, you can get notifications of new inputs through the filter.configure_pid callback. This allows you to keep track of PID re-configurations, for later processing.\n...\nfilter.pids=[];\n...\nfilter.configure_pid=function(pid)\n{\n if (this.pids.indexOf(pid)<0) {\n //this is a new PID, do something\n }\n //check input properties\n let st = pid.get_props('StreamType');\n if (st=='Visual') {\n let width = pid.get_props('Width');\n } \n else if (st=='Audio') {\n let sr = pid.get_props('SampleRate');\n } else {\n //return GF_Err is allowed\n return GF_NOT_SUPPORTED; \n }\n //no return value means no error\n}\nThe above code allows monitoring PID configuration and performs simple PID property check.\nDiscussion \nFilters properties are mapped to their native type, e.g. unsigned int, boolean, string, float and double, or to objects for vector, arrays, and fractions. There are however a few exceptions here:\nthe StreamType property is converted to a string (see gpac -h props and Properties)\nthe PixelFormat property is converted to a string (see gpac -h props and Properties)\nthe AudioFormat property is converted to a string (see gpac -h props and Properties)\nthe CodecID property is converted to a string corresponding to the first short name of the codec (see gpac -h codecs and Properties)\nPacket Query¶\nOnce you have an input PID in place in your filter, you can start fetching packets from this PID in the filter.process callback. The packet access API follows the same principles as non-JS filters:\npackets are always delivered in processing order\nonly the first packet of an input PID packet queue can be fetched, and must be explicitly removed\npackets can be reference counted for later reuse\npackets properties can be reference counted for later reuse, while still discarding the associated packet data, thereby unblocking the filter chain\nIf you need access to the second packet in the PID queue, you need to reference the first packet and drop the input PID packet queue.\nfilter.process = function()\n{\n let pck_in = filter.in_pid.get_packet();\n if (pck_in==null) return;\n //any further call to filter.in_pid.get_packet() will return the same pck_in object, until drop_packet is called\n print('Packet DTS is ' + pck_in.dts);\n let data = pck_in.data;\n //inspect data\n //drop the packet - at this stage, if the packet has not been referenced, its associated data is potentially destroyed and shall no longer be used\n filter.in_pid.drop_packet(); \n}\nThe above code will print the DTS of each packet in the input PID.\nA simple packet inspector¶\nThe following shows a simple packet inspector script\n//prepare our input PID array\nfilter.pids=[];\n//accept any stream types except files, forcing a demultiplexer\nfilter.set_cap({id: \"StreamType\", value: \"File\", excluded: true} );\n//accept only framed inputs (no ADTS or TS packets or the like)\nfilter.set_cap({id: \"Unframed\", value: \"true\", excluded: true} );\n//indicate we accept any number of input PIDs.\nfilter.max_pids=-1;\n//configure callback\nfilter.configure_pid=function(pid)\n{\n if (this.pids.indexOf(pid)<0) {\n print('New pid ' + pid.get_prop('ID'));\n //send a PLAY event on the PID (we are a sink filter)\n let evt = new FilterEvent(GF_FEVT_PLAY);\n evt.start_range = 0.0;\n pid.send_event(evt);\n //remember our PID\n this.pids.push(pid);\n //only fetch full coded frames (Access Units)\n pid.framing_mode=true;\n pid.nb_pck = 0;\n } else {\n print('Reconfigure pid ' + pid.get_prop('ID')); \n }\n //print input properties\n print('Properties:');\n let i=0;\n while (1) {\n let prop = pid.enum_properties(i);\n if (!prop) break;\n i++;\n //print in GPAC JS can use a log level as first argument\n print(GF_LOG_INFO, \"Prop \" + prop.name + \" (type \" + prop.type + \" ): \" + JSON.stringify(prop.value) );\n }\n //no return value means no error\n}\n//process callback\nfilter.process=function()\n{\n this.pids.forEach(function(pid) {\n //dump any packet ready on the PID\n while (1) {\n //get packet\n let pck = pid.get_packet();\n if (!pck) {\n if (pid.eos) print(GF_LOG_INFO, \"pid is in end of stream\");\n break;\n }\n //print packet info\n print(GF_LOG_INFO, \"PID\" + pid.name + \" PCK\" + pid.nb_pck + \" DTS \" + pck.dts + \" CTS \" + pck.cts + \" SAP \" + pck.sap + \" size \" + pck.size);\n //dump all packet properties (usually none)\n let i=0;\n while (1) {\n let prop = pck.enum_properties(i);\n if (!prop) break;\n i++;\n print(GF_LOG_INFO, \"Prop \" + prop.name + \" (type \" + prop.type + \" ): \" + prop.value);\n }\n //access packet data, and dump first 4 bytes\n let data = pck.data;\n if (data) {\n let view = new Uint8Array(data);\n print('data buffer, size ' + view.length + ' first 4 bytes ' + view[0].toString(16) + '' + view[1].toString(16) + '' + view[2].toString(16) + '' + view[3].toString(16));\n }\n //drop packet\n pid.drop_packet();\n }\n }\n );\n}\nTo run the inspector script on a source, simply execute:\ngpac -i source_file script.js\nDeclaring output PIDs¶\nIn GPAC, output PIDs can be declared pretty much at any time in the life cycle of the filter, except upon destruction. You must however declare your output PIDs capabilities:\nfilter.set_cap({id: \"StreamType\", value: \"Video\", output: true});\nfilter.set_cap({id: \"CodecID\", value: \"raw\", output: true});\nThe above script will declare an output capability bundle of RAW (uncompressed) video. Note that we use here 'Video' instead of 'Visual', the two names are equivalent.\nAn output PID can only be created for the current filter. This will setup a new output PID dispatching raw video in rgb format:\nfilter.opid = filter.new_pid();\nfilter.opid.set_prop(\"StreamType\", \"Video\");\nfilter.opid.set_prop(\"CodecID\", \"raw\");\nfilter.opid.set_prop(\"PixelFormat\", \"rgb\");\nYou can also assign user-defined properties to the PID as follows:\nfilter.opid.set_prop(\"MyTestProperty\", \"My Current State\", true);\nPID properties are automatically attached to packets sent on the PID. Any modification to the PID properties will mark the next packet sent as a reconfiguration point, which will trigger a configure_pid on consuming filters. If you wish to attach information on a PID without triggering reconfiguration (for example, PID state not related to packet processing, such as current download rate), you can use PID information. Any modification to the PID information set will mark the next packet sent as an info update point, which will trigger a call to process_event on consuming filters.\nfilter.opid.set_info(\"MyTestInfo\", \"My Current Info\", true);\nWhen a PID is used to forward (potentially after processing) packets from an input PID, it is recommended to copy properties of the source PID to the destination PID as follows:\nfilter.configure_pid = function(pid)\n{\n if (typeof pid.opid == 'undefined') {\n pid.opid = filter.new_pid();\n }\n pid.opid.copy_props(pid);\n //then modify opid properties as desired\n}\nDiscussion\nThe property copy is recommended especially when SourceIDs, URL templating or filter chain templating are used, as explained in general filter concepts. In these cases, the graph resolution or a destination filter may use a property of the input PID, which would be lost if not copied.\nAs a general rule, you should always copy source PID properties to output PID, and then rewrite or remove any needed properties. Removing a property is done by using a nullvalue:\npid.opid.set_props(pid, \"MIMEType\", null);\nCreating new packets¶\nGPAC uses several types of packets:\npackets holding data allocated by the framework. \nExamples:\n//create blank packet of 20 bytes\ndst_pck = outpid.new_packet(20);\n//create packet using the given string as a payload\ndst_pck = outpid.new_packet(\"MyString\");\n//setup array buffer\nab = new ArrayBuffer();\n//create packet containing a copy of the array buffer data\ndst_pck = outpid.new_packet(ab);\npackets holding references to data allocated by a filter. These are usually tracked by the source filter using a callback function to detect packet destruction. An example of this feature is dispatching a video frame from a grabber, in order to avoid memory copy.\nExamples:\n//setup array buffer\nab = new ArrayBuffer();\n//create packet containing a reference to the array buffer. The reference is released when the packet is destroyed\ndst_pck = outpid.new_packet(ab, true);\n//same as above, but also use a callback function to get notification when packet is consumed\ndst_pck = outpid.new_packet(ab, true, () => {\n //do something upon destruction of packet\n });\npackets holding references to other packets, typically used to forward all or part of an input packet. An example of this feature is cropping an input video packet into one or more smaller frames without copying the packet data, by adjusting strides and data pointers.\nExamples:\n//get source packet\nsrc_pck = in_pid.get_packet();\n//create packet containing a reference to the array buffer. The reference is released when the packet is destroyed\ndst_pck = outpid.new_packet(src_pck, true);\n//same as above, but also use a callback function to get notification when packet is consumed\ndst_pck = outpid.new_packet(src_pck, true, () => {\n //do something upon destruction of packet\n });\npackets cloning source packets - this special mode is used to perform in-place processing of packet whenever possible; if a source packet is only used by the calling filter and allows in-place processing, the input data is transferred to the output packet with no copy. \nExamples:\n//get source packet\nsrc_pck = in_pid.get_packet();\n//create packet containing a clone of the input packet. If the input data cannot be cloned, a copy of the data is done, otherwise an error is thrown\ndst_pck = outpid.new_packet(src_pck);\n//create packet containing an explicit copy of the input packet. Even if the input data could have been cloned, a copy of the data is done if possible, otherwise an error is thrown\ndst_pck = outpid.new_packet(src_pck, false, true);\npackets holding references to data accessors available in the filter, currently only use to access internal color planes of a filter or underlying OpenGL textures. \nThis type of packets cannot be created from JS, but they can be used for other operations (packet forwarding, texture setup).\nTheir data can be accessed from JS by creating a clone packet (regular filter for in-place data editing) or a detached clone (sink filters) \n//prepare at global scope or other a cached packet to reduce memory allocations while cloning packets\nfilter.cached_pck = null;\n//get source packet\nsrc_pck = in_pid.get_packet();\nlet data = null;\nif (src_pck.frame_ifce) {\n //we want to edit and send the result, clone the packet\n if (output_pid) {\n dst_pck = outpid.new_packet(src_pck); \n data = dst_pck.data;\n }\n //we want to read only (either sink or we do not send the modified input as the output)\n //we must clone the packet\n else {\n filter.cached_pck = src_pck.clone(filter.cached_pck);\n data = filter.cached_pck;\n }\n}\nOnce a packet is created, you can assign packet info as well as built-in and user properties as usual:\ndst_pck.dts = 100;\ndst_pck.cts = 101;\ndst_pck.sap=GF_FILTER_SAP1;\ndst_pck.duration = 1;\ndst_pck.set_prop('SenderNTP', get_cur_ntp() );\ndst_pck.set_prop('MyUserProp', 'My User Data', true);\nIf needed, you can copy packet information from a source packet. This will copy all built-in and user properties and all packet info \ndst_pck.copy_props(src_pck);\nNote that this property copy is done by default when constructing an output packet from a source packet.\nWhen you're ready, it's time to send your packet:\nOnce send, the packet cannot be resent: it is in a detached state and has no longer any underlying native packet. As a general rule, you should consider a packet send as no longer accessible. \nA simple packet generator¶\nThe following is a simple packet generator creating 100 packets of subtitles\n//we produce one text stream, using codec \"simple text\" ('subs')\nfilter.set_cap({id: \"StreamType\", value: \"Text\", output: true});\nfilter.set_cap({id: \"CodecID\", value: \"subs\", output: true});\nfilter.nb_pck = 0;\nfilter.initialize = function() {\n this.opid = this.new_pid();\n this.opid.set_prop(\"StreamType\", \"Text\");\n this.opid.set_prop(\"CodecID\", \"subs\");\n this.opid.set_prop(\"Timescale\", \"1000\");\n //you can set a decoder config for text streams, usually a file header - not that this is optional\n this.opid.set_prop(\"DecoderConfig\", \"My Super Config\");\n}\nfilter.process = function()\n{\n if (!this.opid)\n return GF_EOS;\n if (this.nb_pck>=100) {\n this.opid.eos = true;\n return GF_EOS;\n }\n this.nb_pck++;\n pck = this.opid.new_packet(\"Packet number \" + this.nb_pck);\n pck.cts = 1000*this.nb_pck;\n pck.dts = 1000*this.nb_pck;\n pck.dur = 1000;\n pck.sap = GF_FILTER_SAP_1;\n pck.send();\n}\nLoading filters from a JS filter¶\nJS filters can load other filters to create complex processing chain. This can be done in the following ways:\nload a source filter: this allows loading a demultiplexing chain loading data from a given URL\nlet src_f = filter.add_source(\"myfile.ts\");\nlet src_f = filter.add_source(\"http://host/source.mp4\");\nAny URL supported by GPAC for source filter loading can be used.\nload a destination (sink) filter: this allows loading a multiplexing chain targeting a given URL\nlet src_f = filter.add_destination(\"myfile.ts\");\nlet src_f = filter.add_destination(\"pipe://mymux.gsf\");\nAny URL supported by GPAC for destination/sink filter loading can be used.\nload a generic filter: this allows loading any filter supported by GPAC\nlet vout_f = filter.add_filter(\"vout\");\nThis will load the video output filter with no specific arguments. \nFor each of these methods, the filter name or URL used can specify filter options, as usual within GPAC filter chains. For example:\nlet vout_f = filter.add_filter(\"vout:vsync=no\");\nThis will load the video output filter with vsync disabled. \nA filter loaded by a JS filter does not expose the JSFilter API, but the FilterInstance API. In other words, you cannot manipulate the loaded filter by adding or removing PIDs or processing/sending packets in place of this filter. If you build a complex filter chain and need to indicate which of your filter outputs may connect to which loaded destination, you must set the FIDs and SourceID options of your filters, or use the set_source() function.\nExample using filter IDs:\nlet a_filter = filter.add_filter(\"vflip:FID=MyFlip\");\n...\na_filter.set_source(filter);\nlet vout_f = filter.add_filter(\"vout:vsync=no\");\n//vout will only accept PID coming from the filter with ID \"MyFlip\"\nvout_f.set_source(a_filter, \"SourceID=MyFlip\");\nExample using PID properties:\nlet an_output = filter.new_pid();\n...\nan_output.set_prop(\"MuxSrc\", \"myPid\");\nlet vout_f = filter.add_filter(\"vout:vsync=no\");\n//vout will only accept PID coming from this filter and with property \"MuxSrc\" set to \"myPid\" \nvout_f.set_source(filter, \"MuxSrc=myPid\");\nIncluding filters in your distribution¶\nJS files located in GPAC distribution or in the directories indicated using -js-dirs option can describe filters usable by the filter engine based on their name (file without extension or directory name).\nSingle file case:\nmyfilters/\nmyfilters/foo.js\nThe filter can be loaded using foo, e.g. gpac -js-dirs=myfilters -i src foo.\nThe JS sources for a given filter can also be gathered in a single folder; in that case, the main JS file shall be called init.js. For example:\nmyfilters/\nmyfilters/foojs/\nmyfilters/foojs/init.js\nThe filter can be loaded using foojs, e.g. gpac -js-dirs=myfilters -i src foojs.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/jsf/jsfilter/"},{"date_scraped_timestamp":1720187998542,"host":"wiki.gpac.io","page_title":"Overview - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nThe multimedia packager available in GPAC is called MP4Box. It is mostly designed for processing ISOBMF files (e.g. MP4, 3GP), but can also be used to import/export media from container files like AVI, MPG, MKV, MPEG-2 TS ...\nFor detailed information on MP4Box options, check this wiki and MP4Box -h.\nThe command-line documentation available in this subsection is based on GPAC 2.0, and is automatically generated from the source tree on a daily basis.\nIn short, MP4Box can be used for:\nmanipulating ISOBMF files like MP4, 3GP, HEIF: removing/editing/tagging tracks, editing for specific devices, tagging... \nimporting audio, video and presentation data (including subtitles) from different sources and in different formats,\npackaging of content for [[HTTP Adaptive Streaming|DASH-intro]] (MPEG-DASH, HLS), RTP/RTSP streaming or HTTP download\nediting/packaging of HEIF image files\nencryption and decryption of streams\nexporting and splitting ISOBMFF files\ntranscoding audio and video\nencoding/decoding presentation languages like MPEG-4 XMT or W3C SVG into/from binary formats like MPEG-4 BIFS or LASeR\nExamples¶\nThis section only lists very basic but common usage of MP4Box. For more examples, check GPAC test suite.\nContent Packaging¶\nMP4Box can be used to repackage existing content to compliant ISO Base Media Files (MP4, 3GP, 3G2, OMA DCF). \nNote MP4Box prior to 0.9.0 does NOT re-encode audio, video and still image content, external tools shall be used for this purpose. With MP4Box 0.9.0 or above, MP4Box can be used in combination with encoding filters to transcode the content.\nRemove specific track/streams from an MP4 file (this removes the third and fourth streams):\nMP4Box -rem 3 -rem 4 file.mp4\nImporting a media file to an existing MP4 file:\nMP4Box -add file.avi my_file.mp4\nImporting a media file to an new MP4 file:\nMP4Box -add file.avi -new new_file.mp4\nAdding a secondary audio track to the previous file:\nMP4Box -add audio2.mp3 new_file.mp4\nMP4Box can import specific media from an existing container. To get the supported media that can be imported from a container:\nMP4Box -info file\nAdd a single audio stream from a container:\nMP4Box -add file.mpg#audio new_file.mp4\nAdd a specific duration of a media from a container:\nMP4Box -add file.mpg#audio:dur=10 new_file.mp4\nAdjust/correct a video stream with an incorrect aspect ratio (DAR = SAR x PAR):\nMP4Box -par 1=4:3 file.mp4\nTo replace the label on an audio or subtitle track (uses udta \"name\" atom, shown by players like vlc):\nMP4Box -udta 3:type=name -udta 3:type=name:str=\"Director Commentary\" file.mp4\nDelivery Setup¶\nMP4Box can be used to prepare files for different delivery protocols, mainly HTTP downloading or RTP streaming.\nTo prepare a file for simple progressive HTTP download, the following instruction will interleave file data by chunks of 500 milliseconds in order to enable playback while downloading the file (HTTP FastStart):\nMP4Box -inter 500 file.mp4\nTo prepare for RTP, the following instruction will create RTP hint tracks for the file. This enables classic streaming servers like DarwinStreamingServer or QuickTime Streaming Server to deliver the file through RTSP/RTP:\nMP4Box -hint file.mp4\nTo prepare for adaptive streaming (MPEG-DASH), the following instruction will create the DASH manifest and associated files. For more information on DASH see [[this page|DASH Support in MP4Box]]:\nMP4Box -dash 1000 file.mp4\nTo prepare for CMAF MPEG-DASH and HLS:\nMP4Box -dash 1000 file.mp4 -out live.m3u8:dual:cmaf\nMP4Box -dash 1000 file.mp4 -out live.mpd --dual --cmaf\nScene Transcoding¶\nMP4Box can be used to encode MPEG-4 scene descriptions BIFS and LASeR and to decode MPEG-4 scene descriptions BIFS and LASeR.\nTo encode an existing description:\nMP4Box -mp4 scene.bt\nNote MP4Box will do its best to encode VRML/X3D to MPEG-4, but that not all tools from X3D or VRML extensions are supported in MPEG-4.\nTo decode an existing BIFS track to a BIFS Text format (VRML-like format)description:\nMP4Box -bt file.mp4\nTo decode an existing BIFS track to XMT-A format:\nMP4Box -xmt file.mp4\nTo decode an existing LASeR track to an XSR format (SAF+LASeR Markup Language) description:\nMP4Box -lsr file.mp4\nTo decode the first sample of an existing LASeR track to an SVG file:\nMP4Box -svg file.mp4\n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/MP4Box/"},{"date_scraped_timestamp":1720187953588,"host":"wiki.gpac.io","page_title":"Overview - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nThis part of the wiki describes general concepts of the GPAC filter architecture, available starting from GPAC 0.9.0.\nThe new architecture allows describing media pipelines as a sequence of processing blocks called filters. Filters in GPAC can be pretty much anything: file/pipe/network access objects, (de-)multiplexers, de/encoders, media segmenters (for HTTP Adaptive Streaming), RTSP server, playlist manager and of course raw domain effects.\nFor more information on this rearchitecture, look here.\nHistorical applications of GPAC (MP4Box, players) typically provide a fixed media pipeline (import or playback) based on a subset of GPAC filters; these pipelines may optionally be extended with custom filters, as illustrated for MP4Box.\nThese applications cannot however create other media pipelines than their built-in ones; a new application, called gpac, has been added to GPAC to allow building completely custom media pipelines, as described here.\nIn this section of the website, you will find the documentation of all options of libgpac, the gpac application and all filters currently built-in. As of 0.9.0, this documentation is automatically generated from the source tree at each new commit.\n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/Filters/?q="},{"date_scraped_timestamp":1720187998542,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"Overview - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nThe multimedia packager available in GPAC is called MP4Box. It is mostly designed for processing ISOBMF files (e.g. MP4, 3GP), but can also be used to import/export media from container files like AVI, MPG, MKV, MPEG-2 TS ...\nFor detailed information on MP4Box options, check this wiki and MP4Box -h.\nThe command-line documentation available in this subsection is based on GPAC 2.0, and is automatically generated from the source tree on a daily basis.\nIn short, MP4Box can be used for:\nmanipulating ISOBMF files like MP4, 3GP, HEIF: removing/editing/tagging tracks, editing for specific devices, tagging... \nimporting audio, video and presentation data (including subtitles) from different sources and in different formats,\npackaging of content for [[HTTP Adaptive Streaming|DASH-intro]] (MPEG-DASH, HLS), RTP/RTSP streaming or HTTP download\nediting/packaging of HEIF image files\nencryption and decryption of streams\nexporting and splitting ISOBMFF files\ntranscoding audio and video\nencoding/decoding presentation languages like MPEG-4 XMT or W3C SVG into/from binary formats like MPEG-4 BIFS or LASeR\nExamples¶\nThis section only lists very basic but common usage of MP4Box. For more examples, check GPAC test suite.\nContent Packaging¶\nMP4Box can be used to repackage existing content to compliant ISO Base Media Files (MP4, 3GP, 3G2, OMA DCF). \nNote MP4Box prior to 0.9.0 does NOT re-encode audio, video and still image content, external tools shall be used for this purpose. With MP4Box 0.9.0 or above, MP4Box can be used in combination with encoding filters to transcode the content.\nRemove specific track/streams from an MP4 file (this removes the third and fourth streams):\nMP4Box -rem 3 -rem 4 file.mp4\nImporting a media file to an existing MP4 file:\nMP4Box -add file.avi my_file.mp4\nImporting a media file to an new MP4 file:\nMP4Box -add file.avi -new new_file.mp4\nAdding a secondary audio track to the previous file:\nMP4Box -add audio2.mp3 new_file.mp4\nMP4Box can import specific media from an existing container. To get the supported media that can be imported from a container:\nMP4Box -info file\nAdd a single audio stream from a container:\nMP4Box -add file.mpg#audio new_file.mp4\nAdd a specific duration of a media from a container:\nMP4Box -add file.mpg#audio:dur=10 new_file.mp4\nAdjust/correct a video stream with an incorrect aspect ratio (DAR = SAR x PAR):\nMP4Box -par 1=4:3 file.mp4\nTo replace the label on an audio or subtitle track (uses udta \"name\" atom, shown by players like vlc):\nMP4Box -udta 3:type=name -udta 3:type=name:str=\"Director Commentary\" file.mp4\nDelivery Setup¶\nMP4Box can be used to prepare files for different delivery protocols, mainly HTTP downloading or RTP streaming.\nTo prepare a file for simple progressive HTTP download, the following instruction will interleave file data by chunks of 500 milliseconds in order to enable playback while downloading the file (HTTP FastStart):\nMP4Box -inter 500 file.mp4\nTo prepare for RTP, the following instruction will create RTP hint tracks for the file. This enables classic streaming servers like DarwinStreamingServer or QuickTime Streaming Server to deliver the file through RTSP/RTP:\nMP4Box -hint file.mp4\nTo prepare for adaptive streaming (MPEG-DASH), the following instruction will create the DASH manifest and associated files. For more information on DASH see [[this page|DASH Support in MP4Box]]:\nMP4Box -dash 1000 file.mp4\nTo prepare for CMAF MPEG-DASH and HLS:\nMP4Box -dash 1000 file.mp4 -out live.m3u8:dual:cmaf\nMP4Box -dash 1000 file.mp4 -out live.mpd --dual --cmaf\nScene Transcoding¶\nMP4Box can be used to encode MPEG-4 scene descriptions BIFS and LASeR and to decode MPEG-4 scene descriptions BIFS and LASeR.\nTo encode an existing description:\nMP4Box -mp4 scene.bt\nNote MP4Box will do its best to encode VRML/X3D to MPEG-4, but that not all tools from X3D or VRML extensions are supported in MPEG-4.\nTo decode an existing BIFS track to a BIFS Text format (VRML-like format)description:\nMP4Box -bt file.mp4\nTo decode an existing BIFS track to XMT-A format:\nMP4Box -xmt file.mp4\nTo decode an existing LASeR track to an XSR format (SAF+LASeR Markup Language) description:\nMP4Box -lsr file.mp4\nTo decode the first sample of an existing LASeR track to an SVG file:\nMP4Box -svg file.mp4\n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/MP4Box/"},{"date_scraped_timestamp":1720187915587,"host":"wiki.gpac.io","page_title":"Overview - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nThis part of the wiki describes general concepts of the GPAC filter architecture, available starting from GPAC 0.9.0.\nThe new architecture allows describing media pipelines as a sequence of processing blocks called filters. Filters in GPAC can be pretty much anything: file/pipe/network access objects, (de-)multiplexers, de/encoders, media segmenters (for HTTP Adaptive Streaming), RTSP server, playlist manager and of course raw domain effects.\nFor more information on this rearchitecture, look here.\nHistorical applications of GPAC (MP4Box, players) typically provide a fixed media pipeline (import or playback) based on a subset of GPAC filters; these pipelines may optionally be extended with custom filters, as illustrated for MP4Box.\nThese applications cannot however create other media pipelines than their built-in ones; a new application, called gpac, has been added to GPAC to allow building completely custom media pipelines, as described here.\nIn this section of the website, you will find the documentation of all options of libgpac, the gpac application and all filters currently built-in. As of 0.9.0, this documentation is automatically generated from the source tree at each new commit.\n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/Filters/"},{"date_scraped_timestamp":1720187995892,"host":"wiki.gpac.io","page_title":"Playback - GPAC wiki","text":"\n \n \n \n \n \n \nIntroduction¶\nGPAC can playback content in two main ways:\nthrough its interactive renderer using the Compositor filter\nthrough simple audio and video output filters.\nThis section of the wiki discusses various use cases for content playback with GPAC.\nSimple Playback¶\nIn most cases your content consists of one audio stream and one video stream and composing them together is not needed.\nYou can play such content using:\ngpac -play source\nThe -play alias resolves to -i @{1} aout vout, so the above is equivalent to:\ngpac -i source aout vout\nIf you only want to play the audio, use\ngpac -i source aout\nIf you only want to play the video, use\ngpac -i source vout\nIf you only want to play the video as fast as possible without dropping, use\ngpac -i source vout:vsync=0\nSee this howto for more details.\nThe media player mode runs the compositor filter with an interactive GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, select streams in a session, view statistics, etc.\nThe player is invoked using gpac application:\n##launching the GUI\ngpac -gui\n##launching the GUI with a target URL\ngpac -gui source_url\nThe GUI accepts command-line options described in \n##help on GUI options\ngpac -h gui\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe simple media player mode runs the compositor filter without any GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, etc.\nThe simple player mode is invoked using gpac application:\ngpac -mp4c source_url\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe available options for this mode can be checked using:\n##help on GUI options\ngpac -h mp4c\nNOTE This mode is mostly used to debug scenes or when GUI is not needed.\n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Playback/?q="},{"date_scraped_timestamp":1720187977571,"host":"wiki.gpac.io","page_title":"Playback - GPAC wiki","text":"\n \n \n \n \n \n \nIntroduction¶\nGPAC can playback content in two main ways:\nthrough its interactive renderer using the Compositor filter\nthrough simple audio and video output filters.\nThis section of the wiki discusses various use cases for content playback with GPAC.\nSimple Playback¶\nIn most cases your content consists of one audio stream and one video stream and composing them together is not needed.\nYou can play such content using:\ngpac -play source\nThe -play alias resolves to -i @{1} aout vout, so the above is equivalent to:\ngpac -i source aout vout\nIf you only want to play the audio, use\ngpac -i source aout\nIf you only want to play the video, use\ngpac -i source vout\nIf you only want to play the video as fast as possible without dropping, use\ngpac -i source vout:vsync=0\nSee this howto for more details.\nThe media player mode runs the compositor filter with an interactive GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, select streams in a session, view statistics, etc.\nThe player is invoked using gpac application:\n##launching the GUI\ngpac -gui\n##launching the GUI with a target URL\ngpac -gui source_url\nThe GUI accepts command-line options described in \n##help on GUI options\ngpac -h gui\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe simple media player mode runs the compositor filter without any GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, etc.\nThe simple player mode is invoked using gpac application:\ngpac -mp4c source_url\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe available options for this mode can be checked using:\n##help on GUI options\ngpac -h mp4c\nNOTE This mode is mostly used to debug scenes or when GUI is not needed.\n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Playback/"},{"date_scraped_timestamp":1720187977571,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"Playback - GPAC wiki","text":"\n \n \n \n \n \n \nIntroduction¶\nGPAC can playback content in two main ways:\nthrough its interactive renderer using the Compositor filter\nthrough simple audio and video output filters.\nThis section of the wiki discusses various use cases for content playback with GPAC.\nSimple Playback¶\nIn most cases your content consists of one audio stream and one video stream and composing them together is not needed.\nYou can play such content using:\ngpac -play source\nThe -play alias resolves to -i @{1} aout vout, so the above is equivalent to:\ngpac -i source aout vout\nIf you only want to play the audio, use\ngpac -i source aout\nIf you only want to play the video, use\ngpac -i source vout\nIf you only want to play the video as fast as possible without dropping, use\ngpac -i source vout:vsync=0\nSee this howto for more details.\nThe media player mode runs the compositor filter with an interactive GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, select streams in a session, view statistics, etc.\nThe player is invoked using gpac application:\n##launching the GUI\ngpac -gui\n##launching the GUI with a target URL\ngpac -gui source_url\nThe GUI accepts command-line options described in \n##help on GUI options\ngpac -h gui\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe simple media player mode runs the compositor filter without any GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, etc.\nThe simple player mode is invoked using gpac application:\ngpac -mp4c source_url\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe available options for this mode can be checked using:\n##help on GUI options\ngpac -h mp4c\nNOTE This mode is mostly used to debug scenes or when GUI is not needed.\n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Playback/"},{"date_scraped_timestamp":1720187995892,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"Playback - GPAC wiki","text":"\n \n \n \n \n \n \nIntroduction¶\nGPAC can playback content in two main ways:\nthrough its interactive renderer using the Compositor filter\nthrough simple audio and video output filters.\nThis section of the wiki discusses various use cases for content playback with GPAC.\nSimple Playback¶\nIn most cases your content consists of one audio stream and one video stream and composing them together is not needed.\nYou can play such content using:\ngpac -play source\nThe -play alias resolves to -i @{1} aout vout, so the above is equivalent to:\ngpac -i source aout vout\nIf you only want to play the audio, use\ngpac -i source aout\nIf you only want to play the video, use\ngpac -i source vout\nIf you only want to play the video as fast as possible without dropping, use\ngpac -i source vout:vsync=0\nSee this howto for more details.\nThe media player mode runs the compositor filter with an interactive GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, select streams in a session, view statistics, etc.\nThe player is invoked using gpac application:\n##launching the GUI\ngpac -gui\n##launching the GUI with a target URL\ngpac -gui source_url\nThe GUI accepts command-line options described in \n##help on GUI options\ngpac -h gui\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe simple media player mode runs the compositor filter without any GUI. It can be used to play interactive BIFS, VRML or SVG files, 360 videos, etc.\nThe simple player mode is invoked using gpac application:\ngpac -mp4c source_url\nUsing this command, the player will use 2 extra threads (i.e. -threads=2 set by default). \nThe available options for this mode can be checked using:\n##help on GUI options\ngpac -h mp4c\nNOTE This mode is mostly used to debug scenes or when GUI is not needed.\n \n \n \n \n ","url":"https://wiki.gpac.io/Player/Playback/?q="},{"date_scraped_timestamp":1720187964569,"host":"wiki.gpac.io","page_title":"ROUTE & ATSC3.0 services - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nGPAC supports both sending and receiving data using the ROUTE (Real-time Object delivery over Unidirectional Transport) RFC9223 protocol.\nIn the media field ROUTE is typically used for multicast ABR. Technically ROUTE allows to broadcast local file systems. This is useful to transport a DASH session over any physical layers (e.g. broadcast (ATSC, DVB, ...), broadband (IP), or hybrid broadcast-broadband). ROUTE technologically inherits from FLUTE.\nReceiving ROUTE implies to dump the local file system described in the ROUTE session. GPAC allows to process this input, including (but not limited to) dumping the files on disk or exposing them through the GPAC embedded HTTP server. This behaviour of re-exposing the data using another protocol is commonly named \"Gateway\".\nSending ROUTE means starting a ROUTE server. This server listens for input data that it will broadcast. It then exposes the data using the ROUTE protocol via a ROUTE URL (in GPAC: route:// or atsc://).\nThe GPAC ROUTE implementation has been tested with both DASH and HLS sessions. The versions of the protocols used for the implementation are ATSC/A331 2017 and 2019. Both the Korean and US flavours of ATSC3 ROUTE are supported (both addressed with atsc://), as well as a generic ROUTE implementation (addressed with route://). Please note that ROUTE was also added to DVB-mABR.\nSetup a ROUTE server¶\nParameters¶\nFirst you need an input content. This can be any input supported by GPAC e.g. a file or a live URL, and you most likely want an adaptive streaming source (DASH, HLS).\nLet's consider the https://akamaibroadcasteruseast.akamaized.net/cmaf/live/657078/akasource/out.mpd URL as an example.\nYou also need a pair of IP address and UDP port. Let's consider 225.1.1.0:6000 as an example.\nStarting the server¶\ngpac -i https://akamaibroadcasteruseast.akamaized.net/cmaf/live/657078/akasource/out.mpd dashin:forward=file -o route://225.1.1.0:6000\nNote\nIf your session is ATSC-compliant, replace route:// by atsc://. Additional options are available in this mode, in particular multiple services and US versus Korean flavour, as described here.\nEnabling low latency¶\nAdd the llmode option:\ngpac -i https://akamaibroadcasteruseast.akamaized.net/cmaf/live/657078/akasource/out.mpd dashin:forward=file -o route://225.1.1.0:6000:llmode\nTechnical details are available here.\nTechnical documentation¶\nSee the route_out filter documentation for technical details.\nSetup ROUTE playback and gateway¶\nLocal playback¶\nTo play a ROUTE session back, simply do:\ngpac -play route://225.1.1.0:6000\nSee the playback howto for more details on content playback with GPAC.\nROUTE gateway¶\nUsing the GPAC HTTP server¶\nYou may want to re-expose a ROUTE session as HTTP (typically to be played back by a third-party player), having gpac act as a gateway.\nLet's consider a HTTP local server address and port at 127.0.0.1:8080:\ngpac -i route://225.1.1.0:6000/:max_segs=4 dashin:forward=file httpout:port=8080 --rdirs=temp --reqlog=* --cors=auto\nDiscussion\n- The dashin filter is required here to indicate we process the ROUTE+DASH session in file mode (no demultiplexing of content). Pushing files using gcache=false is possible but more complex and limited to full segments only for the time being (no low-latency push).\n- The max_segs option limits the number of segments kept on disk, set it to 0 to store the entire session. The number of segments stored on disk is currently NOT derived from the timeshift buffer information of the session. \nYou can also re-push to any output format supported by GPAC, not only DASH/HTTP. For example sending it as an MPEG-2 Transport Stream:\ngpac -i route://225.1.1.0:6000/ -o udp://225.1.1.10:1234/:ext=ts\nPushing to any HTTP ingest/origin¶\ngpac -i route://225.1.1.0:6000 dashin:forward=file -o http://127.0.0.1:8080/live.mpd --hmode=push\nTo test locally, you can start the GPAC HTTP server as standalone, in this example enabling CORS (e.g. for DASH.js access) and logging PUT requests:\ngpac httpout:port=8080:rdirs=$TEMP_DIR:wdir=$TEMP_DIR:reqlog=PUT:cors=auto\nROUTE dumping session¶\nTo dump a ROUTE session to the dump_route folder in standalone mode:\ngpac -i route://225.1.1.0:6000:odir=dump_route\nThe results will be in folder dump_route/serviceN with N the service ID of the session: 1 for pure ROUTE or the ATSC service ID for ATSC 3.0 (details here):\nYou can also forward files to receiving filters and use file templating. The following command will forward received ROUTE files to fout, writing to ATSCN_rec/ folders, with Nthe ATSC service ID:\ngpac -i atsc://225.1.1.0:6000:gcache=false -o ATSC$ServiceID$_rec/$File$:dynext\nTechnical documentation¶\nSee the routein filter documentation for technical details.\nTroubleshooting¶\nAs this is complex technology, small changes can make a big difference in how your system behaves:\nIf your session is ATSC-compliant, replace route:// by atsc://.\nIf no data is seen on the network, your system may require some multicast routing indications.\nYou can improve logging by adding -logs=dash:route@info (or even more verbose: -logs=dash:route@debug) to your command-lines.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/route/"},{"date_scraped_timestamp":1720188122896,"host":"wiki.gpac.io","page_title":"Scene Commands - GPAC wiki","text":"\n \n \n \n \n \n \nWe will now review the syntax of MPEG-4 scene commands in both BT and XMT-A formats. Please remember that BT and XMT languages are case sensitive.\nCommand declaration¶\nBT format¶\nRAP AT time IN esID { .... }\nRAP and IN esID can be omitted most of the time.\nXMT format¶\n<par begin=\"time\" atES_ID=\"esID\" isRAP=\"yes\" > ... </par>\nisRAP and atES_ID can be omitted most of the time. isRAP is a GPAC extension.\nReplacing a simple field¶\nBT format¶\nREPLACE nodeName.fieldName BY newValue\nXMT format¶\n<Replace atNode=\"nodeName\" atField=\"fieldName\" value=\"newValue\" />\nReplacing a SFNode field¶\nBT format¶\nREPLACE nodeName.fieldName BY NodeDeclaration {... }\nXMT format¶\n<Replace atNode=\"nodeName\" atField=\"fieldName\" >\n<NodeXXX />\n</Replace>\nNote that the new node can be DEF'ed, or that a null node may be specified (NULL ).\nReplacing a value in a multiple field¶\nBT format¶\nREPLACE nodeName.fieldName[idx] BY newValue\nXMT format¶\n<Replace atNode=\"nodeName\" atField=\"fieldName\" position=\"idx\" value=\"newValue\" />\nFor XMT-A, idx can also take the special values 'BEGIN' and 'END'. Replacement of a node in an MFNode field is the combination of both syntax\nReplacing a multiple field¶\nBT format¶\nREPLACE nodeName.fieldName BY [value1 ... valueN]\nREPLACE nodeName.fieldName BY [Node { ... } ... Node { ... }]\nXMT format¶\n<Replace atNode=\"nodeName\" atField=\"fieldName\" value=\"value1 ... valueN\" />\n<Replace atNode=\"nodeName\" atField=\"fieldName\" >\n<NodeXXX>...</NodeXXX>\n<NodeXXX>...</NodeXXX>\n</Replace>\nReplacement of a node in an MFNode field is the combination of both syntax\nDeleting a node¶\nBT format¶\nDELETE nodeName.fieldName\nXMT format¶\n<Delete atNode=\"nodeName\" />\n<Delete atNode=\"nodeName\" atField=\"fieldName\" />\nDeleting a value in a multiple field¶\nBT format¶\nDELETE nodeName.fieldName[idx]\nXMT format¶\n<Delete atNode=\"nodeName\" atField=\"fieldName\" position=\"idx\" />\nFor XMT-A, idx can also take the special values 'BEGIN' and 'END'.\nInserting a simple value in a multiple field¶\nBT format¶\nINSERT AT nodeName.fieldName[idx] newValue\nAPPEND TO nodeName.fieldName newValue\nXMT format¶\n<Insert atNode=\"nodeName\" atField=\"fieldName\" position=\"idx\" value=\"newValue\" />\nFor XMT-A, idx can also take the special values 'BEGIN' and 'END'.\nInserting a node in a node list field¶\nBT format¶\nINSERT AT nodeName.fieldName[idx] Node { }\nAPPEND TO nodeName.fieldName Node { }\nXMT format¶\n<Insert atNode=\"nodeName\" atField=\"fieldName\" position=\"idx\" >\n<NodeXXX>...</NodeXXX>\n</Insert>\nFor XMT-A, idx can also take the special values 'BEGIN' and 'END'.\nReplacing a route¶\nBT format¶\nREPLACE ROUTE routeName BY nodeName1.fieldName1 TO nodeName2.fieldName2\nXMT format¶\n<Replace atRoute=\"routeName\">\n<ROUTE fromNode=\"nodeName1\" fromfield=\"fieldName1\" toNode=\"nodeName2\" toField=\"fieldName2\" />\n</Replace>\nInserting a route¶\nBT format¶\nINSERT ROUTE nodeName1.fieldName1 TO nodeName2.fieldName2\nXMT format¶\n<Insert>\n<ROUTE fromNode=\"nodeName1\" fromfield=\"fieldName1\" toNode=\"nodeName2\" toField=\"fieldName2\" />\n</Insert>\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/scenecoding/MPEG-4-Scene-Commands/"},{"date_scraped_timestamp":1720187994933,"host":"wiki.gpac.io","page_title":"Scene Description - GPAC wiki","text":"\n \n \n \n \n \n \nMPEG-4 Scene Encoding Options¶\nGeneral considerations¶\nMP4Box supports encoding and decoding of of BT, XMT, VRML and (partially) X3D formats int MPEG-4 BIFS, and encoding and decoding of XSR and SVG into MPEG-4 LASeR\nAny media track specified through a MuxInfo element will be imported in the resulting MP4 file.\nSee https://wiki.gpac.io/Howtos/scenecoding/MPEG-4-BIFS-Textual-Format and related pages. \nScene Random Access¶\nMP4Box can encode BIFS or LASeR streams and insert random access points at a given frequency. This is useful when packaging content for broadcast, where users will not turn in the scene at the same time. In MPEG-4 terminology, this is called the scene carousel.## BIFS Chunk Processing\nThe BIFS chunk encoding mode allows encoding single BIFS access units from an initial context and a set of commands.\nThe generated AUs are raw BIFS (not SL-packetized), in files called FILE-ESID-AUIDX.bifs, with FILE the basename of the input file.\nCommands with a timing of 0 in the input will modify the carousel version only (i.e. output context).\nCommands with a timing different from 0 in the input will generate new AUs. \nOptions:\n-mp4: specify input file is for BIFS/LASeR encoding\n-def: encode DEF names in BIFS\n-sync (int): force BIFS sync sample generation every given time in ms (cannot be used with -shadow or -carousel )\n-shadow (int): force BIFS sync shadow sample generation every given time in ms (cannot be used with -sync or -carousel )\n-carousel (int): use BIFS carousel (cannot be used with -sync or -shadow )\n-sclog: generate scene codec log file if available\n-ms (string): import tracks from the given file\n-ctx-in (string): specify initial context (MP4/BT/XMT) file for chunk processing. Input file must be a commands-only file\n-ctx-out (string): specify storage of updated context (MP4/BT/XMT) file for chunk processing, optional\n-resolution (int): resolution factor (-8 to 7, default 0) for LASeR encoding, and all coordinates are multiplied by 2^res before truncation (LASeR encoding)\n-coord-bits (int): number of bits used for encoding truncated coordinates (0 to 31, default 12) (LASeR encoding)\n-scale-bits (int): extra bits used for encoding truncated scales (0 to 4, default 0) (LASeR encoding)\n-auto-quant (int): resolution is given as if using -resolution but coord-bits and scale-bits are inferred (LASeR encoding)\n-global-quant (int): resolution is given as if using -resolution but the res is inferred (BIFS encoding) \nLive Scene Encoder Options¶\nThe options shall be specified as opt_name=opt_val.\nOptions: \n-live: enable live BIFS/LASeR encoder\n-dst (string): destination IP\n-port (int, default: 7000): destination port\n-mtu (int, default: 1450): path MTU for RTP packets\n-ifce (string): IP address of the physical interface to use\n-ttl (int, default: 1): time to live for multicast packets\n-sdp (string, default: session.sdp): output SDP file\n-dims: turn on DIMS mode for SVG input\n-no-rap: disable RAP sending and carousel generation\n-src (string): source of scene updates\n-rap (int): duration in ms of base carousel; you can specify the RAP period of a single ESID (not in DIMS) using ESID=X:time \nRuntime options: \nq: quits \nu: inputs some commands to be sent \nU: same as u but signals the updates as critical \ne: inputs some commands to be sent without being aggregated \nE: same as e but signals the updates as critical \nf: forces RAP sending \nF: forces RAP regeneration and sending \np: dumps current scene \nSWF Importer Options¶\nMP4Box can import simple Macromedia Flash files (\".SWF\")\nYou can specify a SWF input file with '-bt', '-xmt' and '-mp4' options \nOptions:\n-global: all SWF defines are placed in first scene replace rather than when needed\n-no-ctrl: use a single stream for movie control and dictionary (this will disable ActionScript)\n-no-text: remove all SWF text\n-no-font: remove all embedded SWF Fonts (local playback host fonts used)\n-no-line: remove all lines from SWF shapes\n-no-grad: remove all gradients from swf shapes\n-quad: use quadratic bezier curves instead of cubic ones\n-xlp: support for lines transparency and scalability\n-ic2d: use indexed curve 2D hardcoded proto\n-same-app: appearance nodes are reused\n-flatten (number): complementary angle below which 2 lines are merged, value 0 means no flattening \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-scene-opts/"},{"date_scraped_timestamp":1720188059683,"host":"wiki.gpac.io","page_title":"Scene Description - GPAC wiki","text":"\n \n \n \n \n \n \nMPEG-4 Scene Encoding Options¶\nGeneral considerations¶\nMP4Box supports encoding and decoding of of BT, XMT, VRML and (partially) X3D formats int MPEG-4 BIFS, and encoding and decoding of XSR and SVG into MPEG-4 LASeR\nAny media track specified through a MuxInfo element will be imported in the resulting MP4 file.\nSee https://wiki.gpac.io/Howtos/scenecoding/MPEG-4-BIFS-Textual-Format and related pages. \nScene Random Access¶\nMP4Box can encode BIFS or LASeR streams and insert random access points at a given frequency. This is useful when packaging content for broadcast, where users will not turn in the scene at the same time. In MPEG-4 terminology, this is called the scene carousel.## BIFS Chunk Processing\nThe BIFS chunk encoding mode allows encoding single BIFS access units from an initial context and a set of commands.\nThe generated AUs are raw BIFS (not SL-packetized), in files called FILE-ESID-AUIDX.bifs, with FILE the basename of the input file.\nCommands with a timing of 0 in the input will modify the carousel version only (i.e. output context).\nCommands with a timing different from 0 in the input will generate new AUs. \nOptions:\n-mp4: specify input file is for BIFS/LASeR encoding\n-def: encode DEF names in BIFS\n-sync (int): force BIFS sync sample generation every given time in ms (cannot be used with -shadow or -carousel )\n-shadow (int): force BIFS sync shadow sample generation every given time in ms (cannot be used with -sync or -carousel )\n-carousel (int): use BIFS carousel (cannot be used with -sync or -shadow )\n-sclog: generate scene codec log file if available\n-ms (string): import tracks from the given file\n-ctx-in (string): specify initial context (MP4/BT/XMT) file for chunk processing. Input file must be a commands-only file\n-ctx-out (string): specify storage of updated context (MP4/BT/XMT) file for chunk processing, optional\n-resolution (int): resolution factor (-8 to 7, default 0) for LASeR encoding, and all coordinates are multiplied by 2^res before truncation (LASeR encoding)\n-coord-bits (int): number of bits used for encoding truncated coordinates (0 to 31, default 12) (LASeR encoding)\n-scale-bits (int): extra bits used for encoding truncated scales (0 to 4, default 0) (LASeR encoding)\n-auto-quant (int): resolution is given as if using -resolution but coord-bits and scale-bits are inferred (LASeR encoding)\n-global-quant (int): resolution is given as if using -resolution but the res is inferred (BIFS encoding) \nLive Scene Encoder Options¶\nThe options shall be specified as opt_name=opt_val.\nOptions: \n-live: enable live BIFS/LASeR encoder\n-dst (string): destination IP\n-port (int, default: 7000): destination port\n-mtu (int, default: 1450): path MTU for RTP packets\n-ifce (string): IP address of the physical interface to use\n-ttl (int, default: 1): time to live for multicast packets\n-sdp (string, default: session.sdp): output SDP file\n-dims: turn on DIMS mode for SVG input\n-no-rap: disable RAP sending and carousel generation\n-src (string): source of scene updates\n-rap (int): duration in ms of base carousel; you can specify the RAP period of a single ESID (not in DIMS) using ESID=X:time \nRuntime options: \nq: quits \nu: inputs some commands to be sent \nU: same as u but signals the updates as critical \ne: inputs some commands to be sent without being aggregated \nE: same as e but signals the updates as critical \nf: forces RAP sending \nF: forces RAP regeneration and sending \np: dumps current scene \nSWF Importer Options¶\nMP4Box can import simple Macromedia Flash files (\".SWF\")\nYou can specify a SWF input file with '-bt', '-xmt' and '-mp4' options \nOptions:\n-global: all SWF defines are placed in first scene replace rather than when needed\n-no-ctrl: use a single stream for movie control and dictionary (this will disable ActionScript)\n-no-text: remove all SWF text\n-no-font: remove all embedded SWF Fonts (local playback host fonts used)\n-no-line: remove all lines from SWF shapes\n-no-grad: remove all gradients from swf shapes\n-quad: use quadratic bezier curves instead of cubic ones\n-xlp: support for lines transparency and scalability\n-ic2d: use indexed curve 2D hardcoded proto\n-same-app: appearance nodes are reused\n-flatten (number): complementary angle below which 2 lines are merged, value 0 means no flattening \n \n \n \n \n ","url":"https://wiki.gpac.io/MP4Box/mp4box-scene-opts/?q="},{"date_scraped_timestamp":1720188019188,"host":"wiki.gpac.io","page_title":"TTXT Format - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nThe 3GPP consortium has defined a standard for text streaming, independent of any scene description such as SMIL, SVG or BIFS: 3GPP Timed Text. MP4Box supports this standard and uses its own textual format to describe a streaming text session.\nAs of version 0.2.3, GPAC supports 3GPP timed text, also known as MPEG-4 Streaming Text. These standards provide a way to stream text and styles applying to text such as font size, color, highlighting. This is a very convenient way to encode subtitles, tickers, etc,... independently of the scene description language (MPEG-4, X3D, SVG/SMIL, ...).\nFor regular MPEG-4/VRML static text, please refer to the MPEG-4 tutorial or any VRML tutorials available on the internet.\nOverview of a timed text stream¶\nThe 3GPP timed text specification is called 3GPP TS 26.245 (3GPP web site) and will give you in-depth knowledge of this format. If you don't want to read and understand all the technical details, there are some important things to know about timed text streams, so let's review them.\nA text stream can be divided in two major sections, very much like most modern audio/video codecs: the configuration data for the decoder and the data samples. As you can guess, this is quite different from most subtitling formats, where subtitles configuration is left up to the player (color, font size, positioning, etc..). \nAuthoring 3GPP text streams therefore requires careful design as to which font size shall be picked, color used and similar. The terminal may not changed these settings as these have been chosen by the author, in other words the text stream becomes an element of the final content just like video or audio tracks (think of it as DVD subtitles whose styles and position are part of the DVD and cannot be changed by the DVD player).\nDecoder Configuration¶\nThe configuration data of a text stream provides information about positioning in the visual display, the list of ALL fonts used in the text stream and default styles (color, bold/italic, background color).\nPositioning¶\nJust like any video or image media, a text stream has size information, usually called Text Track size. This size defines the maximum width and height used by the text stream. All text will be drawn within this defined rectangle which will also act as a clipper (i.e., nothing will appear outside it). \nThis rectangle can be positioned in the final display through horizontal and vertical offsets, allowing for instance to define a text track of 320*50 pixels at the bottom of a 320*240 video.\nDefault Styles¶\nSamples in a text stream usually use the default properties defined in the decoder configuration. Since some styles may be used quite often in a text stream (for example, italic), the decoder configuration data contains a list of default styles, and samples then refer to a specific style in this list. This is known as the TextSampleEntry in 3GPP. \nA single TextSampleEntry contains:\nA list of font names (\"Times\", \"Arial\", etc) with an associated ID - fonts are ALWAYS referenced by IDs in a 3GPP text stream. A compliant decoder shall be able to understand the default font names \"Serif\", \"Sans-serif\" and \"Monospace\".\nText Styles: default font ID, font size, font styles (italic, bold, underlined), text color in RGBA colorspace.\nText Position: in a 3GPP text stream, the text may be drawn in a given rectangle within the stream main display rectangle. This is known as the TextBox in 3GPP. Any text justification is always performed against this TextBox, not against the main display rectangle.\nOther styles: background color in RGBA colorspace, horizontal and vertical justification, text scrolling modes, etc..\nText Samples¶\nText samples convey text data and, optionally, temporary styles to apply to this text. The text data can be encoded in UTF-8 or UTF-16, and may be empty (typically used for subtitles). The temporary styles are known as TextModifiers in 3GPP, and are of two kinds: those impacting the entire text, and those impacting only a sub-string of the text, identified through starting and ending characters.\nSub-String Text Modifiers¶\nStyles: overrides text style (font name, size and style, text color) for a substring of the text. All text characters not covered by temporary modifiers use the default text style from the decoder configuration.\nHighlight: specifies a given substring shall be highlighted.\nHyper Text: specifies a given substring shall be treated as an hyper-link, and gives associated URL.\nBlinking: specifies a given substring shall blink - blinking frequency is left up to the decoder.\nKaraoke: specifies how highlighting shall be dynamically applied to text characters for a karaoke style.\nComplete String Text Modifiers¶\nText Box: specifies a temporary box where this text sample is displayed.\nHighlight Color: specifies the highlight color when highlighting is used. When no highlighting color is given the decoder should use reverse video.\nWrap: specifies text wrapping shall be done on the text string if needed.\nScroll Delay: specifies the amount of time a text shall be presented statically to the user during a scroll.\nGPAC TTXT Format¶\nThere is no official textual representation of a text stream. Moreover, the specification relies on IsoMedia knowledge for most structure descriptions. \nIn order to help authoring text streams, an XML format has been developed in GPAC, called TTXT for timed-text - the extension used being.ttxt. \nThis format is not related in any way to any scene description language to keep the timed text authoring a standalone step in the authoring process.\nObtaining a sample TTXT file¶\nAs said above, this format has been developed in GPAC, and you will likely not find any tool other than MP4Box supporting this format for quite some time. To get a sample file, you have two possibilities:\nFind a 3GP file with a text track, and run MP4Box -ttxt trackID file.3gp. This will dump the text track in TTXT format.\nFind an SRT or SUB subtitle file, and run MP4Box -ttxt file.srt. This will convert the subtitles in TTXT format.\nSyntax of a TTXT file¶\nThe TTXT format is an XML description of the timed text stream, and as such MUST begin with the usual XML header with encoding hints - only UTF-8 has currently been tested. \nThe text stream is encapsulated in a single element at the root of the document, called TextStream. This element has a single defined attribute version identifying the format version - the current and only defined version is \"1.0\". \nThe TextStream element must have one and only one TextStreamHeader child and has as many TextSample children as desired. \nNote: All coordinates are specified in pixels, framebuffer-coordinate like:\nX-axis increases from left to right, with origin (0) on left edge.\nY-axis increases from top to bottom, with origin (0) on top edge.\nThe TextStreamHeader describes all 3GPP text stream decoder configuration. It must contain at least one TextSampleDescription element.\nSyntax¶\n<TextStreamHeader width=\"...\" height=\"...\" translation_x=\"...\" translation_y=\"...\" layer=\"...\" />\n ...\n</TextStreamHeader>\nSemantics¶\nwidth : defines text stream width in pixels (type: unsigned integer). Default value is 400 pixels.\nheight : defines text stream height in pixels (type: unsigned integer). Default value is 80 pixels.\ntranslation_x : defines text stream horizontal translation in main display in pixels (type: signed integer). Default value is 0 pixels.\ntranslation_y : defines text stream vertical translation in main display in pixels (type: signed integer). Default value is 0 pixels.\nlayer : defines text stream z-order (type: signed short). This is only needed when composing several text streams in a single presentation: more negative layer values are towards the viewer. Default value is 0.\nTextSampleDescription¶\nThe TextSampleDescription element may be present as many times as desired in a TextStreamHeader. It defines the default styles text samples may refer to in the stream. The TextSampleDescription may also contain zero or one FontTable, TextBox and Style.\nSyntax¶\n<TextSampleDescription horizontalJustification=\"...\" verticalJustification=\"...\" backColor=\"...\" verticalText=\"...\" fillTextRegion=\"...\" continuousKaraoke=\"...\" scroll=\"...\" scrollMode=\"...\" >\n ...\n</TextSampleDescription>\nSemantics¶\nhorizontalJustification : specifies horizontal text justification in the TextBox. Possible values are \"center\", \"left\" and \"right\". Default value is \"left\".\nverticalJustification : specifies vertical text justification in the TextBox. Possible values are\"center\", \"top\" and \"bottom\". Default value is \"bottom\".\nbackColor : specifies the color to use for background fill. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"00 00 00 00\" (no back color).\nverticalText : specifies whether the text shall be drawn vertically or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nfillTextRegion : specifies whether the entire text stream rectangle shall be filled with the backColor, or only the TextBox. Possible values are \"yes\" and \"no\". Default value is \"no\".\ncontinuousKaraoke : specifies whether karaoke is continuous (all characters from beginning of text samples are highlighted) or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nscroll : specifies text scrolling mode. Possible values are \"In\" (text is scrolling in), \"Out\"(text is scrolling out), \"InOut\" (text is scrolling in then out) and \"None\" (text is not being scrolled). Default value is \"None\".\nscrollMode : specifies text scrolling mode. Possible values are \"Credits\" (scroll from bottom to top), \"Marquee\" (scroll from right to left), \"Down\" (scroll from top to bottom) and\"Right\" (scroll from left to right). Default value is \"Credits\".\nFontTable¶\nThe FontTable element specifies all fonts used by samples referring to this stream description. There should be one and only one FontTable element in a TextSampleDescription. If not found, the default \"Serif\" font will be used with an ID of 1.\nSyntax¶\n<FontTable>\n <FontTableEntry fontID=\"...\" fontName=\"...\" />\n</FontTable>\nSemantics¶\nThe FontTable has no attribute, it is a collection of FontTableEntry elements. The FontTableEntry element has two attributes:\nfontName : specifies the font name to use. A terminal shall understand the names Serif,Sans-Serif and Monospace.\nfontID : specifies the associated ID.\nNOTE : There are no default values for this element, omitting values will result in undefined stream behaviour.\nTextBox¶\nThe TextBox element specifies where the text should be drawn within the stream main display rectangle. There should be one TextBox in a TextSampleDescription element. If the text box is not found or empty, the entire display rectangle will be used for the default text box.\nSyntax¶\n<TextBox top=\"...\" left=\"...\" bottom=\"...\" right=\"...\" />\nSemantics¶\ntop : specifies vertical offset from stream main display rectangle top edge (type: signed integer). Default value: 0 pixels.\nleft : specifies horizontal offset from stream main display rectangle left edge (type: signed integer). Default value: 0 pixels.\nbottom : specifies vertical extend of the text box (type: signed integer). Default value: 0 pixels.\nright : specifies horizontal extend of the text box (type: signed integer). Default value: 0 pixels.\nStyle¶\nThe Style element specifies the default text style for this sample description. There should be one and only one Style element in a TextSampleDescription. If not found, all default values are used with a fontID of 1.\nSyntax¶\n<Style fromChar=\"...\" toChar=\"...\" fontID=\"...\" fontSize=\"...\" color=\"...\" styles=\"...\" />\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\nfontID : specifies the ID of the font to use, as defined in the FontTable element (type: unsigned integer). Default value: 1.\nfontSize : specifies the font size to use (type: unsigned integer). Default value: 18.\ncolor : specifies the color to use for text. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"ff ff ff ff\" (opaque white).\nstyles : specifies the font styles to use (type: string). If set, it shall consist of a space-separated list of the following styles: \"Bold\" (text is in bold), \"Italic\" (text is in italic) and\"Underlined\" (text is underlined). Default value: no style (\"\").\nTextSample¶\nThe TextSample element describes a given text sample and all its associated style. Most of the time, no children elements will be specified.\nSyntax¶\n<TextSample sampleTime=\"...\" sampleDescriptionIndex=\"...\" text=\"...\" scrollDelay=\"...\" highlightColor=\"...\" wrap=\"...\" >\n...\n</TextSample>\nSemantics¶\nsampleTime : specifies the time at which the text sample shall be displayed. Time can be expressed in the usual \"hh:mm:ss.ms\" in hours, minutes, seconds and milliseconds format, or as a double-precision number in second unit. Default value is \"0\" second.\nsampleDescriptionIndex : specifies the TextSampleDescription this sample referred to. This is a 1-based index, value 0 is forbidden. Default value is \"1\", which means you do not need to specify this field most of the time if the main sample description is the first one.\ntext : the text data itself. String shall be formatted as a series of lines, each line being enclosed by single-quote characters ('). This text MUST follow XML text encoding conventions. Currently only UTF-8 text is supported.\nscrollDelay : specifies the scrollDelay in seconds (type: double-precision number. This is the delay after a scroll In and/or before scroll Out. Default value \"0\".\nhighlightColor : specifies the highlight color to be used by any enclosed highlighted strings (including karaoke). Expressed as other colors (see Styles above). Default value \"0 0 0 0\".\nwrap : specifies whether text should be wraped or not. Possible values are \"Automatic\" (text is wrapped by terminal) or \"None\" (text is not wrapped). Default value is \"None\".\nWhen text style modification or any special text effects are desired, they are described through children elements of the TextSample element. It is invalid to specify several modifiers of the same type (for instance two hyper-links) on the same character, you must make sure modifiers of a same type do not have overlapping character ranges.\nStyle¶\nSee above for semantics.\nTextBox¶\nSee above for semantics. At most one TextBox modifier shall be set per sample.\nHighlight¶\nThe Highlight modifier indicates a given substring shall be highlighted.\nSyntax¶\n<Highlight fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nBlinking¶\nThe Blinking modifier indicates a given substring shall blink. The blinking rate is up to the terminal.\nSyntax¶\n<Blinking fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nHyperLink¶\nThe Hyperlink modifier indicates that a given substring shall be treated as a hyper link.\nSyntax¶\n<Hyperlink fromChar=\"...\" toChar=\"...\" URL=\"...\" URLToolTip=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nURL : URL this HyperLink is linking to. UTF-8 only format supported.\nURLToolTip : \"tooltip\" presented to the user if supported by decoder. UTF-8 only format supported.\nKaraoke¶\nThe Karaoke element indicates dynamic highlighting, or karaoke, applies to this sample. At most one Karaoke element shall be set per TextSample. The Karaoke is defined as a sequence of highlighting times, all specified as children element of the Karaoke through the KaraokeRangeelement.\nSyntax¶\n<Karaoke startTime=\"...\">\n <KaraokeRange fromChar=\"...\" toChar=\"...\" endTime=\"...\"/>\n< /Karaoke >\nSemantics¶\nstartTime : specifies the highlighting start time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\").\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nendTime : specifies the highlighting end time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\"). The start time of a karaoke segment is the end time of the previous one, or the start time of the Karaoke element.\nEncoding of text streams¶\nUsing text streams in a 3GPP file or outisde the MPEG-4 Systems scene description is quite straightforward. All you need to do is add the text stream to your (existing) file with MP4Box. \nFor example, adding a text track and an AVI file into a new 3GP file can be done in a single call:\nMP4Box -3gp -add movie.avi -add text.ttxt dest.3gp.\nNote: By default, when importing ttxt with MP4Box, the duration of the last text sample is the same as the duration of the previous sample. To specify the duration of the last sample, add an extra text sample at the end with no text content.\nNote that you can use either SRT/SUB subtitles or TTXT files with the -add option. Other subtitles formats are currently not supported.\nUsing text streams within MPEG-4 scene description is much more tricky (is it?). You will add an ObjectDescriptor to your scene as you add an audio/video object, specifying the file name with the usual MuxInfo. Note: when using SRT/SUB, make sure you don't have TextNode specified in the MuxInfo, as this ALWAYS triggers subtitles to BIFS conversion and not 3GPP timed text.\nThe second step is controlling the new text object in the same way you control an audio or visual object. This is done through the AnimationStream node. Instead of controlling a BIFS stream, you can start/stop/play the text stream with this node.\nGPAC Implementation¶\nDuring playback, GPAC does not support dynamic highlighting (Karaoke) nor soft text wrapping (wrapping is only done at newline characters).\nGPAC should support vertical text drawing and alignment, but this has not be really tested yet.\nGPAC should support UTF-16 text in decoding and hinting, but DOES NOT SUPPORT UTF-16 text at encoding time yet.\nGPAC should support text streams placed below the main video or on its right, but cannot currently handle text streams placed above or on the left of the main video.\nSample TTXT Files¶\nThe following files are taken from a 3GPP test suite and are pure translations to TTXT with MP4Box. They should give you a good overview of the format and help you author your own test tracks.\nDownload GPAC TTXT sample streams .zip .tgz\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/TTXT-Format-Documentation/?q="},{"date_scraped_timestamp":1720188002521,"error":"Aborted due to cross-transaction contention. This occurs when multiple transactions attempt to access the same data, requiring Firestore to abort at least one in order to enforce serializability.","host":"wiki.gpac.io","page_title":"TTXT Format - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nThe 3GPP consortium has defined a standard for text streaming, independent of any scene description such as SMIL, SVG or BIFS: 3GPP Timed Text. MP4Box supports this standard and uses its own textual format to describe a streaming text session.\nAs of version 0.2.3, GPAC supports 3GPP timed text, also known as MPEG-4 Streaming Text. These standards provide a way to stream text and styles applying to text such as font size, color, highlighting. This is a very convenient way to encode subtitles, tickers, etc,... independently of the scene description language (MPEG-4, X3D, SVG/SMIL, ...).\nFor regular MPEG-4/VRML static text, please refer to the MPEG-4 tutorial or any VRML tutorials available on the internet.\nOverview of a timed text stream¶\nThe 3GPP timed text specification is called 3GPP TS 26.245 (3GPP web site) and will give you in-depth knowledge of this format. If you don't want to read and understand all the technical details, there are some important things to know about timed text streams, so let's review them.\nA text stream can be divided in two major sections, very much like most modern audio/video codecs: the configuration data for the decoder and the data samples. As you can guess, this is quite different from most subtitling formats, where subtitles configuration is left up to the player (color, font size, positioning, etc..). \nAuthoring 3GPP text streams therefore requires careful design as to which font size shall be picked, color used and similar. The terminal may not changed these settings as these have been chosen by the author, in other words the text stream becomes an element of the final content just like video or audio tracks (think of it as DVD subtitles whose styles and position are part of the DVD and cannot be changed by the DVD player).\nDecoder Configuration¶\nThe configuration data of a text stream provides information about positioning in the visual display, the list of ALL fonts used in the text stream and default styles (color, bold/italic, background color).\nPositioning¶\nJust like any video or image media, a text stream has size information, usually called Text Track size. This size defines the maximum width and height used by the text stream. All text will be drawn within this defined rectangle which will also act as a clipper (i.e., nothing will appear outside it). \nThis rectangle can be positioned in the final display through horizontal and vertical offsets, allowing for instance to define a text track of 320*50 pixels at the bottom of a 320*240 video.\nDefault Styles¶\nSamples in a text stream usually use the default properties defined in the decoder configuration. Since some styles may be used quite often in a text stream (for example, italic), the decoder configuration data contains a list of default styles, and samples then refer to a specific style in this list. This is known as the TextSampleEntry in 3GPP. \nA single TextSampleEntry contains:\nA list of font names (\"Times\", \"Arial\", etc) with an associated ID - fonts are ALWAYS referenced by IDs in a 3GPP text stream. A compliant decoder shall be able to understand the default font names \"Serif\", \"Sans-serif\" and \"Monospace\".\nText Styles: default font ID, font size, font styles (italic, bold, underlined), text color in RGBA colorspace.\nText Position: in a 3GPP text stream, the text may be drawn in a given rectangle within the stream main display rectangle. This is known as the TextBox in 3GPP. Any text justification is always performed against this TextBox, not against the main display rectangle.\nOther styles: background color in RGBA colorspace, horizontal and vertical justification, text scrolling modes, etc..\nText Samples¶\nText samples convey text data and, optionally, temporary styles to apply to this text. The text data can be encoded in UTF-8 or UTF-16, and may be empty (typically used for subtitles). The temporary styles are known as TextModifiers in 3GPP, and are of two kinds: those impacting the entire text, and those impacting only a sub-string of the text, identified through starting and ending characters.\nSub-String Text Modifiers¶\nStyles: overrides text style (font name, size and style, text color) for a substring of the text. All text characters not covered by temporary modifiers use the default text style from the decoder configuration.\nHighlight: specifies a given substring shall be highlighted.\nHyper Text: specifies a given substring shall be treated as an hyper-link, and gives associated URL.\nBlinking: specifies a given substring shall blink - blinking frequency is left up to the decoder.\nKaraoke: specifies how highlighting shall be dynamically applied to text characters for a karaoke style.\nComplete String Text Modifiers¶\nText Box: specifies a temporary box where this text sample is displayed.\nHighlight Color: specifies the highlight color when highlighting is used. When no highlighting color is given the decoder should use reverse video.\nWrap: specifies text wrapping shall be done on the text string if needed.\nScroll Delay: specifies the amount of time a text shall be presented statically to the user during a scroll.\nGPAC TTXT Format¶\nThere is no official textual representation of a text stream. Moreover, the specification relies on IsoMedia knowledge for most structure descriptions. \nIn order to help authoring text streams, an XML format has been developed in GPAC, called TTXT for timed-text - the extension used being.ttxt. \nThis format is not related in any way to any scene description language to keep the timed text authoring a standalone step in the authoring process.\nObtaining a sample TTXT file¶\nAs said above, this format has been developed in GPAC, and you will likely not find any tool other than MP4Box supporting this format for quite some time. To get a sample file, you have two possibilities:\nFind a 3GP file with a text track, and run MP4Box -ttxt trackID file.3gp. This will dump the text track in TTXT format.\nFind an SRT or SUB subtitle file, and run MP4Box -ttxt file.srt. This will convert the subtitles in TTXT format.\nSyntax of a TTXT file¶\nThe TTXT format is an XML description of the timed text stream, and as such MUST begin with the usual XML header with encoding hints - only UTF-8 has currently been tested. \nThe text stream is encapsulated in a single element at the root of the document, called TextStream. This element has a single defined attribute version identifying the format version - the current and only defined version is \"1.0\". \nThe TextStream element must have one and only one TextStreamHeader child and has as many TextSample children as desired. \nNote: All coordinates are specified in pixels, framebuffer-coordinate like:\nX-axis increases from left to right, with origin (0) on left edge.\nY-axis increases from top to bottom, with origin (0) on top edge.\nThe TextStreamHeader describes all 3GPP text stream decoder configuration. It must contain at least one TextSampleDescription element.\nSyntax¶\n<TextStreamHeader width=\"...\" height=\"...\" translation_x=\"...\" translation_y=\"...\" layer=\"...\" />\n ...\n</TextStreamHeader>\nSemantics¶\nwidth : defines text stream width in pixels (type: unsigned integer). Default value is 400 pixels.\nheight : defines text stream height in pixels (type: unsigned integer). Default value is 80 pixels.\ntranslation_x : defines text stream horizontal translation in main display in pixels (type: signed integer). Default value is 0 pixels.\ntranslation_y : defines text stream vertical translation in main display in pixels (type: signed integer). Default value is 0 pixels.\nlayer : defines text stream z-order (type: signed short). This is only needed when composing several text streams in a single presentation: more negative layer values are towards the viewer. Default value is 0.\nTextSampleDescription¶\nThe TextSampleDescription element may be present as many times as desired in a TextStreamHeader. It defines the default styles text samples may refer to in the stream. The TextSampleDescription may also contain zero or one FontTable, TextBox and Style.\nSyntax¶\n<TextSampleDescription horizontalJustification=\"...\" verticalJustification=\"...\" backColor=\"...\" verticalText=\"...\" fillTextRegion=\"...\" continuousKaraoke=\"...\" scroll=\"...\" scrollMode=\"...\" >\n ...\n</TextSampleDescription>\nSemantics¶\nhorizontalJustification : specifies horizontal text justification in the TextBox. Possible values are \"center\", \"left\" and \"right\". Default value is \"left\".\nverticalJustification : specifies vertical text justification in the TextBox. Possible values are\"center\", \"top\" and \"bottom\". Default value is \"bottom\".\nbackColor : specifies the color to use for background fill. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"00 00 00 00\" (no back color).\nverticalText : specifies whether the text shall be drawn vertically or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nfillTextRegion : specifies whether the entire text stream rectangle shall be filled with the backColor, or only the TextBox. Possible values are \"yes\" and \"no\". Default value is \"no\".\ncontinuousKaraoke : specifies whether karaoke is continuous (all characters from beginning of text samples are highlighted) or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nscroll : specifies text scrolling mode. Possible values are \"In\" (text is scrolling in), \"Out\"(text is scrolling out), \"InOut\" (text is scrolling in then out) and \"None\" (text is not being scrolled). Default value is \"None\".\nscrollMode : specifies text scrolling mode. Possible values are \"Credits\" (scroll from bottom to top), \"Marquee\" (scroll from right to left), \"Down\" (scroll from top to bottom) and\"Right\" (scroll from left to right). Default value is \"Credits\".\nFontTable¶\nThe FontTable element specifies all fonts used by samples referring to this stream description. There should be one and only one FontTable element in a TextSampleDescription. If not found, the default \"Serif\" font will be used with an ID of 1.\nSyntax¶\n<FontTable>\n <FontTableEntry fontID=\"...\" fontName=\"...\" />\n</FontTable>\nSemantics¶\nThe FontTable has no attribute, it is a collection of FontTableEntry elements. The FontTableEntry element has two attributes:\nfontName : specifies the font name to use. A terminal shall understand the names Serif,Sans-Serif and Monospace.\nfontID : specifies the associated ID.\nNOTE : There are no default values for this element, omitting values will result in undefined stream behaviour.\nTextBox¶\nThe TextBox element specifies where the text should be drawn within the stream main display rectangle. There should be one TextBox in a TextSampleDescription element. If the text box is not found or empty, the entire display rectangle will be used for the default text box.\nSyntax¶\n<TextBox top=\"...\" left=\"...\" bottom=\"...\" right=\"...\" />\nSemantics¶\ntop : specifies vertical offset from stream main display rectangle top edge (type: signed integer). Default value: 0 pixels.\nleft : specifies horizontal offset from stream main display rectangle left edge (type: signed integer). Default value: 0 pixels.\nbottom : specifies vertical extend of the text box (type: signed integer). Default value: 0 pixels.\nright : specifies horizontal extend of the text box (type: signed integer). Default value: 0 pixels.\nStyle¶\nThe Style element specifies the default text style for this sample description. There should be one and only one Style element in a TextSampleDescription. If not found, all default values are used with a fontID of 1.\nSyntax¶\n<Style fromChar=\"...\" toChar=\"...\" fontID=\"...\" fontSize=\"...\" color=\"...\" styles=\"...\" />\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\nfontID : specifies the ID of the font to use, as defined in the FontTable element (type: unsigned integer). Default value: 1.\nfontSize : specifies the font size to use (type: unsigned integer). Default value: 18.\ncolor : specifies the color to use for text. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"ff ff ff ff\" (opaque white).\nstyles : specifies the font styles to use (type: string). If set, it shall consist of a space-separated list of the following styles: \"Bold\" (text is in bold), \"Italic\" (text is in italic) and\"Underlined\" (text is underlined). Default value: no style (\"\").\nTextSample¶\nThe TextSample element describes a given text sample and all its associated style. Most of the time, no children elements will be specified.\nSyntax¶\n<TextSample sampleTime=\"...\" sampleDescriptionIndex=\"...\" text=\"...\" scrollDelay=\"...\" highlightColor=\"...\" wrap=\"...\" >\n...\n</TextSample>\nSemantics¶\nsampleTime : specifies the time at which the text sample shall be displayed. Time can be expressed in the usual \"hh:mm:ss.ms\" in hours, minutes, seconds and milliseconds format, or as a double-precision number in second unit. Default value is \"0\" second.\nsampleDescriptionIndex : specifies the TextSampleDescription this sample referred to. This is a 1-based index, value 0 is forbidden. Default value is \"1\", which means you do not need to specify this field most of the time if the main sample description is the first one.\ntext : the text data itself. String shall be formatted as a series of lines, each line being enclosed by single-quote characters ('). This text MUST follow XML text encoding conventions. Currently only UTF-8 text is supported.\nscrollDelay : specifies the scrollDelay in seconds (type: double-precision number. This is the delay after a scroll In and/or before scroll Out. Default value \"0\".\nhighlightColor : specifies the highlight color to be used by any enclosed highlighted strings (including karaoke). Expressed as other colors (see Styles above). Default value \"0 0 0 0\".\nwrap : specifies whether text should be wraped or not. Possible values are \"Automatic\" (text is wrapped by terminal) or \"None\" (text is not wrapped). Default value is \"None\".\nWhen text style modification or any special text effects are desired, they are described through children elements of the TextSample element. It is invalid to specify several modifiers of the same type (for instance two hyper-links) on the same character, you must make sure modifiers of a same type do not have overlapping character ranges.\nStyle¶\nSee above for semantics.\nTextBox¶\nSee above for semantics. At most one TextBox modifier shall be set per sample.\nHighlight¶\nThe Highlight modifier indicates a given substring shall be highlighted.\nSyntax¶\n<Highlight fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nBlinking¶\nThe Blinking modifier indicates a given substring shall blink. The blinking rate is up to the terminal.\nSyntax¶\n<Blinking fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nHyperLink¶\nThe Hyperlink modifier indicates that a given substring shall be treated as a hyper link.\nSyntax¶\n<Hyperlink fromChar=\"...\" toChar=\"...\" URL=\"...\" URLToolTip=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nURL : URL this HyperLink is linking to. UTF-8 only format supported.\nURLToolTip : \"tooltip\" presented to the user if supported by decoder. UTF-8 only format supported.\nKaraoke¶\nThe Karaoke element indicates dynamic highlighting, or karaoke, applies to this sample. At most one Karaoke element shall be set per TextSample. The Karaoke is defined as a sequence of highlighting times, all specified as children element of the Karaoke through the KaraokeRangeelement.\nSyntax¶\n<Karaoke startTime=\"...\">\n <KaraokeRange fromChar=\"...\" toChar=\"...\" endTime=\"...\"/>\n< /Karaoke >\nSemantics¶\nstartTime : specifies the highlighting start time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\").\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nendTime : specifies the highlighting end time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\"). The start time of a karaoke segment is the end time of the previous one, or the start time of the Karaoke element.\nEncoding of text streams¶\nUsing text streams in a 3GPP file or outisde the MPEG-4 Systems scene description is quite straightforward. All you need to do is add the text stream to your (existing) file with MP4Box. \nFor example, adding a text track and an AVI file into a new 3GP file can be done in a single call:\nMP4Box -3gp -add movie.avi -add text.ttxt dest.3gp.\nNote: By default, when importing ttxt with MP4Box, the duration of the last text sample is the same as the duration of the previous sample. To specify the duration of the last sample, add an extra text sample at the end with no text content.\nNote that you can use either SRT/SUB subtitles or TTXT files with the -add option. Other subtitles formats are currently not supported.\nUsing text streams within MPEG-4 scene description is much more tricky (is it?). You will add an ObjectDescriptor to your scene as you add an audio/video object, specifying the file name with the usual MuxInfo. Note: when using SRT/SUB, make sure you don't have TextNode specified in the MuxInfo, as this ALWAYS triggers subtitles to BIFS conversion and not 3GPP timed text.\nThe second step is controlling the new text object in the same way you control an audio or visual object. This is done through the AnimationStream node. Instead of controlling a BIFS stream, you can start/stop/play the text stream with this node.\nGPAC Implementation¶\nDuring playback, GPAC does not support dynamic highlighting (Karaoke) nor soft text wrapping (wrapping is only done at newline characters).\nGPAC should support vertical text drawing and alignment, but this has not be really tested yet.\nGPAC should support UTF-16 text in decoding and hinting, but DOES NOT SUPPORT UTF-16 text at encoding time yet.\nGPAC should support text streams placed below the main video or on its right, but cannot currently handle text streams placed above or on the left of the main video.\nSample TTXT Files¶\nThe following files are taken from a 3GPP test suite and are pure translations to TTXT with MP4Box. They should give you a good overview of the format and help you author your own test tracks.\nDownload GPAC TTXT sample streams .zip .tgz\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/TTXT-Format-Documentation/"},{"date_scraped_timestamp":1720188002521,"host":"wiki.gpac.io","page_title":"TTXT Format - GPAC wiki","text":"\n \n \n \n \n \n \nForeword¶\nThe 3GPP consortium has defined a standard for text streaming, independent of any scene description such as SMIL, SVG or BIFS: 3GPP Timed Text. MP4Box supports this standard and uses its own textual format to describe a streaming text session.\nAs of version 0.2.3, GPAC supports 3GPP timed text, also known as MPEG-4 Streaming Text. These standards provide a way to stream text and styles applying to text such as font size, color, highlighting. This is a very convenient way to encode subtitles, tickers, etc,... independently of the scene description language (MPEG-4, X3D, SVG/SMIL, ...).\nFor regular MPEG-4/VRML static text, please refer to the MPEG-4 tutorial or any VRML tutorials available on the internet.\nOverview of a timed text stream¶\nThe 3GPP timed text specification is called 3GPP TS 26.245 (3GPP web site) and will give you in-depth knowledge of this format. If you don't want to read and understand all the technical details, there are some important things to know about timed text streams, so let's review them.\nA text stream can be divided in two major sections, very much like most modern audio/video codecs: the configuration data for the decoder and the data samples. As you can guess, this is quite different from most subtitling formats, where subtitles configuration is left up to the player (color, font size, positioning, etc..). \nAuthoring 3GPP text streams therefore requires careful design as to which font size shall be picked, color used and similar. The terminal may not changed these settings as these have been chosen by the author, in other words the text stream becomes an element of the final content just like video or audio tracks (think of it as DVD subtitles whose styles and position are part of the DVD and cannot be changed by the DVD player).\nDecoder Configuration¶\nThe configuration data of a text stream provides information about positioning in the visual display, the list of ALL fonts used in the text stream and default styles (color, bold/italic, background color).\nPositioning¶\nJust like any video or image media, a text stream has size information, usually called Text Track size. This size defines the maximum width and height used by the text stream. All text will be drawn within this defined rectangle which will also act as a clipper (i.e., nothing will appear outside it). \nThis rectangle can be positioned in the final display through horizontal and vertical offsets, allowing for instance to define a text track of 320*50 pixels at the bottom of a 320*240 video.\nDefault Styles¶\nSamples in a text stream usually use the default properties defined in the decoder configuration. Since some styles may be used quite often in a text stream (for example, italic), the decoder configuration data contains a list of default styles, and samples then refer to a specific style in this list. This is known as the TextSampleEntry in 3GPP. \nA single TextSampleEntry contains:\nA list of font names (\"Times\", \"Arial\", etc) with an associated ID - fonts are ALWAYS referenced by IDs in a 3GPP text stream. A compliant decoder shall be able to understand the default font names \"Serif\", \"Sans-serif\" and \"Monospace\".\nText Styles: default font ID, font size, font styles (italic, bold, underlined), text color in RGBA colorspace.\nText Position: in a 3GPP text stream, the text may be drawn in a given rectangle within the stream main display rectangle. This is known as the TextBox in 3GPP. Any text justification is always performed against this TextBox, not against the main display rectangle.\nOther styles: background color in RGBA colorspace, horizontal and vertical justification, text scrolling modes, etc..\nText Samples¶\nText samples convey text data and, optionally, temporary styles to apply to this text. The text data can be encoded in UTF-8 or UTF-16, and may be empty (typically used for subtitles). The temporary styles are known as TextModifiers in 3GPP, and are of two kinds: those impacting the entire text, and those impacting only a sub-string of the text, identified through starting and ending characters.\nSub-String Text Modifiers¶\nStyles: overrides text style (font name, size and style, text color) for a substring of the text. All text characters not covered by temporary modifiers use the default text style from the decoder configuration.\nHighlight: specifies a given substring shall be highlighted.\nHyper Text: specifies a given substring shall be treated as an hyper-link, and gives associated URL.\nBlinking: specifies a given substring shall blink - blinking frequency is left up to the decoder.\nKaraoke: specifies how highlighting shall be dynamically applied to text characters for a karaoke style.\nComplete String Text Modifiers¶\nText Box: specifies a temporary box where this text sample is displayed.\nHighlight Color: specifies the highlight color when highlighting is used. When no highlighting color is given the decoder should use reverse video.\nWrap: specifies text wrapping shall be done on the text string if needed.\nScroll Delay: specifies the amount of time a text shall be presented statically to the user during a scroll.\nGPAC TTXT Format¶\nThere is no official textual representation of a text stream. Moreover, the specification relies on IsoMedia knowledge for most structure descriptions. \nIn order to help authoring text streams, an XML format has been developed in GPAC, called TTXT for timed-text - the extension used being.ttxt. \nThis format is not related in any way to any scene description language to keep the timed text authoring a standalone step in the authoring process.\nObtaining a sample TTXT file¶\nAs said above, this format has been developed in GPAC, and you will likely not find any tool other than MP4Box supporting this format for quite some time. To get a sample file, you have two possibilities:\nFind a 3GP file with a text track, and run MP4Box -ttxt trackID file.3gp. This will dump the text track in TTXT format.\nFind an SRT or SUB subtitle file, and run MP4Box -ttxt file.srt. This will convert the subtitles in TTXT format.\nSyntax of a TTXT file¶\nThe TTXT format is an XML description of the timed text stream, and as such MUST begin with the usual XML header with encoding hints - only UTF-8 has currently been tested. \nThe text stream is encapsulated in a single element at the root of the document, called TextStream. This element has a single defined attribute version identifying the format version - the current and only defined version is \"1.0\". \nThe TextStream element must have one and only one TextStreamHeader child and has as many TextSample children as desired. \nNote: All coordinates are specified in pixels, framebuffer-coordinate like:\nX-axis increases from left to right, with origin (0) on left edge.\nY-axis increases from top to bottom, with origin (0) on top edge.\nThe TextStreamHeader describes all 3GPP text stream decoder configuration. It must contain at least one TextSampleDescription element.\nSyntax¶\n<TextStreamHeader width=\"...\" height=\"...\" translation_x=\"...\" translation_y=\"...\" layer=\"...\" />\n ...\n</TextStreamHeader>\nSemantics¶\nwidth : defines text stream width in pixels (type: unsigned integer). Default value is 400 pixels.\nheight : defines text stream height in pixels (type: unsigned integer). Default value is 80 pixels.\ntranslation_x : defines text stream horizontal translation in main display in pixels (type: signed integer). Default value is 0 pixels.\ntranslation_y : defines text stream vertical translation in main display in pixels (type: signed integer). Default value is 0 pixels.\nlayer : defines text stream z-order (type: signed short). This is only needed when composing several text streams in a single presentation: more negative layer values are towards the viewer. Default value is 0.\nTextSampleDescription¶\nThe TextSampleDescription element may be present as many times as desired in a TextStreamHeader. It defines the default styles text samples may refer to in the stream. The TextSampleDescription may also contain zero or one FontTable, TextBox and Style.\nSyntax¶\n<TextSampleDescription horizontalJustification=\"...\" verticalJustification=\"...\" backColor=\"...\" verticalText=\"...\" fillTextRegion=\"...\" continuousKaraoke=\"...\" scroll=\"...\" scrollMode=\"...\" >\n ...\n</TextSampleDescription>\nSemantics¶\nhorizontalJustification : specifies horizontal text justification in the TextBox. Possible values are \"center\", \"left\" and \"right\". Default value is \"left\".\nverticalJustification : specifies vertical text justification in the TextBox. Possible values are\"center\", \"top\" and \"bottom\". Default value is \"bottom\".\nbackColor : specifies the color to use for background fill. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"00 00 00 00\" (no back color).\nverticalText : specifies whether the text shall be drawn vertically or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nfillTextRegion : specifies whether the entire text stream rectangle shall be filled with the backColor, or only the TextBox. Possible values are \"yes\" and \"no\". Default value is \"no\".\ncontinuousKaraoke : specifies whether karaoke is continuous (all characters from beginning of text samples are highlighted) or not. Possible values are \"yes\" and \"no\". Default value is \"no\".\nscroll : specifies text scrolling mode. Possible values are \"In\" (text is scrolling in), \"Out\"(text is scrolling out), \"InOut\" (text is scrolling in then out) and \"None\" (text is not being scrolled). Default value is \"None\".\nscrollMode : specifies text scrolling mode. Possible values are \"Credits\" (scroll from bottom to top), \"Marquee\" (scroll from right to left), \"Down\" (scroll from top to bottom) and\"Right\" (scroll from left to right). Default value is \"Credits\".\nFontTable¶\nThe FontTable element specifies all fonts used by samples referring to this stream description. There should be one and only one FontTable element in a TextSampleDescription. If not found, the default \"Serif\" font will be used with an ID of 1.\nSyntax¶\n<FontTable>\n <FontTableEntry fontID=\"...\" fontName=\"...\" />\n</FontTable>\nSemantics¶\nThe FontTable has no attribute, it is a collection of FontTableEntry elements. The FontTableEntry element has two attributes:\nfontName : specifies the font name to use. A terminal shall understand the names Serif,Sans-Serif and Monospace.\nfontID : specifies the associated ID.\nNOTE : There are no default values for this element, omitting values will result in undefined stream behaviour.\nTextBox¶\nThe TextBox element specifies where the text should be drawn within the stream main display rectangle. There should be one TextBox in a TextSampleDescription element. If the text box is not found or empty, the entire display rectangle will be used for the default text box.\nSyntax¶\n<TextBox top=\"...\" left=\"...\" bottom=\"...\" right=\"...\" />\nSemantics¶\ntop : specifies vertical offset from stream main display rectangle top edge (type: signed integer). Default value: 0 pixels.\nleft : specifies horizontal offset from stream main display rectangle left edge (type: signed integer). Default value: 0 pixels.\nbottom : specifies vertical extend of the text box (type: signed integer). Default value: 0 pixels.\nright : specifies horizontal extend of the text box (type: signed integer). Default value: 0 pixels.\nStyle¶\nThe Style element specifies the default text style for this sample description. There should be one and only one Style element in a TextSampleDescription. If not found, all default values are used with a fontID of 1.\nSyntax¶\n<Style fromChar=\"...\" toChar=\"...\" fontID=\"...\" fontSize=\"...\" color=\"...\" styles=\"...\" />\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0. Note: This field MUST be set to 0 when used in TextSampleDescription.\nfontID : specifies the ID of the font to use, as defined in the FontTable element (type: unsigned integer). Default value: 1.\nfontSize : specifies the font size to use (type: unsigned integer). Default value: 18.\ncolor : specifies the color to use for text. Expressed as space-separated, hexadecimal R, G, B and A values - for example, semi-transparent red is \"ff 00 00 7f\". Default value is \"ff ff ff ff\" (opaque white).\nstyles : specifies the font styles to use (type: string). If set, it shall consist of a space-separated list of the following styles: \"Bold\" (text is in bold), \"Italic\" (text is in italic) and\"Underlined\" (text is underlined). Default value: no style (\"\").\nTextSample¶\nThe TextSample element describes a given text sample and all its associated style. Most of the time, no children elements will be specified.\nSyntax¶\n<TextSample sampleTime=\"...\" sampleDescriptionIndex=\"...\" text=\"...\" scrollDelay=\"...\" highlightColor=\"...\" wrap=\"...\" >\n...\n</TextSample>\nSemantics¶\nsampleTime : specifies the time at which the text sample shall be displayed. Time can be expressed in the usual \"hh:mm:ss.ms\" in hours, minutes, seconds and milliseconds format, or as a double-precision number in second unit. Default value is \"0\" second.\nsampleDescriptionIndex : specifies the TextSampleDescription this sample referred to. This is a 1-based index, value 0 is forbidden. Default value is \"1\", which means you do not need to specify this field most of the time if the main sample description is the first one.\ntext : the text data itself. String shall be formatted as a series of lines, each line being enclosed by single-quote characters ('). This text MUST follow XML text encoding conventions. Currently only UTF-8 text is supported.\nscrollDelay : specifies the scrollDelay in seconds (type: double-precision number. This is the delay after a scroll In and/or before scroll Out. Default value \"0\".\nhighlightColor : specifies the highlight color to be used by any enclosed highlighted strings (including karaoke). Expressed as other colors (see Styles above). Default value \"0 0 0 0\".\nwrap : specifies whether text should be wraped or not. Possible values are \"Automatic\" (text is wrapped by terminal) or \"None\" (text is not wrapped). Default value is \"None\".\nWhen text style modification or any special text effects are desired, they are described through children elements of the TextSample element. It is invalid to specify several modifiers of the same type (for instance two hyper-links) on the same character, you must make sure modifiers of a same type do not have overlapping character ranges.\nStyle¶\nSee above for semantics.\nTextBox¶\nSee above for semantics. At most one TextBox modifier shall be set per sample.\nHighlight¶\nThe Highlight modifier indicates a given substring shall be highlighted.\nSyntax¶\n<Highlight fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nBlinking¶\nThe Blinking modifier indicates a given substring shall blink. The blinking rate is up to the terminal.\nSyntax¶\n<Blinking fromChar=\"...\" toChar=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nHyperLink¶\nThe Hyperlink modifier indicates that a given substring shall be treated as a hyper link.\nSyntax¶\n<Hyperlink fromChar=\"...\" toChar=\"...\" URL=\"...\" URLToolTip=\"...\"/>\nSemantics¶\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nURL : URL this HyperLink is linking to. UTF-8 only format supported.\nURLToolTip : \"tooltip\" presented to the user if supported by decoder. UTF-8 only format supported.\nKaraoke¶\nThe Karaoke element indicates dynamic highlighting, or karaoke, applies to this sample. At most one Karaoke element shall be set per TextSample. The Karaoke is defined as a sequence of highlighting times, all specified as children element of the Karaoke through the KaraokeRangeelement.\nSyntax¶\n<Karaoke startTime=\"...\">\n <KaraokeRange fromChar=\"...\" toChar=\"...\" endTime=\"...\"/>\n< /Karaoke >\nSemantics¶\nstartTime : specifies the highlighting start time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\").\nfromChar : specifies the first character (0-based index) in the string this style applies to (type: unsigned integer). Default value is 0.\ntoChar : specifies the first character (0-based index) in the string this style stops applying to (type: unsigned integer). Default value is 0.\nendTime : specifies the highlighting end time offset in seconds from the beginning of the sample (type: double-precision number, default value \"0\"). The start time of a karaoke segment is the end time of the previous one, or the start time of the Karaoke element.\nEncoding of text streams¶\nUsing text streams in a 3GPP file or outisde the MPEG-4 Systems scene description is quite straightforward. All you need to do is add the text stream to your (existing) file with MP4Box. \nFor example, adding a text track and an AVI file into a new 3GP file can be done in a single call:\nMP4Box -3gp -add movie.avi -add text.ttxt dest.3gp.\nNote: By default, when importing ttxt with MP4Box, the duration of the last text sample is the same as the duration of the previous sample. To specify the duration of the last sample, add an extra text sample at the end with no text content.\nNote that you can use either SRT/SUB subtitles or TTXT files with the -add option. Other subtitles formats are currently not supported.\nUsing text streams within MPEG-4 scene description is much more tricky (is it?). You will add an ObjectDescriptor to your scene as you add an audio/video object, specifying the file name with the usual MuxInfo. Note: when using SRT/SUB, make sure you don't have TextNode specified in the MuxInfo, as this ALWAYS triggers subtitles to BIFS conversion and not 3GPP timed text.\nThe second step is controlling the new text object in the same way you control an audio or visual object. This is done through the AnimationStream node. Instead of controlling a BIFS stream, you can start/stop/play the text stream with this node.\nGPAC Implementation¶\nDuring playback, GPAC does not support dynamic highlighting (Karaoke) nor soft text wrapping (wrapping is only done at newline characters).\nGPAC should support vertical text drawing and alignment, but this has not be really tested yet.\nGPAC should support UTF-16 text in decoding and hinting, but DOES NOT SUPPORT UTF-16 text at encoding time yet.\nGPAC should support text streams placed below the main video or on its right, but cannot currently handle text streams placed above or on the left of the main video.\nSample TTXT Files¶\nThe following files are taken from a 3GPP test suite and are pure translations to TTXT with MP4Box. They should give you a good overview of the format and help you author your own test tracks.\nDownload GPAC TTXT sample streams .zip .tgz\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/TTXT-Format-Documentation/"},{"date_scraped_timestamp":1720187979919,"host":"wiki.gpac.io","page_title":"Upgrading or Changing Branch - GPAC wiki","text":"\n \n \n \n \n \n \nIn source tree building¶\nIf you build GPAC directly in the source tree (i.e., running ./configure && make in the same directory as the configure script), the following steps must be done when upgrading your code base to a new version of GPAC, or when switching branches:\nuninstall any previous version of GPAC (optional, the build system as of 1.0 is independent from the presence of any other version of libgpac headers on the system)\nmake uninstall\nclean all dependencies and obj files - this will remove any local build files (dep, obj) and configuration file (config.mak, config.log ...) \nmake distclean\nreconfigure \n./configure\nbuild\nmake -j\nOut of source tree building¶\nTo avoid the issue of cleaning dependencies, it is safer to have one dedicated build directory for each branch you test:\nmkdir bin/master && cd bin/master && ../../configure && make -j\nmkdir bin/somebranch && cd bin/master && git checkout somebranch && ../../configure && make -j\nBy doing so, you don't need to cleanup or reconfigure when changing branches:\ncd bin/master && git checkout master && git pull && make -j\ncd bin/somebranch && git checkout somebranch && git pull && make -j\nYou may however need to re-run the configure script in case the build system was modified after a git pull, but this is not very frequent (and usually the build will fail in that case).\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/Upgrading/"},{"date_scraped_timestamp":1720188109279,"host":"wiki.gpac.io","page_title":"Windows - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWindows users often ask about how to compile GPAC and generate an installer. The main problem on platforms, like Windows, that don't rely on a modern package system is to get your dependencies with the correct version. Fortunately, we release a package containing most of the extra-libs we use.\nThe first reaction of unix users when compiling GPAC is to use an existing gnu/gcc compilation chain (Cygwin, MinGW...). However another free solution exists using Microsoft Visual Studio Express Edition. This article gives you some strong clues about the tools you need to recompile binaries and generate an installer using the latter.\nThis article was made using a clean install within a virtualized Windows XP. It focuses on command-lines but converting and compiling projects from the GUI works.\nGetting the tools¶\nDownload Visual C++ Express on the web (2010 has been used for this article).\nEnsure you have access to msbuild.exe which is provided with the .NET framework 3.5 (it should be installed on any up-to-date Windows): %WINDIR%\\Microsoft.NET\\Framework\\v3.5\\MSBuild.exe\nFrom now, I'll assume it is in your path and therefore refer it as MSBuild.exe\nYou'll need to compile assembly code using NASM. Add the binaries' directory to your PATH.\nSDL (alternative audio/video output for the player) and Gecko (to have the GPAC player as a Firefox plugin) installs need wget.exe to download resources from web servers.\nSDL install needs to unzip a file. Therefore put unzip.exe within your PATH to have SDL support.\nOpenHEVC support requires that you have git.exe in your PATH.\nIf you have MFC/AFX support (i.e. Visual C++ with a non-free edition) and wish to have native audio/video outputs using DirectX on your player, you shall install the DirectX SDK from the Microsoft website.\nRem: some users told us June 2010 DirectX SDK got rid of some needed components. February 2010 SDK should work.\nNote: if you don't have the MFC/AFX headers, GPAC still provides you with SDL as an audio/video output.\nGetting the source code (or organizing it)¶\nMake sure you have git and subversion installed on your system, and add the subversion binaries to your path.\nCreate a directory that I'll call \"root directory\" in this article. All the commands are executed from your root directory. Open a prompt and then type the following command:\n> git clone https://github.com/gpac/gpac.git\nDownload the file at https://download.tsi.telecom-paristech.fr/gpac/gpac_extra_libs.zip.\nUnzip gpac_extra_libs.zip so that you have a gpac_extra_libs directory containing the files.\nThe GPAC extra_libs package is provided with a Visual Studio 2005 solution, we need to upgrade it (note: if you use the graphical interface, Visual C++ will try to convert it automatically) :\nIf you have a commercial edition of Visual C++:\nFind devenv within your Visual C++ install directory (should be %VS90COMNTOOLS%\\\\..\\\\IDE\\\\devenv.exe)\n> msbuild.exe gpac_extra_libs/build/msvc/BuildAll_vc10.sln /t:Build /p:Configuration=Release /p:Platform=Win32\nFor recent versions of Visual Studio (2015+), add /p:PlatformToolset=v140 to the previous command.\nIf you have a free edition of Visual C++:\nFind vcbuild.exe within your Visual C++ install directory (should be %PROGRAMFILES%\\Microsoft Visual Studio 10.0\\VC\\vcpackages\\vcbuild.exe)\n> vcbuild.exe gpac_extra_libs\\build\\msvc\\BuildAll_vc10.sln\n /msbuild:Configuration=Release\n /msbuild:Platform=Win32\nVCbuild actually compiles the solution as it converts it. Sometimes VCbuild tries to build configurations that will fail (\"RELEASE|SMARTPHONE 2003 (ARMV4)\" for instance) : ignore them.\nCheck the compilation has succeeded by listing the directory containing the binaries:\n> dir gpac_extra_libs\\lib\\win32\\release\nYou're expected to see .lib and .dll files relative to the extra libs you're interested in (compulsory zlib, then optional js, png, jpeg, faad, mad, freetype, etc.).\nCopy the binaries to the right GPAC directories. Execute:\n> gpac_extra_libs\\CopyLibs2Public.bat x86\nCompiling GPAC¶\n> gpac_public\\version.bat\n> msbuild.exe gpac_public/build/msvc10/gpac.sln /t:Build /p:Configuration=Release /p:Platform=Win32\nIf you get a pre-generation error, please check subversion binaries (and especially svnversion) are in your PATH.\nAll projects won't compile on your desktop using this configuration. For instance Platinum (UPnP) support is not covered by this article.\nIf you want to have some audio/video support, make sure you installed the optional packages at the \"Getting the tools\" step.\nMaking an installer¶\nGPAC uses NSIS to generate an installer. The NSIS script is located at bin\\Win32\\release\\nsis_install\\gpac_installer.nsi.\nThe current script won't succeed unless you have successfully compiled everything.\nTherefore you need to modify the script (remove lines relative to the missing binaries). Depending on what you have done, you have two possibilities:\nlaunch the script from the bin\\Win32\\release\\nsis_install directory to generate an unversionned installer;\nif you have no local modification on the repository, launch the batch from the GPAC root directory (gpac_public\\generate_installer.bat) to generate a clean versionned installer of GPAC.\n \n \n \n \n ","url":"https://wiki.gpac.io/Build/old/Command-line-GPAC-compiling-on-Windows-x86-using-free-Microsoft-Visual-C%2B%2B/"},{"date_scraped_timestamp":1720187929593,"host":"wiki.gpac.io","page_title":"Windows - GPAC wiki","text":"\n \n \n \n \n \nTo build on Windows, you'll need: \n * Git\n * Visual Studio (at least VS2015 is recommended)\nFor a full build, other tools might be required, they will be mentioned at the time. \nNOTE FOR Windows XP Users¶\nWindows XP is no longer supported (by GPAC nor Microsoft) in our regular build system. See this discussion for further details.\n# Build MP4Box only¶\nTo build only the MP4Box command line utility, you will need to:\nGet the source\ngit clone https://github.com/gpac/gpac.git gpac_public\nOpen the \"Developer Command Prompt for VS 2015\" (or equivalent)\nIt might be called something else depending on your version of Visual Studio. The goal is to be able to run the msbuild.exe command.\nRun \n> cd gpac_public\\build\\msvc14\n> msbuild.exe gpac.sln /maxcpucount /t:Rebuild /p:Configuration=\"Release - MP4Box_only\" /p:Platform=x64 /p:PlatformToolset=v140 /p:WindowsTargetPlatformVersion=8.1\nYou can adjust the parameters: \n - /p:Platform=x64: Change to /p:Platform=Win32 to get a 32 bits build\n - /p:PlatformToolset=v140 and /p:WindowsTargetPlatformVersion=8.1: Change this depending on your Visual Studio version and the Windows SDK version you have installed. (e.g.: /p:PlatformToolset=v143 /p:WindowsTargetPlatformVersion=10.0 for VS 2022 on Windows 10)\nYou can find out what versions you have by opening the gpac.sln solution in Visual Studio, opening the property page of one the project, and checking the \"Platform Toolset\" and \"Windows SDK Version\" fields. \nAlternatively, you can build with the Visual Studio GUI. If needed, use the \"Retarget Solution\" function (by right-clicking on the solution in the Solution Explorer) to adjust the Windows SDK version. \nThe binary MP4Box.exe will be bin\\x64\\Release - MP4Box_only\nYou can add this directory to your PATH environment variable. Or move the binary to a destination in your PATH. Or just use it locally. \n# Full GPAC build¶\nTo get a full build of the whole GPAC project, you will first need to build some dependencies, copy them over to the main gpac repository, and build it. \nLet's take it step by step. To keep things clear, let's call your main working directory <GPAC_ROOT_DIR>\nGet the code¶\n## get gpac source\n<GPAC_ROOT_DIR> > git clone https://github.com/gpac/gpac.git gpac_public\n## get dependencies\n<GPAC_ROOT_DIR> > git clone https://github.com/gpac/deps_windows\n<GPAC_ROOT_DIR> > cd deps_windows\n<GPAC_ROOT_DIR>\\deps_windows > git submodule update --init --recursive --force --checkout\nNaming the main gpac source repository gpac_public is important because some subsequent scripts will have this name hard-coded. (If you don't want to do that, you can find instances of \"gpac_public\" in the deps_windows repository and replace them with whatever your set up is.)\nBuilding dependencies¶\nIf you plan on compiling the xvid dependency, make sure you have NASM installed.\nOpen the \"Developer Command Prompt for VS 2015\" (or equivalent in your install). \nRun:\n<GPAC_ROOT_DIR>\\deps_windows > cd build\\msvc\n<GPAC_ROOT_DIR>\\deps_windows\\build\\msvc > msbuild.exe BuildAll_vc10.sln /maxcpucount /t:Build /p:Configuration=Release /p:Platform=x64 /p:PlatformToolset=v140 /p:WindowsTargetPlatformVersion=8.1\nSee the remarks above to adjust the parameters.\nCopying dependencies¶\nSimply run: \n<GPAC_ROOT_DIR>\\deps_windows > CopyLibs2Public.bat all\nBuilding GPAC¶\nNow that all dependencies have been set up, we can be build GPAC proper by simply running, inside a Developer Command Prompt:\n<GPAC_ROOT_DIR>\\gpac_public\\build\\msvc14 > msbuild.exe gpac.sln /maxcpucount /t:Rebuild /p:Configuration=Release /p:Platform=x64 /p:PlatformToolset=v140 /p:WindowsTargetPlatformVersion=8.1\nSee the previous remarks about how to adjust the toolset options.\nOf course you can also open the gpac.sln solution in the Visual Studio GUI and build/debug from there. In Visual Studio you can use right-click on the gpac solution > \"Retarget Solution\" to adjust the SDK and toolset parameters according to your system. \nThe binaries will be in \n<GPAC_ROOT_DIR>\\gpac_public\\bin\\<PLATFORM>\\<CONFIGURATION>\n(e.g. <GPAC_ROOT_DIR>\\gpac_public\\bin\\x64\\Release)\nYou can add this to your PATH to run it from anywhere.\nPackaging GPAC (optional)¶\nIf you want to generate your own GPAC installer, you will first need to install NSIS.\nYou can then run:\n<GPAC_ROOT_DIR>\\gpac_public > generate_installer.bat x64\nIf you want to generate an installer from not committed/pushed changes, you might have to edit the script to remove this check. \nIssues¶\nThis method is how the official GPAC builds are made. It might not work on all configurations/systems as is. \nYou can report building issues in the github issue tracker for GPAC. \n \n \n \n \n ","url":"https://wiki.gpac.io/Build/build/GPAC-Build-Guide-for-Windows/"},{"date_scraped_timestamp":1720188019100,"host":"wiki.gpac.io","page_title":"Working with Network Captures - GPAC wiki","text":"\n \n \n \n \n \n \nOverview¶\nWe discuss here how to use network captures with GPAC 2.3-DEV or above.\nOverview¶\nGPAC can:\nwrite packets to a custom file format called GPC (GPAC Packet Capture), and\nread packets from pcap, pcapng and gpc files.\nGPAC uses a concept of network capture rules identified by ID. This allows having a set of filters in GPAC use one capture file while other filters will use either no capture file or another capture file.\nNetwork capture rules are defined using the -netcap option. \nCreating network captures¶\nWhen creating a network capture with GPAC, output packets are NOT sent over the network, they are only written to file. \nUse Wireshark if you need to both send and record packets.\nTo record UDP output in TS format, use:\ngpac -i source.mp4 reframer:rt=on -o udp://234.0.0.1:1234/:ext=ts -netcap=dst=cap_ts.gpc\nTo record RTP output, use:\ngpac -i source.mp4 reframer:rt=on -o session.sdp -netcap=dst=cap_rtp.gpc\nTo record ROUTE output, use:\ngpac -i source.mp4 reframer:rt=on -o route://234.1.1.1:1234/live.mpd:dynamic -netcap=dst=cap_route.gpc\nYou can also use Wireshark to capture packets.\nPlaying network captures¶\nTo replay a capture file (from the above examples), use: \ngpac -i udp://234.0.0.1:1234/ inspect:deep -netcap=src=cap_ts.gpc\ngpac -i session.sdp inspect:deep -netcap=src=cap_rtp.gpc\ngpac -i route://234.1.1.1:1234/live.mpd inspect:deep -netcap=src=cap_route.gpc\nBy default, the packets from a capture file are made available to the app in real-time. You can disable this using nrt flag:\ngpac -i session.sdp inspect:deep -netcap=src=cap_rtp.gpc,nrt\nNOTE Using nrt will likely require much more memory when consuming the data in real-time (e.g. playback).\nAssigning network captures to filters¶\nIf you need one filter to read from a capture file and others to not use a capture file, you will need to set an ID to the network capture rules and tag the filters to use this ID.\nFor example, playing a route capture and exposing the resulting files to a real HTTP server filter:\ngpac -i route://234.1.1.1:1234/live.mpd:gcache=false:NCID=RTE -netcap=id=RTE,src=route.gpc httpout:port=8080:rdirs=./dash\nStress-testing¶\nYou can also add rules for packet dropping or modifications, in order to simulate errors in the transmission chain or to test GPAC demux reliability.\nThis can be done using a capture file or using the device network interface(s). The assignment to filters using NCID also works when not using a capture file:\ngpac -i session.sdp inspect:deep -netcap=src=cap_rtp.gpc,nrt[s=10]\nThis will drop the 10th UDP packet of the capture file, read in non-realtime mode:\ngpac -i session.sdp inspect:deep -netcap=[f=100]\nThis will drop the first packet every 100 packets coming from the network interface.\n \n \n \n \n ","url":"https://wiki.gpac.io/Howtos/network-capture/"},{"date_scraped_timestamp":1720188054077,"host":"wiki.gpac.io","page_title":"XML Binary Format - GPAC wiki","text":"\n \n \n \n \n \n \nIt is possible to describe bit sequences when importing XML data. This applies to:\nNHML: some elements in the format may or must have child bitstream constructors\nEncryption: a DRMInfo element may have child bitstream constructors\nData loading in filter properties using bxml@ syntax\nISOBMFF box patches\nBlob generation using MP4Box\nThe XML syntax used is a sequence of one or more BS elements specifying a value and a number of bits to use.\nIf a non BS element is found, its content is recursively parsed for BS elements.\nSyntax¶\n<BS  bits=\"...\" value=\"...\" mediaOffset=\"...\" mediaFile=\"...\" dataLength=\"...\" text=\"...\" fcc=\"...\"/>\nSemantics¶\nbits : number of bits used to write the value\nvalue : integer value to write\nfloat : float value to write, (32 bits)\ndouble : double value to write (64 bits)\nmediaFile or dataFile: file to get data from\nmediaOffset or dataOffset: offset in the file\ndataLength: number of bytes to copy from the file\ntext or string: writes text without trailing 0. If bits is set, first writes the size of the text string using bits bits\ntextmode: if set to yes, opens the indicate file in text mode\nfcc: writes a four character code on 32 bits\nID128: writes a 128 bit value given in hexadecimal\ndata64: writes data given encoded in base64. If bits is set, first writes the size of the data using bits bits.\ndata: writes data given in hexadecimal. If bits is set, first writes the size of the data using bits bits.\nendian: if set to little, writes integers in little endian formats (for 16 and 32 bits only), otherwise in big endian format.\nbase64: writes tag as base64 encoded date. Possible values are:\ntrue, yes: the given tag is written as base64, except if base64 context is active (see below)\nstart: starts base64 context or ignored if base64 context is already started : all following elements are written in a secondary bitstream\nend: closes base64 context or ignored if no base64 context is started: the separate bitstream content is written as base 64 encoded data\nbase64Prefix: when closing base64 context, prepend base64 string length on given number of bits. If 16, 32 or 64 bits are used, use endian to indicate endianness. Default value is 0 (no prefix).\nNote: When recursive parsing is used, a new base64 encoding context is created for each child parsed.\nNHML Example¶\nThis example was used to generate files conforming to ISO/IEC 14496-18 AMD1. It shows how the bitstream constructor is used to create a custom font sample description fntC in the stsd entry called fnt1. The duration on the last sample is used to extend the duration of the track.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<NHNTStream version=\"1.0\" timeScale=\"1000\" trackID=\"1\" mediaType=\"fdsm\" mediaSubType=\"fnt1\">\n <DecoderSpecificInfo>\n <BS id=\"size\" bits=\"32\" value=\"24\" />\n <!-- box size is 4+4+3+strlen(TriodPostnaja)-->\n <BS id=\"type\" fcc=\"fntC\" />\n <BS id=\"fontFormat\" bits=\"7\" value=\"1\" />\n <BS id=\"storeFont\" bits=\"1\" value=\"0\" />\n <BS id=\"fontName\" bits=\"8\" text=\"TriodPostnaja\" />\n <BS id=\"fontSubsetID\" bits=\"7\" value=\"1\" />\n <BS id=\"reserved\" bits=\"1\" value=\"1\" />\n </DecoderSpecificInfo>\n <NHNTSample DTS=\"0\" isRAP=\"yes\" mediaFile=\"TriodPostnaja\\_subsets/TriodPostnaja\\_CyrillicCaps.ttf\" />\n <NHNTSample DTS=\"2000\" isRAP=\"yes\" mediaFile=\"TriodPostnaja\\_subsets/TriodPostnaja\\_CyrillicSmall.ttf\" />\n <NHNTSample DTS=\"4000\" isRAP=\"yes\" mediaFile=\"TriodPostnaja\\_subsets/TriodPostnaja\\_LatinCaps.ttf\" />\n <NHNTSample DTS=\"6000\" isRAP=\"yes\" mediaFile=\"TriodPostnaja\\_subsets/TriodPostnaja\\_LatinSmall.ttf\" />\n <NHNTSample DTS=\"8000\" duration=\"4000\" isRAP=\"yes\" mediaFile=\"TriodPostnaja\\_subsets/TriodPostnaja\\_symbols+numerals.ttf\" />\n</NHNTStream>\nWhen used in an NHML sample, if a BS element describes file data (dataLength and/or mediaOffset are set) but no file is given, the source file is:\nthe mediaFile indicated at the sample level, if present\notherwise the baseMediaFile indicated at the NHML stream level, if present\notherwise the media file associated with the NHML, e.g. track.media for track.nhml\nMP4Box XML binary generation¶\nAs of revision 5601, it is possible to convert an XML file with BS syntax element to a binary file directly using MP4Box -bin source.xml . The source file can be any XML file. BS element can furthermore be located in children nodes if needed.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<SomeRoot>\n <SomeChild>\n <BS id=\"size\" bits=\"32\" value=\"100\" />\n <SomeOtherChild>\n <BS id=\"size\" bits=\"32\" value=\"20000\" endian=\"little\"/>\n </SomeOtherChild>\n </SomeChild>\n</SomeRoot>\nThis example source.xml will generate a binary file containing 2 32-bits integers (first big endian, second little endian).\nCommon Encryption binary generation¶\nSee [[Common Encryption]]\nFilter property specification using binary XML¶\ngpac -i somesource:#MyProp=bxml@blob.xml ...\nThis example loads PID(s) from somesource and assigns them a property with name MyProp of type data with the property content set to the binarized XML in blob.xml.\nISOBMFF blob patching using binary XML¶\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\" />\n<GPACBOXES>\n<Box path=\"moov-\">\n<BS fcc=\"GPAC\"/>\n<BS value=\"2\" bits=\"32\"/>\n<BS value=\"1\" bits=\"32\"/>\n</Box>\n</GPACBOXES>\nThis box patch inserts at the beginning of the moov box a new box of type GPAC with a payload of 8 bytes 0x0000000100000002.\nBase64 binary XML¶\n<?xml version=\"1.0\" encoding=\"UTF-8\" />\n<SomeRoot>\n <SomeChild>\n <BS bits=\"32\" value=\"100\" />\n <BS bits=\"32\" value=\"1000\" base64=\"start\"/>\n <BS bits=\"32\" value=\"10000\"/>\n <BS bits=\"32\" value=\"100000\" base64=\"end\"/>\n </SomeChild>\n</SomeRoot>\nThis will write the value 100 in a 32 bit field big-endian, followed by the base64 data resulting of the encoding of the three values 1000, 10000 and 100000 (all coded here as 32 bits big endian values).\n \n \n \n \n ","url":"https://wiki.gpac.io/xmlformats/XML-Binary/"},{"date_scraped_timestamp":1720187910677,"host":"wiki.gpac.io","page_title":"bsagg - GPAC wiki","text":"\n \n \n \n \n \n \nCompressed layered bitstream aggregator¶\nRegister name used to load filter: bsagg\nThis filter is not checked during graph resolution and needs explicit loading.\nFilters of this class can connect to each-other. \nThis filter aggregates layers and sublayers into a single output PID. \nThe filter supports AVC|H264, HEVC and VVC stream reconstruction, and is passthrough for other codec types. \nAggregation is based on temporalID value (start from 1) and layerID value (start from 0).\nFor AVC|H264, layerID is the dependency value, or quality value if svcqid is set. \nThe filter can also be used on AVC and HEVC DolbyVision dual-streams to aggregate base stream and DV RPU/EL. \nThe filter does not forward aggregator or extractor NAL units. \nOptions¶\nsvcqid (bool, default: false): use qualityID instead of dependencyID for SVC splitting \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/bsagg/"},{"date_scraped_timestamp":1720188127559,"host":"wiki.gpac.io","page_title":"bssplit - GPAC wiki","text":"\n \n \n \n \n \n \nCompressed layered bitstream splitter¶\nRegister name used to load filter: bssplit\nThis filter is not checked during graph resolution and needs explicit loading.\nFilters of this class can connect to each-other. \nThis filter splits input stream by layers and sublayers \nThe filter supports AVC|H264, HEVC and VVC stream splitting and is pass-through for other codec types. \nSplitting is based on temporalID value (start from 1) and layerID value (start from 0).\nFor AVC|H264, layerID is the dependency value, or quality value if svcqid is set. \nEach input stream is filtered according to the ltid option as follows: \nno value set: input stream is split by layerID, i.e. each layer creates an output \nall: input stream is split by layerID and temporalID, i.e. each {layerID,temporalID} creates an output \nlID: input stream is split according to layer lID value, and temporalID is ignored \n.tID: input stream is split according to temporal sub-layer tID value and layerID is ignored \nlID.tID: input stream is split according to layer lID and sub-layer tID values \nNote: A tID value of 0 in ltid is equivalent to value 1. \nMultiple values can be given in ltid, in which case each value gives the maximum {layerID,temporalID} values for the current layer.\nA few examples on an input with 2 layers each with 2 temporal sublayers: \nltid=0.2: this will split the stream in: \none stream with {lID=0,tID=1} and {lID=0,tID=2} NAL units \none stream with all other layers/substreams \nltid=0.1,1.1: this will split the stream in: \none stream with {lID=0,tID=1} NAL units \none stream with {lID=0,tID=2}, {lID=1,tID=1} NAL units \none stream with the rest {lID=0,tID=2}, {lID=1,tID=2} NAL units \nltid=0.1,0.2: this will split the stream in: \none stream with {lID=0,tID=1} NAL units \none stream with {lID=0,tID=2} NAL units \none stream with the rest {lID=1,tID=1}, {lID=1,tID=2} NAL units \nThe filter can also be used on AVC and HEVC DolbyVision streams to split base stream and DV RPU/EL. \nThe filter does not create aggregator or extractor NAL units. \nOptions¶\nltid (strl): temporal and layer ID of output streams\nsvcqid (bool, default: false): use qualityID instead of dependencyID for SVC splitting\nsig_ltid (bool, default: false): signal maximum temporal (max_temporal_id) and layer ID (max_layer_id) of output streams (mostly used for debug) \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/bssplit/"},{"date_scraped_timestamp":1720188094819,"host":"wiki.gpac.io","page_title":"compositor - GPAC wiki","text":"\n \n \n \n \n \n \nCompositor¶\nRegister name used to load filter: compositor\nThis filter may be automatically loaded during graph resolution. \nThe GPAC compositor allows mixing audio, video, text and graphics in a timed fashion.\nThe compositor operates either in media-client or filter-only mode. \nIn this mode, the compositor acts as a pseudo-sink for the video side and creates its own output window.\nThe video frames are dispatched to the output video PID in the form of frame pointers requiring later GPU read if used.\nThe audio part acts as a regular filter, potentially mixing and resampling the audio inputs to generate its output.\nUser events are directly processed by the filter in this mode. \nFilter mode¶\nIn this mode, the compositor acts as a regular filter generating frames based on the loaded scene.\nIt will generate its outputs based on the input video frames, and will process user event sent by consuming filter(s).\nIf no input video frames (e.g. pure BIFS / SVG / VRML), the filter will generate frames based on the fps, at constant or variable frame rate.\nIt will stop generating frames as soon as all input streams are done, unless extended/reduced by dur.\nIf audio streams are loaded, an audio output PID is created. \nThe default output pixel format in filter mode is: \nrgb when the filter is explicitly loaded by the application \nrgba when the filter is loaded during a link resolution \nThis can be changed by assigning the opfmt option.\nIf either opfmt specifies alpha channel or bc is not 0 but has alpha=0, background creation in default scene will be skipped. \nIn filter-only mode, the special URL gpid:// is used to locate PIDs in the scene description, in order to design scenes independently from source media.\nWhen such a PID is associated to a Background2D node in BIFS (no SVG mapping yet), the compositor operates in pass-through mode.\nIn this mode, only new input frames on the pass-through PID will generate new frames, and the scene clock matches the input packet time.\nThe output size and pixel format will be set to the input size and pixel format, unless specified otherwise in the filter options. \nIf only 2D graphics are used and display driver is not forced, 2D rasterizer will happen in the output pixel format (including YUV pixel formats).\nIn this case, in-place processing (rasterizing over the input frame data) will be used whenever allowed by input data. \nIf 3D graphics are used or display driver is forced, OpenGL will be used on offscreen surface and the output packet will be an OpenGL texture. \nSpecific URL syntaxes¶\nThe compositor accepts any URL type supported by GPAC. It also accepts the following schemes for URLs: \nviews:// : creates an auto-stereo scene of N views from views://v1::.::vN \nmosaic:// : creates a mosaic of N views from mosaic://v1::.::vN \nFor both syntaxes, vN can be any type of URL supported by GPAC.\nFor views:// syntax, the number of rendered views is set by nbviews: \nIf the URL gives less views than rendered, the views will be repeated \nIf the URL gives more views than rendered, the extra views will be ignored \nThe compositor can act as a source filter when the src option is explicitly set, independently from the operating mode:\nExample\ngpac compositor:src=source.mp4 vout\nThe compositor can act as a source filter when the source url uses one of the compositor built-in protocol schemes:\nExample\ngpac -i mosaic://URL1:URL2 vout\nOptions¶\naa (enum, default: all, updatable): set anti-aliasing mode for raster graphics; whether the setting is applied or not depends on the graphics module or graphic card \nnone: no anti-aliasing \ntext: anti-aliasing for text only \nall: complete anti-aliasing \nhlfill (uint, default: 0x0, updatable): set highlight fill color (ARGB)\nhlline (uint, default: 0xFF000000, updatable): set highlight stroke color (ARGB)\nhllinew (flt, default: 1.0, updatable): set highlight stroke width\nsz (bool, default: true, updatable): enable scalable zoom. When scalable zoom is enabled, resizing the output window will also recompute all vectorial objects. Otherwise only the final buffer is stretched\nbc (uint, default: 0, updatable): default background color to use when displaying transparent images or video with no scene composition instructions\nyuvhw (bool, default: true, updatable): enable YUV hardware for 2D blit\nblitp (bool, default: true, updatable): partial hardware blit. If not set, will force more redraw\nsoftblt (bool, default: true): enable software blit/stretch in 2D. If disabled, vector graphics rasterizer will always be used\nstress (bool, default: false, updatable): enable stress mode of compositor (rebuild all vector graphics and texture states at each frame)\nfast (bool, default: false, updatable): enable speed optimization - whether the setting is applied or not depends on the graphics module / graphic card\nbvol (enum, default: no, updatable): draw bounding volume of objects \nno: disable bounding box \nbox: draws a rectangle (2D) or box (3D) \naabb: draws axis-aligned bounding-box tree (3D) or rectangle (2D) \ntextxt (enum, default: default, updatable): specify whether text shall be drawn to a texture and then rendered or directly rendered. Using textured text can improve text rendering in 3D and also improve text-on-video like content \ndefault: use texturing for OpenGL rendering, no texture for 2D rasterizer \nnever: never uses text textures \nalways: always render text to texture before drawing \nout8b (bool, default: false, updatable): convert 10-bit video to 8 bit texture before GPU upload\ndrop (bool, default: false, updatable): drop late frame when drawing. If not set, frames are not dropped until a desynchronization of 1 second or more is observed\nsclock (bool, default: false, updatable): force synchronizing all streams on a single clock\nsgaze (bool, default: false, updatable): simulate gaze events through mouse\nckey (uint, default: 0, updatable): color key to use in windowless mode (0xFFRRGGBB). GPAC currently does not support true alpha blitting to desktop due to limitations in most windowing toolkit, it therefore uses color keying mechanism. The alpha part of the key is used for global transparency of the output, if supported\ntimeout (uint, default: 10000, updatable): timeout in ms after which a source is considered dead (0 disable timeout)\nfps (frac, default: 30/1, updatable): simulation frame rate when animation-only sources are played (ignored when video is present)\ntimescale (uint, default: 0, updatable): timescale used for output packets when no input video PID. A value of 0 means fps numerator\nautofps (bool, default: true): use video input fps for output, ignored in player mode. If no video or not set, uses fps\nvfr (bool, default: false): only emit frames when changes are detected. (always true in player mode and when filter is dynamically loaded)\ndur (dbl, default: 0, updatable): duration of generation. Mostly used when no video input is present. Negative values mean number of frames, positive values duration in second, 0 stops as soon as all streams are done\nfsize (bool, default: false, updatable): force the scene to resize to the biggest bitmap available if no size info is given in the BIFS configuration\nmode2d (enum, default: defer, updatable): specify whether immediate drawing should be used or not \nimmediate: the screen is completely redrawn at each frame (always on if pass-through mode is detected) \ndefer: object positioning is tracked from frame to frame and dirty rectangles info is collected in order to redraw the minimal amount of the screen buffer \ndebug: only renders changed areas, resetting other areas \nWhether the setting is applied or not depends on the graphics module and player mode \namc (bool, default: true): audio multichannel support; if disabled always down-mix to stereo. Useful if the multichannel output does not work properly\nasr (uint, default: 0): force output sample rate (0 for auto)\nach (uint, default: 0): force output channels (0 for auto)\nalayout (uint, default: 0): force output channel layout (0 for auto)\nafmt (afmt, default: s16, Enum: none|u8|s16|s16b|s24|s24b|s32|s32b|flt|fltb|dbl|dblb|u8p|s16p|s24p|s32p|fltp|dblp): force output channel format (0 for auto) \nasize (uint, default: 1024): audio output packet size in samples\nabuf (uint, default: 100): audio output buffer duration in ms - the audio renderer fills the output PID up to this value. A too low value will lower latency but can have real-time playback issues\navol (uint, default: 100, updatable): audio volume in percent\napan (uint, default: 50, updatable): audio pan in percent, 50 is no pan\nasync (bool, default: true, updatable): audio resynchronization; if disabled, audio data is never dropped but may get out of sync\nmax_aspeed (dbl, default: 2.0, updatable): silence audio if playback speed is greater than specified value\nmax_vspeed (dbl, default: 4.0, updatable): move to i-frame only decoding if playback speed is greater than specified value\nbuffer (uint, default: 3000, updatable): playout buffer in ms (overridden by BufferLength property of input PID)\nrbuffer (uint, default: 1000, updatable): rebuffer trigger in ms (overridden by RebufferLength property of input PID)\nmbuffer (uint, default: 3000, updatable): max buffer in ms, must be greater than playout buffer (overridden by BufferMaxOccupancy property of input PID)\nntpsync (uint, default: 0, updatable): ntp resync threshold in ms (drops frame if their NTP is more than the given threshold above local ntp), 0 disables ntp drop\nnojs (bool, default: false): disable javascript\nnoback (bool, default: false): ignore background nodes and viewport fill (useful when dumping to PNG)\nogl (enum, default: auto, updatable): specify 2D rendering mode \nauto: automatically decides between on, off and hybrid based on content \noff: disables OpenGL; 3D will not be rendered \non: uses OpenGL for all graphics; this will involve polygon tesselation and 2D graphics will not look as nice as 2D mode \nhybrid: the compositor performs software drawing of 2D graphics with no textures (better quality) and uses OpenGL for all 2D objects with textures and 3D objects \npbo (bool, default: false, updatable): enable PixelBufferObjects to push YUV textures to GPU in OpenGL Mode. This may slightly increase the performances of the playback\nnav (enum, default: none, updatable): override the default navigation mode of MPEG-4/VRML (Walk) and X3D (Examine) \nnone: disables navigation \nwalk: 3D world walk \nfly: 3D world fly (no ground detection) \npan: 2D/3D world zoom/pan \ngame: 3D world game (mouse gives walk direction) \nslide: 2D/3D world slide \nexam: 2D/3D object examine \norbit: 3D object orbit \nvr: 3D world VR (yaw/pitch/roll) \nlinegl (bool, default: false, updatable): indicate that outlining shall be done through OpenGL pen width rather than vectorial outlining\nepow2 (bool, default: true, updatable): emulate power-of-2 textures for OpenGL (old hardware). Ignored if OpenGL rectangular texture extension is enabled \nyes: video texture is not resized but emulated with padding. This usually speeds up video mapping on shapes but disables texture transformations \nno: video is resized to a power of 2 texture when mapping to a shape \npaa (bool, default: false, updatable): indicate whether polygon antialiasing should be used in full antialiasing mode. If not set, only lines and points antialiasing are used\nbcull (enum, default: on, updatable): indicate whether backface culling shall be disable or not \non: enables backface culling \noff: disables backface culling \nalpha: only enables backface culling for transparent meshes \nwire (enum, default: none, updatable): wireframe mode \nnone: objects are drawn as solid \nonly: objects are drawn as wireframe only \nsolid: objects are drawn as solid and wireframe is then drawn \nnorms (enum, default: none, updatable): normal vector drawing for debug \nnone: no normals drawn \nface: one normal per face drawn \nvertex: one normal per vertex drawn \nrext (bool, default: true, updatable): use non power of two (rectangular) texture GL extension\ncull (bool, default: true, updatable): use aabb culling: large objects are rendered in multiple calls when not fully in viewport\ndepth_gl_scale (flt, default: 100, updatable): set depth scaler\ndepth_gl_type (enum, default: none, updatable): set geometry type used to draw depth video \nnone: no geometric conversion \npoint: compute point cloud from pixel+depth \nstrip: same as point but thins point set \nnbviews (uint, default: 0, updatable): number of views to use in stereo mode\nstereo (enum, default: none, updatable): stereo output type. If your graphic card does not support OpenGL shaders, only top and side modes will be available \nnone: no stereo \nside: images are displayed side by side from left to right \ntop: images are displayed from top (laft view) to bottom (right view) \nhmd: same as side except that view aspect ratio is not changed \nana: standard color anaglyph (red for left view, green and blue for right view) is used (forces views=2) \ncols: images are interleaved by columns, left view on even columns and left view on odd columns (forces views=2) \nrows: images are interleaved by columns, left view on even rows and left view on odd rows (forces views=2) \nspv5: images are interleaved by for SpatialView 5 views display, fullscreen mode (forces views=5) \nalio8: images are interleaved by for Alioscopy 8 views displays, fullscreen mode (forces views=8) \ncustom: images are interleaved according to the shader file indicated in mvshader. The shader is exposed each view as uniform sampler2D gfViewX, where X is the view number starting from the left \nmvshader (str, updatable): file path to the custom multiview interleaving shader\nfpack (enum, default: none, updatable): default frame packing of input video \nnone: no frame packing \ntop: top bottom frame packing \nside: side by side packing \ncamlay (enum, default: offaxis, updatable): camera layout in multiview modes \nstraight: camera is moved along a straight line, no rotation \noffaxis: off-axis projection is used \nlinear: camera is moved along a straight line with rotation \ncircular: camera is moved along a circle with rotation \niod (flt, default: 6.4, updatable): inter-ocular distance (eye separation) in cm (distance between the cameras). \nrview (bool, default: false, updatable): reverse view order\ndbgpack (bool, default: false, updatable): view packed stereo video as single image (show all)\ntvtn (uint, default: 30, updatable): number of point sampling for tile visibility algorithm\ntvtt (uint, default: 8, updatable): number of points above which the tile is considered visible\ntvtd (enum, default: off, updatable): debug tiles and full coverage SRD \noff: regular draw \npartial: only displaying partial tiles, not the full sphere video \nfull: only display the full sphere video \ntvtf (bool, default: false, updatable): force all tiles to be considered visible, regardless of viewpoint\nfov (flt, default: 1.570796326794897, updatable): default field of view for VR\nvertshader (str): path to vertex shader file\nfragshader (str): path to fragment shader file\nautocal (bool, default: false, updatable): auto calibration of znear/zfar in depth rendering mode\ndispdepth (sint, default: -1, updatable): display depth, negative value uses default screen height\ndispdist (flt, default: 50, updatable): distance in cm between the camera and the zero-disparity plane. There is currently no automatic calibration of depth in GPAC\nfocdist (flt, default: 0, updatable): distance of focus point\nosize (v2di, default: 0x0, updatable): force output size. If not set, size is derived from inputs\ndpi (v2di, default: 96x96, updatable): default dpi if not indicated by video output\ndbgpvr (flt, default: 0, updatable): debug scene used by PVR addon\nplayer (enum, default: no): set compositor in player mode \nno: regular mode \nbase: player mode \ngui: player mode with GUI auto-start \nnoaudio (bool, default: false): disable audio output\nopfmt (pfmt, default: none, Enum: none|yuv420|yvu420|yuv420_10|yuv422|yuv422_10|yuv444|yuv444_10|uyvy|vyuy|yuyv|yvyu|uyvl|vyul|yuyl|yvyl|nv12|nv21|nv1l|nv2l|yuva|yuvd|yuv444a|yuv444p|v308|yuv444ap|v408|v410|v210|grey|algr|gral|rgb4|rgb5|rgb6|rgba|argb|bgra|abgr|rgb|bgr|xrgb|rgbx|xbgr|bgrx|rgbd|rgbds|uncv): pixel format to use for output. Ignored in player mode \ndrv (enum, default: auto): indicate if graphics driver should be used \nno: never loads a graphics driver, software blit is used, no 3D possible (in player mode, disables OpenGL) \nyes: always loads a graphics driver, output pixel format will be RGB (in player mode, same as auto) \nauto: decides based on the loaded content \nsrc (cstr): URL of source content\ngaze_x (sint, default: 0, updatable): horizontal gaze coordinate (0=left, width=right)\ngaze_y (sint, default: 0, updatable): vertical gaze coordinate (0=top, height=bottom)\ngazer_enabled (bool, default: false, updatable): enable gaze event dispatch\nsubtx (sint, default: 0, updatable): horizontal translation in pixels towards right for subtitles renderers\nsubty (sint, default: 0, updatable): vertical translation in pixels towards top for subtitles renderers\nsubfs (uint, default: 0, updatable): font size for subtitles renderers (0 means automatic)\nsubd (sint, default: 0, updatable): subtitle delay in milliseconds for subtitles renderers\naudd (sint, default: 0, updatable): audio delay in milliseconds\nclipframe (bool, default: false): visual output is clipped to bounding rectangle \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/compositor/?q="},{"date_scraped_timestamp":1720188082960,"host":"wiki.gpac.io","page_title":"compositor - GPAC wiki","text":"\n \n \n \n \n \n \nCompositor¶\nRegister name used to load filter: compositor\nThis filter may be automatically loaded during graph resolution. \nThe GPAC compositor allows mixing audio, video, text and graphics in a timed fashion.\nThe compositor operates either in media-client or filter-only mode. \nIn this mode, the compositor acts as a pseudo-sink for the video side and creates its own output window.\nThe video frames are dispatched to the output video PID in the form of frame pointers requiring later GPU read if used.\nThe audio part acts as a regular filter, potentially mixing and resampling the audio inputs to generate its output.\nUser events are directly processed by the filter in this mode. \nFilter mode¶\nIn this mode, the compositor acts as a regular filter generating frames based on the loaded scene.\nIt will generate its outputs based on the input video frames, and will process user event sent by consuming filter(s).\nIf no input video frames (e.g. pure BIFS / SVG / VRML), the filter will generate frames based on the fps, at constant or variable frame rate.\nIt will stop generating frames as soon as all input streams are done, unless extended/reduced by dur.\nIf audio streams are loaded, an audio output PID is created. \nThe default output pixel format in filter mode is: \nrgb when the filter is explicitly loaded by the application \nrgba when the filter is loaded during a link resolution \nThis can be changed by assigning the opfmt option.\nIf either opfmt specifies alpha channel or bc is not 0 but has alpha=0, background creation in default scene will be skipped. \nIn filter-only mode, the special URL gpid:// is used to locate PIDs in the scene description, in order to design scenes independently from source media.\nWhen such a PID is associated to a Background2D node in BIFS (no SVG mapping yet), the compositor operates in pass-through mode.\nIn this mode, only new input frames on the pass-through PID will generate new frames, and the scene clock matches the input packet time.\nThe output size and pixel format will be set to the input size and pixel format, unless specified otherwise in the filter options. \nIf only 2D graphics are used and display driver is not forced, 2D rasterizer will happen in the output pixel format (including YUV pixel formats).\nIn this case, in-place processing (rasterizing over the input frame data) will be used whenever allowed by input data. \nIf 3D graphics are used or display driver is forced, OpenGL will be used on offscreen surface and the output packet will be an OpenGL texture. \nSpecific URL syntaxes¶\nThe compositor accepts any URL type supported by GPAC. It also accepts the following schemes for URLs: \nviews:// : creates an auto-stereo scene of N views from views://v1::.::vN \nmosaic:// : creates a mosaic of N views from mosaic://v1::.::vN \nFor both syntaxes, vN can be any type of URL supported by GPAC.\nFor views:// syntax, the number of rendered views is set by nbviews: \nIf the URL gives less views than rendered, the views will be repeated \nIf the URL gives more views than rendered, the extra views will be ignored \nThe compositor can act as a source filter when the src option is explicitly set, independently from the operating mode:\nExample\ngpac compositor:src=source.mp4 vout\nThe compositor can act as a source filter when the source url uses one of the compositor built-in protocol schemes:\nExample\ngpac -i mosaic://URL1:URL2 vout\nOptions¶\naa (enum, default: all, updatable): set anti-aliasing mode for raster graphics; whether the setting is applied or not depends on the graphics module or graphic card \nnone: no anti-aliasing \ntext: anti-aliasing for text only \nall: complete anti-aliasing \nhlfill (uint, default: 0x0, updatable): set highlight fill color (ARGB)\nhlline (uint, default: 0xFF000000, updatable): set highlight stroke color (ARGB)\nhllinew (flt, default: 1.0, updatable): set highlight stroke width\nsz (bool, default: true, updatable): enable scalable zoom. When scalable zoom is enabled, resizing the output window will also recompute all vectorial objects. Otherwise only the final buffer is stretched\nbc (uint, default: 0, updatable): default background color to use when displaying transparent images or video with no scene composition instructions\nyuvhw (bool, default: true, updatable): enable YUV hardware for 2D blit\nblitp (bool, default: true, updatable): partial hardware blit. If not set, will force more redraw\nsoftblt (bool, default: true): enable software blit/stretch in 2D. If disabled, vector graphics rasterizer will always be used\nstress (bool, default: false, updatable): enable stress mode of compositor (rebuild all vector graphics and texture states at each frame)\nfast (bool, default: false, updatable): enable speed optimization - whether the setting is applied or not depends on the graphics module / graphic card\nbvol (enum, default: no, updatable): draw bounding volume of objects \nno: disable bounding box \nbox: draws a rectangle (2D) or box (3D) \naabb: draws axis-aligned bounding-box tree (3D) or rectangle (2D) \ntextxt (enum, default: default, updatable): specify whether text shall be drawn to a texture and then rendered or directly rendered. Using textured text can improve text rendering in 3D and also improve text-on-video like content \ndefault: use texturing for OpenGL rendering, no texture for 2D rasterizer \nnever: never uses text textures \nalways: always render text to texture before drawing \nout8b (bool, default: false, updatable): convert 10-bit video to 8 bit texture before GPU upload\ndrop (bool, default: false, updatable): drop late frame when drawing. If not set, frames are not dropped until a desynchronization of 1 second or more is observed\nsclock (bool, default: false, updatable): force synchronizing all streams on a single clock\nsgaze (bool, default: false, updatable): simulate gaze events through mouse\nckey (uint, default: 0, updatable): color key to use in windowless mode (0xFFRRGGBB). GPAC currently does not support true alpha blitting to desktop due to limitations in most windowing toolkit, it therefore uses color keying mechanism. The alpha part of the key is used for global transparency of the output, if supported\ntimeout (uint, default: 10000, updatable): timeout in ms after which a source is considered dead (0 disable timeout)\nfps (frac, default: 30/1, updatable): simulation frame rate when animation-only sources are played (ignored when video is present)\ntimescale (uint, default: 0, updatable): timescale used for output packets when no input video PID. A value of 0 means fps numerator\nautofps (bool, default: true): use video input fps for output, ignored in player mode. If no video or not set, uses fps\nvfr (bool, default: false): only emit frames when changes are detected. (always true in player mode and when filter is dynamically loaded)\ndur (dbl, default: 0, updatable): duration of generation. Mostly used when no video input is present. Negative values mean number of frames, positive values duration in second, 0 stops as soon as all streams are done\nfsize (bool, default: false, updatable): force the scene to resize to the biggest bitmap available if no size info is given in the BIFS configuration\nmode2d (enum, default: defer, updatable): specify whether immediate drawing should be used or not \nimmediate: the screen is completely redrawn at each frame (always on if pass-through mode is detected) \ndefer: object positioning is tracked from frame to frame and dirty rectangles info is collected in order to redraw the minimal amount of the screen buffer \ndebug: only renders changed areas, resetting other areas \nWhether the setting is applied or not depends on the graphics module and player mode \namc (bool, default: true): audio multichannel support; if disabled always down-mix to stereo. Useful if the multichannel output does not work properly\nasr (uint, default: 0): force output sample rate (0 for auto)\nach (uint, default: 0): force output channels (0 for auto)\nalayout (uint, default: 0): force output channel layout (0 for auto)\nafmt (afmt, default: s16, Enum: none|u8|s16|s16b|s24|s24b|s32|s32b|flt|fltb|dbl|dblb|u8p|s16p|s24p|s32p|fltp|dblp): force output channel format (0 for auto) \nasize (uint, default: 1024): audio output packet size in samples\nabuf (uint, default: 100): audio output buffer duration in ms - the audio renderer fills the output PID up to this value. A too low value will lower latency but can have real-time playback issues\navol (uint, default: 100, updatable): audio volume in percent\napan (uint, default: 50, updatable): audio pan in percent, 50 is no pan\nasync (bool, default: true, updatable): audio resynchronization; if disabled, audio data is never dropped but may get out of sync\nmax_aspeed (dbl, default: 2.0, updatable): silence audio if playback speed is greater than specified value\nmax_vspeed (dbl, default: 4.0, updatable): move to i-frame only decoding if playback speed is greater than specified value\nbuffer (uint, default: 3000, updatable): playout buffer in ms (overridden by BufferLength property of input PID)\nrbuffer (uint, default: 1000, updatable): rebuffer trigger in ms (overridden by RebufferLength property of input PID)\nmbuffer (uint, default: 3000, updatable): max buffer in ms, must be greater than playout buffer (overridden by BufferMaxOccupancy property of input PID)\nntpsync (uint, default: 0, updatable): ntp resync threshold in ms (drops frame if their NTP is more than the given threshold above local ntp), 0 disables ntp drop\nnojs (bool, default: false): disable javascript\nnoback (bool, default: false): ignore background nodes and viewport fill (useful when dumping to PNG)\nogl (enum, default: auto, updatable): specify 2D rendering mode \nauto: automatically decides between on, off and hybrid based on content \noff: disables OpenGL; 3D will not be rendered \non: uses OpenGL for all graphics; this will involve polygon tesselation and 2D graphics will not look as nice as 2D mode \nhybrid: the compositor performs software drawing of 2D graphics with no textures (better quality) and uses OpenGL for all 2D objects with textures and 3D objects \npbo (bool, default: false, updatable): enable PixelBufferObjects to push YUV textures to GPU in OpenGL Mode. This may slightly increase the performances of the playback\nnav (enum, default: none, updatable): override the default navigation mode of MPEG-4/VRML (Walk) and X3D (Examine) \nnone: disables navigation \nwalk: 3D world walk \nfly: 3D world fly (no ground detection) \npan: 2D/3D world zoom/pan \ngame: 3D world game (mouse gives walk direction) \nslide: 2D/3D world slide \nexam: 2D/3D object examine \norbit: 3D object orbit \nvr: 3D world VR (yaw/pitch/roll) \nlinegl (bool, default: false, updatable): indicate that outlining shall be done through OpenGL pen width rather than vectorial outlining\nepow2 (bool, default: true, updatable): emulate power-of-2 textures for OpenGL (old hardware). Ignored if OpenGL rectangular texture extension is enabled \nyes: video texture is not resized but emulated with padding. This usually speeds up video mapping on shapes but disables texture transformations \nno: video is resized to a power of 2 texture when mapping to a shape \npaa (bool, default: false, updatable): indicate whether polygon antialiasing should be used in full antialiasing mode. If not set, only lines and points antialiasing are used\nbcull (enum, default: on, updatable): indicate whether backface culling shall be disable or not \non: enables backface culling \noff: disables backface culling \nalpha: only enables backface culling for transparent meshes \nwire (enum, default: none, updatable): wireframe mode \nnone: objects are drawn as solid \nonly: objects are drawn as wireframe only \nsolid: objects are drawn as solid and wireframe is then drawn \nnorms (enum, default: none, updatable): normal vector drawing for debug \nnone: no normals drawn \nface: one normal per face drawn \nvertex: one normal per vertex drawn \nrext (bool, default: true, updatable): use non power of two (rectangular) texture GL extension\ncull (bool, default: true, updatable): use aabb culling: large objects are rendered in multiple calls when not fully in viewport\ndepth_gl_scale (flt, default: 100, updatable): set depth scaler\ndepth_gl_type (enum, default: none, updatable): set geometry type used to draw depth video \nnone: no geometric conversion \npoint: compute point cloud from pixel+depth \nstrip: same as point but thins point set \nnbviews (uint, default: 0, updatable): number of views to use in stereo mode\nstereo (enum, default: none, updatable): stereo output type. If your graphic card does not support OpenGL shaders, only top and side modes will be available \nnone: no stereo \nside: images are displayed side by side from left to right \ntop: images are displayed from top (laft view) to bottom (right view) \nhmd: same as side except that view aspect ratio is not changed \nana: standard color anaglyph (red for left view, green and blue for right view) is used (forces views=2) \ncols: images are interleaved by columns, left view on even columns and left view on odd columns (forces views=2) \nrows: images are interleaved by columns, left view on even rows and left view on odd rows (forces views=2) \nspv5: images are interleaved by for SpatialView 5 views display, fullscreen mode (forces views=5) \nalio8: images are interleaved by for Alioscopy 8 views displays, fullscreen mode (forces views=8) \ncustom: images are interleaved according to the shader file indicated in mvshader. The shader is exposed each view as uniform sampler2D gfViewX, where X is the view number starting from the left \nmvshader (str, updatable): file path to the custom multiview interleaving shader\nfpack (enum, default: none, updatable): default frame packing of input video \nnone: no frame packing \ntop: top bottom frame packing \nside: side by side packing \ncamlay (enum, default: offaxis, updatable): camera layout in multiview modes \nstraight: camera is moved along a straight line, no rotation \noffaxis: off-axis projection is used \nlinear: camera is moved along a straight line with rotation \ncircular: camera is moved along a circle with rotation \niod (flt, default: 6.4, updatable): inter-ocular distance (eye separation) in cm (distance between the cameras). \nrview (bool, default: false, updatable): reverse view order\ndbgpack (bool, default: false, updatable): view packed stereo video as single image (show all)\ntvtn (uint, default: 30, updatable): number of point sampling for tile visibility algorithm\ntvtt (uint, default: 8, updatable): number of points above which the tile is considered visible\ntvtd (enum, default: off, updatable): debug tiles and full coverage SRD \noff: regular draw \npartial: only displaying partial tiles, not the full sphere video \nfull: only display the full sphere video \ntvtf (bool, default: false, updatable): force all tiles to be considered visible, regardless of viewpoint\nfov (flt, default: 1.570796326794897, updatable): default field of view for VR\nvertshader (str): path to vertex shader file\nfragshader (str): path to fragment shader file\nautocal (bool, default: false, updatable): auto calibration of znear/zfar in depth rendering mode\ndispdepth (sint, default: -1, updatable): display depth, negative value uses default screen height\ndispdist (flt, default: 50, updatable): distance in cm between the camera and the zero-disparity plane. There is currently no automatic calibration of depth in GPAC\nfocdist (flt, default: 0, updatable): distance of focus point\nosize (v2di, default: 0x0, updatable): force output size. If not set, size is derived from inputs\ndpi (v2di, default: 96x96, updatable): default dpi if not indicated by video output\ndbgpvr (flt, default: 0, updatable): debug scene used by PVR addon\nplayer (enum, default: no): set compositor in player mode \nno: regular mode \nbase: player mode \ngui: player mode with GUI auto-start \nnoaudio (bool, default: false): disable audio output\nopfmt (pfmt, default: none, Enum: none|yuv420|yvu420|yuv420_10|yuv422|yuv422_10|yuv444|yuv444_10|uyvy|vyuy|yuyv|yvyu|uyvl|vyul|yuyl|yvyl|nv12|nv21|nv1l|nv2l|yuva|yuvd|yuv444a|yuv444p|v308|yuv444ap|v408|v410|v210|grey|algr|gral|rgb4|rgb5|rgb6|rgba|argb|bgra|abgr|rgb|bgr|xrgb|rgbx|xbgr|bgrx|rgbd|rgbds|uncv): pixel format to use for output. Ignored in player mode \ndrv (enum, default: auto): indicate if graphics driver should be used \nno: never loads a graphics driver, software blit is used, no 3D possible (in player mode, disables OpenGL) \nyes: always loads a graphics driver, output pixel format will be RGB (in player mode, same as auto) \nauto: decides based on the loaded content \nsrc (cstr): URL of source content\ngaze_x (sint, default: 0, updatable): horizontal gaze coordinate (0=left, width=right)\ngaze_y (sint, default: 0, updatable): vertical gaze coordinate (0=top, height=bottom)\ngazer_enabled (bool, default: false, updatable): enable gaze event dispatch\nsubtx (sint, default: 0, updatable): horizontal translation in pixels towards right for subtitles renderers\nsubty (sint, default: 0, updatable): vertical translation in pixels towards top for subtitles renderers\nsubfs (uint, default: 0, updatable): font size for subtitles renderers (0 means automatic)\nsubd (sint, default: 0, updatable): subtitle delay in milliseconds for subtitles renderers\naudd (sint, default: 0, updatable): audio delay in milliseconds\nclipframe (bool, default: false): visual output is clipped to bounding rectangle \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/compositor/"},{"date_scraped_timestamp":1720188098764,"host":"wiki.gpac.io","page_title":"dasher - GPAC wiki","text":"\n \n \n \n \n \n \nDASH and HLS segmenter¶\nRegister name used to load filter: dasher\nThis filter may be automatically loaded during graph resolution.\nThis filter requires the graph resolver to be activated. \nThis filter provides segmentation and manifest generation for MPEG-DASH and HLS formats.\nThe segmenter currently supports: \nMPD and m3u8 generation (potentially in parallel) \nISOBMFF, MPEG-2 TS, MKV and raw bitstream segment formats \noverride of profiles and levels in manifest for codecs \nmost MPEG-DASH profiles \nstatic and dynamic (live) manifest offering \ncontext store and reload for batch processing of live/dynamic sessions \nThe filter does perform per-segment real-time regulation using sreg.\nIf you need per-frame real-time regulation on non-real-time inputs, insert a reframer before to perform real-time regulation.\nExample\ngpac -i file.mp4 reframer:rt=on -o live.mpd:dmode=dynamic\nTemplate strings¶\nThe segmenter uses templates to derive output file names, regardless of the DASH mode (even when templates are not used). The default one is $File$_dash for ondemand and single file modes, and $File$_$Number$ for separate segment files\nExample\ntemplate=Great_$File$_$Width$_$Number$\nIf input is foo.mp4 with 640x360 video resolution, this will resolve in Great_foo_640_$Number$ for the DASH template.\nExample\ntemplate=Great_$File$_$Width$\nIf input is foo.mp4 with 640x360 video resolution, this will resolve in Great_foo_640.mp4 for onDemand case. \nStandard DASH replacement strings: \n$Number[%%0Nd]$: replaced by the segment number, possibly prefixed with 0 \n$RepresentationID$: replaced by representation name \n$Time$: replaced by segment start time \n$Bandwidth$: replaced by representation bandwidth. \nNote: these strings are not replaced in the manifest templates elements. \nAdditional replacement strings (not DASH, not generic GPAC replacements but may occur multiple times in template): \n$Init=NAME$: replaced by NAME for init segment, ignored otherwise \n$XInit=NAME$: complete replace by NAME for init segment, ignored otherwise \n$InitExt=EXT$: replaced by EXT for init segment file extensions, ignored otherwise \n$Index=NAME$: replaced by NAME for index segments, ignored otherwise \n$Path=PATH$: replaced by PATH when creating segments, ignored otherwise \n$Segment=NAME$: replaced by NAME for media segments, ignored for init segments \n$SegExt=EXT$: replaced by EXT for media segment file extensions, ignored for init segments \n$FS$ (FileSuffix): replaced by _trackN in case the input is an AV multiplex, or kept empty otherwise \nNote: these strings are replaced in the manifest templates elements. \nPID assignment and configuration¶\nTo assign PIDs into periods and adaptation sets and configure the session, the segmenter looks for the following properties on each input PID: \nRepresentation: assigns representation ID to input PID. If not set, the default behavior is to have each media component in different adaptation sets. Setting the Representation allows explicit multiplexing of the source(s) \nPeriod: assigns period ID to input PID. If not set, the default behavior is to have all media in the same period with the same start time \nPStart: assigns period start. If not set, 0 is assumed, and periods appear in the Period ID declaration order. If negative, this gives the period order (-1 first, then -2 ...). If positive, this gives the true start time and will abort DASHing at period end \nNote: When both positive and negative values are found, the by-order periods (negative) will be inserted AFTER the timed period (positive) \nASID: assigns parent adaptation set ID. If not 0, only sources with same AS ID will be in the same adaptation set \nNote: If multiple streams in source, only the first stream will have an AS ID assigned \nxlink: for remote periods, only checked for null PID \nRole, PDesc, ASDesc, ASCDesc, RDesc: various descriptors to set for period, AS or representation \nBUrl: overrides segmenter [-base] with a set of BaseURLs to use for the PID (per representation) \nTemplate: overrides segmenter template for this PID \nDashDur: overrides segmenter segment duration for this PID \nStartNumber: sets the start number for the first segment in the PID, default is 1 \nIntraOnly: indicates input PID follows HLS EXT-X-I-FRAMES-ONLY guidelines \nCropOrigin: indicates x and y coordinates of video for SRD (size is video size) \nSRD: indicates SRD position and size of video for SRD, ignored if CropOrigin is set \nSRDRef: indicates global width and height of SRD, ignored if CropOrigin is set \nHLSPL: name of variant playlist, can use templates \nHLSMExt: list of extensions to add to master playlist entries, ['foo','bar=val'] added as ,foo,bar=val \nHLSVExt: list of extensions to add to variant playlist, ['#foo','#bar=val'] added as #foo \\n #bar=val \nNon-dash properties: Bitrate, SAR, Language, Width, Height, SampleRate, NumChannels, Language, ID, DependencyID, FPS, Interlaced, Codec. These properties are used to setup each representation and can be overridden on input PIDs using the general PID property settings (cf global help). \nExample\ngpac -i test.mp4:#Bitrate=1M -o test.mpd\nThis will force declaring a bitrate of 1M for the representation, regardless of actual input bitrate.\nExample\ngpac -i muxav.mp4 -o test.mpd\nThis will create un-multiplexed DASH segments.\nExample\ngpac -i muxav.mp4:#Representation=1 -o test.mpd\nThis will create multiplexed DASH segments.\nExample\ngpac -i m1.mp4 -i m2.mp4:#Period=Yep -o test.mpd\nThis will put src m1.mp4 in first period, m2.mp4 in second period.\nExample\ngpac -i m1.mp4:#BUrl=http://foo/bar -o test.mpd\nThis will assign a baseURL to src m1.mp4.\nExample\ngpac -i m1.mp4:#ASCDesc=&lt;ElemName val=\"attval\"&gt;text&lt;/ElemName&gt; -o test.mpd\nThis will assign the specified XML descriptor to the adaptation set.\nNote: this can be used to inject most DASH descriptors not natively handled by the segmenter.\nThe segmenter handles the XML descriptor as a string and does not attempt to validate it. Descriptors, as well as some segmenter filter arguments, are string lists (comma-separated by default), so that multiple descriptors can be added:\nExample\ngpac -i m1.mp4:#RDesc=&lt;Elem attribute=\"1\"/&gt;,&lt;Elem2&gt;text&lt;/Elem2&gt; -o test.mpd\nThis will insert two descriptors in the representation(s) of m1.mp4.\nExample\ngpac -i video.mp4:#Template=foo$Number$ -i audio.mp4:#Template=bar$Number$ -o test.mpd\nThis will assign different templates to the audio and video sources.\nExample\ngpac -i null:#xlink=http://foo/bar.xml:#PDur=4 -i m.mp4:#PStart=-1 -o test.mpd\nThis will insert an create an MPD with first a remote period then a regular one.\nExample\ngpac -i null:#xlink=http://foo/bar.xml:#PStart=6 -i m.mp4 -o test.mpd\nThis will create an MPD with first a regular period, dashing only 6s of content, then a remote one.\nExample\ngpac -i v1:#SRD=0x0x1280x360:#SRDRef=1280x720 -i v2:#SRD=0x360x1280x360 -o test.mpd\nThis will layout the v2 below v1 using a global SRD size of 1280x720. \nThe segmenter will create multiplexing filter chains for each representation and will reassign PID IDs so that each media component (video, audio, ...) in an adaptation set has the same ID. \nFor HLS, the output manifest PID will deliver the master playlist and the variant playlists.\nThe default variant playlist are $NAME_$N.m3u8, where $NAME is the radical of the output file name and $N is the 1-based index of the variant. \nWhen HLS mode is enabled, the segment template is relative to the variant playlist file, which can also be templated.\nExample\ngpac -i av.mp4:#HLSPL=$Type$/index.m3u8 -o dash/live.m3u8:dual:template='$Number$'\nThis will put video segments and playlist in dash/video/ and audio segments and playlist in dash/audio/ \nSegmentation¶\nThe default behavior of the segmenter is to estimate the theoretical start time of each segment based on target segment duration, and start a new segment when a packet with SAP type 1,2,3 or 4 with time greater than the theoretical time is found.\nThis behavior can be changed to find the best SAP packet around a segment theoretical boundary using sbound: \nclosest mode: the segment will start at the closest SAP of the theoretical boundary \nin mode: the segment will start at or before the theoretical boundary \nWarning: These modes will introduce delay in the segmenter (typically buffering of one GOP) and should not be used for low-latency modes. \nThe segmenter can also be configured to: \ncompletely ignore SAP when segmenting using sap. \nignore SAP on non-video streams when segmenting using strict_sap. \nWhen seg_sync is disabled, the segmenter will by default announce a new segment in the manifest(s) as soon as its size/offset is known or its name is known, but the segment (or part in LL-HLS) may still not be completely written/sent.\nThis may result in temporary mismatches between segment/part size currently received versus size as advertized in manifest.\nWhen seg_sync is enabled, the segmenter will wait for the last byte of the fragment/segment to be pushed before announcing a new segment in the manifest(s). This can however slightly increase the latency in MPEG-DASH low-latency. \nWhen (-sflush)[] is set to single, segmentation is skipped and a single segment is generated per input. \nDynamic (real-time live) Mode¶\nThe dasher does not perform real-time regulation by default.\nFor regular segmentation, you should enable segment regulation sreg if your sources are not real-time.\nExample\ngpac -i source.mp4 -o live.mpd:segdur=2:profile=live:dmode=dynamic:sreg\nFor low latency segmentation with fMP4, you will need to specify the following options: \ncdur: set the fMP4 fragment duration \nasto: set the availability time offset for DASH. This value should be equal or slightly greater than segment duration minus cdur \nllhls: enable low latency for HLS \nNote: llhls does not force cmaf mode to allow for multiplexed media in segments but it enforces to tfdt_traf in the muxer. \nIf your sources are not real-time, insert a reframer filter with real-time regulation\nExample\ngpac -i source.mp4 reframer:rt=on -o live.mpd:segdur=2:cdur=0.2:asto=1.8:profile=live:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and indicate to the client that requests can be made 1.8 seconds earlier than segment complete availability on server.\nExample\ngpac -i source.mp4 reframer:rt=on -o live.m3u8:segdur=2:cdur=0.2:llhls=br:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and produce HLS low latency parts using byte ranges in the final segment.\nExample\ngpac -i source.mp4 reframer:rt=on -o live.m3u8:segdur=2:cdur=0.2:llhls=sf:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and produce HLS low latency parts using dedicated files. \nYou can combine LL-HLS and DASH-LL generation:\nExample\ngpac -i source.mp4 reframer:rt=on -o live.mpd:dual:segdur=2:cdur=0.2:asto=1.8:llhls=br:profile=live:dmode=dynamic\nFor DASH, the filter will use the local clock for UTC anchor points in DASH.\nThe filter can fetch and signal clock in other ways using utcs.\nExample\nThis will use the local clock and insert in the MPD a UTCTiming descriptor containing the local clock.\nExample\n[opts]::utcs=http://time.akamai.com[::opts]\nThis will fetch time from http://time.akamai.com, use it as the UTC reference for segment generation and insert in the MPD a UTCTiming descriptor containing the time server URL.\nNote: if not set as a global option using --utcs=, you must escape the url using double :: or use other separators. \nCue-driven segmentation¶\nThe segmenter can take a list of instructions, or Cues, to use for the segmentation process, in which case only these are used to derive segment boundaries. Cues can be set through XML files or injected in input packets. \nCue files can be specified for the entire segmenter, or per PID using DashCue property.\nCues are given in an XML file with a root element called <DASHCues>, with currently no attribute specified. The children are one or more <Stream> elements, with attributes: \nid: integer for stream/track/PID ID \ntimescale: integer giving the units of following timestamps \nmode: if present and value is edit, the timestamp are in presentation time (edit list applied) otherwise they are in media time \nts_offset: integer giving a value (in timescale) to subtract to the DTS/CTS values listed \nThe children of <Stream> are one or more <Cue> elements, with attributes: \nsample: integer giving the sample/frame number of a sample at which splitting shall happen \ndts: long integer giving the decoding time stamp of a sample at which splitting shall happen \ncts: long integer giving the composition / presentation time stamp of a sample at which splitting shall happen \nWarning: Cues shall be listed in decoding order. \nIf the DashCue property of a PID equals inband, the PID will be segmented according to the CueStart property of input packets.\nThis feature is typically combined with a list of files as input:\nExample\ngpac -i list.m3u:sigcues -o res/live.mpd\nThis will load the flist filter in cue mode, generating continuous timelines from the sources and injecting a CueStart property at each new file. \nIf the cues option equals none, the DashCue property of input PIDs will be ignored. \nManifest Generation only mode¶\nThe segmenter can be used to generate manifests from already fragmented ISOBMFF inputs using sigfrag.\nIn this case, segment boundaries are attached to each packet starting a segment and used to drive the segmentation.\nThis can be used with single-track ISOBMFF sources, either single file or multi file.\nFor single file source: \nif onDemand profile is requested, sources have to be formatted as a DASH self-initializing media segment with the proper sidx. \ntemplates are disabled. \nsseg is forced for all profiles except onDemand ones. \nFor multi files source: \ninput shall be a playlist containing the initial file followed by the ordered list of segments. \nif no template is provided, the full or main profile will be used \nif -template is provided, it shall be correct: the filter will not try to guess one from the input file names and will not validate it either. \nThe manifest generation-only mode supports both MPD and HLS generation. \nExample\ngpac -i ondemand_src.mp4 -o dash.mpd:sigfrag:profile=onDemand\nThis will generate a DASH manifest for onDemand Profile based on the input file.\nExample\ngpac -i ondemand_src.mp4 -o dash.m3u8:sigfrag\nThis will generate a HLS manifest based on the input file.\nExample\ngpac -i seglist.txt -o dash.mpd:sigfrag\nThis will generate a DASH manifest in Main Profile based on the input files.\nExample\ngpac -i seglist.txt:Template=$XInit=init$$q1/$Number$ -o dash.mpd:sigfrag:profile=live\nThis will generate a DASH manifest in live Profile based on the input files. The input file will contain init.mp4, q1/1.m4s, q1/2.m4s... \nCue Generation only mode¶\nThe segmenter can be used to only generate segment boundaries from a set of inputs using gencues, without generating manifests or output files.\nIn this mode, output PIDs are declared directly rather than redirected to media segment files.\nThe segmentation logic is not changed, and packets are forwarded with the same information and timing as in regular mode. \nOutput PIDs are forwarded with DashCue=inband property, so that any subsequent dasher follows the same segmentation process (see above). \nThe first packet in a segment has: \nproperty FileNumber (and, if multiple files, FileName) set as usual \nproperty CueStart set \nproperty DFPStart=0 set if this is the first packet in a period \nThis mode can be used to pre-segment the streams for later processing that must take place before final dashing.\nExample\ngpac -i source.mp4 dasher:gencues cecrypt:cfile=roll_seg.xml -o live.mpd\nThis will allow the encrypter to locate dash boundaries and roll keys at segment boundaries.\nExample\ngpac -i s1.mp4 -i s2.mp4:#CryptInfo=clear:#Period=3 -i s3.mp4:#Period=3 dasher:gencues cecrypt:cfile=roll_period.xml -o live.mpd\nIf the DRM file uses keyRoll=period, this will generate: \nfirst period crypted with one key \nsecond period clear \nthird period crypted with another key \nForced-Template mode¶\nWhen tpl_force is set, the template string is not analyzed nor modified for missing elements.\nThis is typically used to redirect segments to a given destination regardless of the dash profile.\nExample\ngpac -i SRC -o null:ext=mpd:tpl_force --template=pipe://mypipe\nThis will trash the manifest and open mypipe as destination for the muxer result. \nWarning: Options for segment destination cannot be set through the template, global options must be used. \nBatch Operations¶\nThe segmentation can be performed in multiple calls using a DASH context set with state.\nBetween calls, the PIDs are reassigned by checking that the PID ID match between the calls and: \nthe input file names match between the calls \nor the representation ID (and period ID if specified) match between the calls \nIf a PID is not matched, it will be assigned to a new period. \nThe default behaviour assume that the same inputs are used for segmentation and rebuilds a contiguous timeline at each new file start.\nIf the inputs change but form a continuous timeline, [-keep_ts])() must be used to skip timeline reconstruction. \nThe inputs will be segmented for a duration of subdur if set, otherwise the input media duration.\nWhen inputs are over, they are restarted if loop is set otherwise a new period is created.\nTo avoid this behaviour, the sflush option should be set to end or single, indicating that further sources for the same representations will be added in subsequent calls. When sflush is not off, the (-loop)[] option is ignored. \nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX && gpac -i SRC -o dash.mpd:segdur=2:state=CTX\nThis will generate all dash segments for SRC (last one possibly shorter) and create a new period at end of input.\nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX:loop && gpac -i SRC -o dash.mpd:segdur=2:state=CTX:loop\nThis will generate all dash segments for SRC and restart SRC to fill-up last segment.\nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX:sflush=end && gpac -i SRC -o dash.mpd:segdur=2:state=CTX:sflush=end\nThis will generate all dash segments for SRC without looping/closing the period at end of input. Timestamps in the second call will be rewritten to be contiguous with timestamp at end of first call.\nExample\ngpac -i SRC1 -o dash.mpd:segdur=2:state=CTX:sflush=end:keep_ts && gpac -i SRC2 -o dash.mpd:segdur=2:state=CTX:sflush=end:keep_ts\nThis will generate all dash segments for SRC1 without looping/closing the period at end of input, then for SRC2. Timestamps of the sources will not be rewritten. \nNote: The default behaviour of MP4Box -dash-ctx option is to set the (-loop)[] to true. \nOutput redirecting¶\nWhen loaded implicitly during link resolution, the dasher will only link its outputs to the target sink\nExample\ngpac -i SRC -o URL1:OPTS1 -o URL2:OPTS1\nThis will create one dasher (with options OPTS1) for the URL1 and one dasher (with options OPTS1) for URL2.\nThis allows dashing to multiple outputs with different formats, dash durations, etc. \nIt can be useful to redirect all the filter outputs to several sinks, for example to push through ROUTE and through HTTP the same segments.\nIn order to do this, the filter MUST be explicitly loaded and all options related to dash and MP4 must be set either globally or on the dasher filter.\nExample\ngpac -i SRC dasher:cmfc:segdur=2 -o URL1 -o URL2\nThis will create a single dasher whose outputs (manifests and segments) will be redirected to the given URLs.\nWhen explicitly loading the filter, the dual option will be disabled unless mname is set to the alternate output name. \nMultiplexer development considerations¶\nOutput multiplexers allowing segmented output must obey the following: \ninspect packet properties \nFileNumber: if set, indicate the start of a new DASH segment \nFileName: if set, indicate the file name. If not present, output shall be a single file. This is only set for packet carrying the FileNumber property, and only on one PID (usually the first) for multiplexed outputs \nIDXName: gives the optional index name. If not present, index shall be in the same file as dash segment. Only used for MPEG-2 TS for now \nEODS: property is set on packets with no payload and no timestamp to signal the end of a DASH segment. This is only used when stopping/resuming the segmentation process, in order to flush segments without dispatching an EOS (see subdur ) \nfor each segment done, send a downstream event on the first connected PID signaling the size of the segment and the size of its index if any \nfor multiplexers with init data, send a downstream event signaling the size of the init and the size of the global index if any \nthe following filter options are passed to multiplexers, which should declare them as arguments: \nnoinit: disables output of init segment for the multiplexer (used to handle bitstream switching with single init in DASH) \nfrag: indicates multiplexer shall use fragmented format (used for ISOBMFF mostly) \nsubs_sidx=0: indicates an SIDX shall be generated - only added if not already specified by user \nxps_inband=all|no|both: indicates AVC/HEVC/... parameter sets shall be sent inband, out of band, or both \nnofragdef: indicates fragment defaults should be set in each segment rather than in init segment \nThe segmenter adds the following properties to the output PIDs: \nDashMode: identifies VoD (single file with global index) or regular DASH mode used by segmenter \nDashDur: identifies target DASH segment duration - this can be used to estimate the SIDX size for example \nLLHLS: identifies LLHLS is used; the multiplexer must send fragment size events back to the dasher, and set LLHLSFragNum on the first packet of each fragment \nSegSync: indicates that fragments/segments must be completely flushed before sending back size events \nOptions¶\nsegdur (frac, default: 0/0): target segment duration in seconds. A value less than or equal to 0 defaults to 1.0 second\ntpl (bool, default: true): use template mode (multiple segment, template URLs)\nstl (bool, default: false): use segment timeline (ignored in on_demand mode)\ndmode (enum, default: static, updatable): dash content mode \nstatic: static content \ndynamic: live generation \ndynlast: last call for live, will turn the MPD into static \ndynauto: live generation and move to static manifest upon end of stream \nsseg (bool, default: false): single segment is used\nsfile (bool, default: false): use a single file for all segments (default in on_demand)\nalign (bool, default: true): enable segment time alignment between representations\nsap (bool, default: true): enable splitting segments at SAP boundaries\nmix_codecs (bool, default: false): enable mixing different codecs in an adaptation set\nntp (enum, default: rem): insert/override NTP clock at the beginning of each segment \nrem: removes NTP from all input packets \nyes: inserts NTP at each segment start \nkeep: leaves input packet NTP untouched \nno_sar (bool, default: false): do not check for identical sample aspect ratio for adaptation sets\nbs_switch (enum, default: def): bitstream switching mode (single init segment) \ndef: resolves to off for onDemand and inband for live \noff: disables BS switching \non: enables it if same decoder configuration is possible \ninband: moves decoder config inband if possible \nboth: inband and outband parameter sets \npps: moves PPS and APS inband, keep VPS,SPS and DCI out of band (used for VVC RPR) \nforce: enables it even if only one representation \nmulti: uses multiple stsd entries in ISOBMFF \ntemplate (str): template string to use to generate segment name\nsegext (str): file extension to use for segments\ninitext (str): file extension to use for the init segment\nmuxtype (enum, default: auto): muxtype to use for the segments \nmp4: uses ISOBMFF format \nts: uses MPEG-2 TS format \nmkv: uses Matroska format \nwebm: uses WebM format \nogg: uses OGG format \nraw: uses raw media format (disables multiplexed representations) \nauto: guess format based on extension, default to mp4 if no extension \nrawsub (bool, default: no): use raw subtitle format instead of encapsulating in container\nasto (dbl, default: 0): availabilityStartTimeOffset to use in seconds. A negative value simply increases the AST, a positive value sets the ASToffset to representations\nprofile (enum, default: auto): target DASH profile. This will set default option values to ensure conformance to the desired profile. For MPEG-2 TS, only main and live are used, others default to main \nauto: turns profile to live for dynamic and full for non-dynamic \nlive: DASH live profile, using segment template \nonDemand: MPEG-DASH live profile \nmain: MPEG-DASH main profile, using segment list \nfull: MPEG-DASH full profile \nhbbtv1.5.live: HBBTV 1.5 DASH profile \ndashavc264.live: DASH-IF live profile \ndashavc264.onDemand: DASH-IF onDemand profile \ndashif.ll: DASH IF low-latency profile (set UTC server to time.akamai.com if none set) \nprofX (str): list of profile extensions, as used by DASH-IF and DVB. The string will be colon-concatenated with the profile used. If starting with +, the profile string by default is erased and + is skipped\ncp (enum, default: set): content protection element location \nset: in adaptation set element \nrep: in representation element \nboth: in both adaptation set and representation elements \npssh (enum, default: v): storage mode for PSSH box \nf: stores in movie fragment only \nv: stores in movie only, or movie and fragments if key roll is detected \nm: stores in mpd only \nmf: stores in mpd and movie fragment \nmv: stores in mpd and movie \nn: discard pssh from mpd and segments \nbuf (sint, default: -100): min buffer duration in ms. negative value means percent of segment duration (e.g. -150 = 1.5*seg_dur)\nspd (sint, default: 0): suggested presentation delay in ms\ntimescale (sint, default: 0): set timescale for timeline and segment list/template. A value of 0 picks up the first timescale of the first stream in an adaptation set. A negative value forces using stream timescales for each timed element (multiplication of segment list/template/timelines). A positive value enforces the MPD timescale\ncheck_dur (bool, default: true): check duration of sources in period, trying to have roughly equal duration. Enforced whenever period start times are used\nskip_seg (bool, default: false): increment segment number whenever an empty segment would be produced - NOT DASH COMPLIANT\ntitle (str): MPD title\nsource (str): MPD Source\ninfo (str): MPD info url\ncprt (str): MPD copyright string\nlang (str): language of MPD Info\nlocation (strl): set MPD locations to given URL\nbase (strl): set base URLs of MPD\nrefresh (dbl, default: 0): refresh rate for dynamic manifests, in seconds (a negative value sets the MPD duration, value 0 uses dash duration)\ntsb (dbl, default: 30): time-shift buffer depth in seconds (a negative value means infinity)\nkeep_segs (bool, default: false): do not delete segments no longer in time-shift buffer\nast (str): set start date (as xs:date, e.g. YYYY-MM-DDTHH:MM:SSZ) for live mode. Default is now. !! Do not use with multiple periods, nor when DASH duration is not a multiple of GOP size !!\nstate (str): path to file used to store/reload state info when simulating live. This is stored as a valid MPD with GPAC XML extensions\nkeep_ts (bool, default: false): do not shift timestamp when reloading a context\nloop (bool, default: false): loop sources when dashing with subdur and state. If not set, a new period is created once the sources are over\nsubdur (dbl, default: 0): maximum duration of the input file to be segmented. This does not change the segment duration, segmentation stops once segments produced exceeded the duration\nsplit (bool, default: true): enable cloning samples for text/metadata/scene description streams, marking further clones as redundant\nhlsc (bool, default: false): insert clock reference in variant playlist in live HLS\ncues (str): set cue file\nstrict_cues (bool, default: false): strict mode for cues, complains if splitting is not on SAP type 1/2/3 or if unused cue is found\nstrict_sap (enum, default: off): strict mode for sap \noff: ignore SAP types for PID other than video, enforcing AdaptationSet@startsWithSAP=1 \nsig: same as off but keep AdaptationSet@startsWithSAP to the true SAP value \non: warn if any PID uses SAP 3 or 4 and switch to FULL profile \nintra: ignore SAP types greater than 3 on all media types \nsubs_sidx (sint, default: -1): number of subsegments per sidx. Negative value disables sidx. Only used to inherit sidx option of destination\ncmpd (bool, default: false): skip line feed and spaces in MPD XML for compactness\nstyp (str): indicate the 4CC to use for styp boxes when using ISOBMFF output\ndual (bool): indicate to produce both MPD and M3U files\nsigfrag (bool): use manifest generation only mode\nsbound (enum, default: out): indicate how the theoretical segment start TSS (= segment_number * duration) should be handled \nout: segment split as soon as TSS is exceeded (TSS <= segment_start) \nclosest: segment split at closest SAP to theoretical bound \nin: TSS is always in segment (TSS >= segment_start) \nreschedule (bool, default: false): reschedule sources with no period ID assigned once done (dynamic mode only)\nsreg (bool, default: false): regulate the session \nwhen using subdur and context, only generate segments from the past up to live edge \notherwise in dynamic mode without context, do not generate segments ahead of time \nscope_deps (bool, default: true): scope PID dependencies to be within source. If disabled, PID dependencies will be checked across all input PIDs regardless of their sources\nutcs (str): URL to use as time server / UTCTiming source. Special value inband enables inband UTC (same as publishTime), special prefix xsd@ uses xsDateTime schemeURI rather than ISO\nsflush (enum, default: off): segment flush mode - see filter help: \noff: no specific actions \nsingle: force generating a single segment for each input \nend: skip loop detection and clamp duration adjustment at end of input, used for state mode \nlast_seg_merge (bool, default: false): force merging last segment if less than half the target duration\nmha_compat (enum, default: no): adaptation set generation mode for compatible MPEG-H Audio profile \nno: only generate the adaptation set for the main profile \ncomp: only generate the adaptation sets for all compatible profiles \nall: generate the adaptation set for the main profile and all compatible profiles \nmname (str): output manifest name for ATSC3 multiplexing (using 'm3u8' only toggles HLS generation)\nllhls (enum, default: off): HLS low latency type \noff: do not use LL-HLS \nbr: use LL-HLS with byte-range for segment parts, pointing to full segment (DASH-LL compatible) \nsf: use separate files for segment parts (post-fixed .1, .2 etc.) \nbrsf: generate two sets of manifest, one for byte-range and one for files (_IF added before extension of manifest) \nhlsdrm (str): cryp file info for HLS full segment encryption\nhlsx (strl): list of string to append to master HLS header before variants with ['#foo','#bar=val'] added as #foo \\n #bar=val\nhlsiv (bool, default: true): inject IV in variant HLS playlist` \n<a id=\"ll_preload_hint\">__ll_preload_hint__</a> (bool, default: _true_): inject preload hint for LL-HLS \n<a id=\"ll_rend_rep\">__ll_rend_rep__</a> (bool, default: _true_): inject rendition reports for LL-HLS \n<a id=\"ll_part_hb\">__ll_part_hb__</a> (dbl, default: _-1_): user-defined part hold-back for LLHLS, negative value means 3 times max part duration in session \n<a id=\"ckurl\">__ckurl__</a> (str): set the ClearKey URL common to all encrypted streams (overriden byCKUrl` pid property)\nhls_absu (enum, default: no): use absolute url in HLS generation using first URL in base \nno: do not use absolute URL \nvar: use absolute URL only in variant playlists \nmas: use absolute URL only in master playlist \nboth: use absolute URL everywhere \nhls_ap (bool, default: false): use audio as primary media instead of video when generating playlists\nseg_sync (enum, default: auto): control how waiting on last packet P of fragment/segment to be written impacts segment injection in manifest \nno: do not wait for P \nyes: wait for P \nauto: wait for P if HLS is used \ncmaf (enum, default: no): use cmaf guidelines \nno: CMAF not enforced \ncmfc: use CMAF cmfc guidelines \ncmf2: use CMAF cmf2 guidelines \npswitch (enum, default: single): period switch control mode \nsingle: change period if PID configuration changes \nforce: force period switch at each PID reconfiguration instead of absorbing PID reconfiguration (for splicing or add insertion not using periodID) \nstsd: change period if PID configuration changes unless new configuration was advertised in initial config \nchain (str): URL of next MPD for regular chaining\nchain_fbk (str): URL of fallback MPD\ngencues (bool, default: false): only insert segment boundaries and do not generate manifests\nforce_init (bool, default: false): force init segment creation in bitstream switching mode\nkeep_src (bool, default: false): keep source URLs in manifest generation mode\ngxns (bool, default: false): insert some gpac extensions in manifest (for now, only tfdt of first segment)\ndkid (enum, default: auto): control injection of default KID in MPD \noff: default KID not injected \non: default KID always injected \nauto: default KID only injected if no key roll is detected (as per DASH-IF guidelines) \ntpl_force (bool, default: false): use template string as is without trying to add extension or solve conflicts in names\nttml_agg (bool, default: false): force aggregation of TTML samples of a DASH segment into a single sample \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/dasher/?q="},{"date_scraped_timestamp":1720188087040,"host":"wiki.gpac.io","page_title":"dasher - GPAC wiki","text":"\n \n \n \n \n \n \nDASH and HLS segmenter¶\nRegister name used to load filter: dasher\nThis filter may be automatically loaded during graph resolution.\nThis filter requires the graph resolver to be activated. \nThis filter provides segmentation and manifest generation for MPEG-DASH and HLS formats.\nThe segmenter currently supports: \nMPD and m3u8 generation (potentially in parallel) \nISOBMFF, MPEG-2 TS, MKV and raw bitstream segment formats \noverride of profiles and levels in manifest for codecs \nmost MPEG-DASH profiles \nstatic and dynamic (live) manifest offering \ncontext store and reload for batch processing of live/dynamic sessions \nThe filter does perform per-segment real-time regulation using sreg.\nIf you need per-frame real-time regulation on non-real-time inputs, insert a reframer before to perform real-time regulation.\nExample\ngpac -i file.mp4 reframer:rt=on -o live.mpd:dmode=dynamic\nTemplate strings¶\nThe segmenter uses templates to derive output file names, regardless of the DASH mode (even when templates are not used). The default one is $File$_dash for ondemand and single file modes, and $File$_$Number$ for separate segment files\nExample\ntemplate=Great_$File$_$Width$_$Number$\nIf input is foo.mp4 with 640x360 video resolution, this will resolve in Great_foo_640_$Number$ for the DASH template.\nExample\ntemplate=Great_$File$_$Width$\nIf input is foo.mp4 with 640x360 video resolution, this will resolve in Great_foo_640.mp4 for onDemand case. \nStandard DASH replacement strings: \n$Number[%%0Nd]$: replaced by the segment number, possibly prefixed with 0 \n$RepresentationID$: replaced by representation name \n$Time$: replaced by segment start time \n$Bandwidth$: replaced by representation bandwidth. \nNote: these strings are not replaced in the manifest templates elements. \nAdditional replacement strings (not DASH, not generic GPAC replacements but may occur multiple times in template): \n$Init=NAME$: replaced by NAME for init segment, ignored otherwise \n$XInit=NAME$: complete replace by NAME for init segment, ignored otherwise \n$InitExt=EXT$: replaced by EXT for init segment file extensions, ignored otherwise \n$Index=NAME$: replaced by NAME for index segments, ignored otherwise \n$Path=PATH$: replaced by PATH when creating segments, ignored otherwise \n$Segment=NAME$: replaced by NAME for media segments, ignored for init segments \n$SegExt=EXT$: replaced by EXT for media segment file extensions, ignored for init segments \n$FS$ (FileSuffix): replaced by _trackN in case the input is an AV multiplex, or kept empty otherwise \nNote: these strings are replaced in the manifest templates elements. \nPID assignment and configuration¶\nTo assign PIDs into periods and adaptation sets and configure the session, the segmenter looks for the following properties on each input PID: \nRepresentation: assigns representation ID to input PID. If not set, the default behavior is to have each media component in different adaptation sets. Setting the Representation allows explicit multiplexing of the source(s) \nPeriod: assigns period ID to input PID. If not set, the default behavior is to have all media in the same period with the same start time \nPStart: assigns period start. If not set, 0 is assumed, and periods appear in the Period ID declaration order. If negative, this gives the period order (-1 first, then -2 ...). If positive, this gives the true start time and will abort DASHing at period end \nNote: When both positive and negative values are found, the by-order periods (negative) will be inserted AFTER the timed period (positive) \nASID: assigns parent adaptation set ID. If not 0, only sources with same AS ID will be in the same adaptation set \nNote: If multiple streams in source, only the first stream will have an AS ID assigned \nxlink: for remote periods, only checked for null PID \nRole, PDesc, ASDesc, ASCDesc, RDesc: various descriptors to set for period, AS or representation \nBUrl: overrides segmenter [-base] with a set of BaseURLs to use for the PID (per representation) \nTemplate: overrides segmenter template for this PID \nDashDur: overrides segmenter segment duration for this PID \nStartNumber: sets the start number for the first segment in the PID, default is 1 \nIntraOnly: indicates input PID follows HLS EXT-X-I-FRAMES-ONLY guidelines \nCropOrigin: indicates x and y coordinates of video for SRD (size is video size) \nSRD: indicates SRD position and size of video for SRD, ignored if CropOrigin is set \nSRDRef: indicates global width and height of SRD, ignored if CropOrigin is set \nHLSPL: name of variant playlist, can use templates \nHLSMExt: list of extensions to add to master playlist entries, ['foo','bar=val'] added as ,foo,bar=val \nHLSVExt: list of extensions to add to variant playlist, ['#foo','#bar=val'] added as #foo \\n #bar=val \nNon-dash properties: Bitrate, SAR, Language, Width, Height, SampleRate, NumChannels, Language, ID, DependencyID, FPS, Interlaced, Codec. These properties are used to setup each representation and can be overridden on input PIDs using the general PID property settings (cf global help). \nExample\ngpac -i test.mp4:#Bitrate=1M -o test.mpd\nThis will force declaring a bitrate of 1M for the representation, regardless of actual input bitrate.\nExample\ngpac -i muxav.mp4 -o test.mpd\nThis will create un-multiplexed DASH segments.\nExample\ngpac -i muxav.mp4:#Representation=1 -o test.mpd\nThis will create multiplexed DASH segments.\nExample\ngpac -i m1.mp4 -i m2.mp4:#Period=Yep -o test.mpd\nThis will put src m1.mp4 in first period, m2.mp4 in second period.\nExample\ngpac -i m1.mp4:#BUrl=http://foo/bar -o test.mpd\nThis will assign a baseURL to src m1.mp4.\nExample\ngpac -i m1.mp4:#ASCDesc=&lt;ElemName val=\"attval\"&gt;text&lt;/ElemName&gt; -o test.mpd\nThis will assign the specified XML descriptor to the adaptation set.\nNote: this can be used to inject most DASH descriptors not natively handled by the segmenter.\nThe segmenter handles the XML descriptor as a string and does not attempt to validate it. Descriptors, as well as some segmenter filter arguments, are string lists (comma-separated by default), so that multiple descriptors can be added:\nExample\ngpac -i m1.mp4:#RDesc=&lt;Elem attribute=\"1\"/&gt;,&lt;Elem2&gt;text&lt;/Elem2&gt; -o test.mpd\nThis will insert two descriptors in the representation(s) of m1.mp4.\nExample\ngpac -i video.mp4:#Template=foo$Number$ -i audio.mp4:#Template=bar$Number$ -o test.mpd\nThis will assign different templates to the audio and video sources.\nExample\ngpac -i null:#xlink=http://foo/bar.xml:#PDur=4 -i m.mp4:#PStart=-1 -o test.mpd\nThis will insert an create an MPD with first a remote period then a regular one.\nExample\ngpac -i null:#xlink=http://foo/bar.xml:#PStart=6 -i m.mp4 -o test.mpd\nThis will create an MPD with first a regular period, dashing only 6s of content, then a remote one.\nExample\ngpac -i v1:#SRD=0x0x1280x360:#SRDRef=1280x720 -i v2:#SRD=0x360x1280x360 -o test.mpd\nThis will layout the v2 below v1 using a global SRD size of 1280x720. \nThe segmenter will create multiplexing filter chains for each representation and will reassign PID IDs so that each media component (video, audio, ...) in an adaptation set has the same ID. \nFor HLS, the output manifest PID will deliver the master playlist and the variant playlists.\nThe default variant playlist are $NAME_$N.m3u8, where $NAME is the radical of the output file name and $N is the 1-based index of the variant. \nWhen HLS mode is enabled, the segment template is relative to the variant playlist file, which can also be templated.\nExample\ngpac -i av.mp4:#HLSPL=$Type$/index.m3u8 -o dash/live.m3u8:dual:template='$Number$'\nThis will put video segments and playlist in dash/video/ and audio segments and playlist in dash/audio/ \nSegmentation¶\nThe default behavior of the segmenter is to estimate the theoretical start time of each segment based on target segment duration, and start a new segment when a packet with SAP type 1,2,3 or 4 with time greater than the theoretical time is found.\nThis behavior can be changed to find the best SAP packet around a segment theoretical boundary using sbound: \nclosest mode: the segment will start at the closest SAP of the theoretical boundary \nin mode: the segment will start at or before the theoretical boundary \nWarning: These modes will introduce delay in the segmenter (typically buffering of one GOP) and should not be used for low-latency modes. \nThe segmenter can also be configured to: \ncompletely ignore SAP when segmenting using sap. \nignore SAP on non-video streams when segmenting using strict_sap. \nWhen seg_sync is disabled, the segmenter will by default announce a new segment in the manifest(s) as soon as its size/offset is known or its name is known, but the segment (or part in LL-HLS) may still not be completely written/sent.\nThis may result in temporary mismatches between segment/part size currently received versus size as advertized in manifest.\nWhen seg_sync is enabled, the segmenter will wait for the last byte of the fragment/segment to be pushed before announcing a new segment in the manifest(s). This can however slightly increase the latency in MPEG-DASH low-latency. \nWhen (-sflush)[] is set to single, segmentation is skipped and a single segment is generated per input. \nDynamic (real-time live) Mode¶\nThe dasher does not perform real-time regulation by default.\nFor regular segmentation, you should enable segment regulation sreg if your sources are not real-time.\nExample\ngpac -i source.mp4 -o live.mpd:segdur=2:profile=live:dmode=dynamic:sreg\nFor low latency segmentation with fMP4, you will need to specify the following options: \ncdur: set the fMP4 fragment duration \nasto: set the availability time offset for DASH. This value should be equal or slightly greater than segment duration minus cdur \nllhls: enable low latency for HLS \nNote: llhls does not force cmaf mode to allow for multiplexed media in segments but it enforces to tfdt_traf in the muxer. \nIf your sources are not real-time, insert a reframer filter with real-time regulation\nExample\ngpac -i source.mp4 reframer:rt=on -o live.mpd:segdur=2:cdur=0.2:asto=1.8:profile=live:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and indicate to the client that requests can be made 1.8 seconds earlier than segment complete availability on server.\nExample\ngpac -i source.mp4 reframer:rt=on -o live.m3u8:segdur=2:cdur=0.2:llhls=br:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and produce HLS low latency parts using byte ranges in the final segment.\nExample\ngpac -i source.mp4 reframer:rt=on -o live.m3u8:segdur=2:cdur=0.2:llhls=sf:dmode=dynamic\nThis will create DASH segments of 2 seconds made of fragments of 200 ms and produce HLS low latency parts using dedicated files. \nYou can combine LL-HLS and DASH-LL generation:\nExample\ngpac -i source.mp4 reframer:rt=on -o live.mpd:dual:segdur=2:cdur=0.2:asto=1.8:llhls=br:profile=live:dmode=dynamic\nFor DASH, the filter will use the local clock for UTC anchor points in DASH.\nThe filter can fetch and signal clock in other ways using utcs.\nExample\nThis will use the local clock and insert in the MPD a UTCTiming descriptor containing the local clock.\nExample\n[opts]::utcs=http://time.akamai.com[::opts]\nThis will fetch time from http://time.akamai.com, use it as the UTC reference for segment generation and insert in the MPD a UTCTiming descriptor containing the time server URL.\nNote: if not set as a global option using --utcs=, you must escape the url using double :: or use other separators. \nCue-driven segmentation¶\nThe segmenter can take a list of instructions, or Cues, to use for the segmentation process, in which case only these are used to derive segment boundaries. Cues can be set through XML files or injected in input packets. \nCue files can be specified for the entire segmenter, or per PID using DashCue property.\nCues are given in an XML file with a root element called <DASHCues>, with currently no attribute specified. The children are one or more <Stream> elements, with attributes: \nid: integer for stream/track/PID ID \ntimescale: integer giving the units of following timestamps \nmode: if present and value is edit, the timestamp are in presentation time (edit list applied) otherwise they are in media time \nts_offset: integer giving a value (in timescale) to subtract to the DTS/CTS values listed \nThe children of <Stream> are one or more <Cue> elements, with attributes: \nsample: integer giving the sample/frame number of a sample at which splitting shall happen \ndts: long integer giving the decoding time stamp of a sample at which splitting shall happen \ncts: long integer giving the composition / presentation time stamp of a sample at which splitting shall happen \nWarning: Cues shall be listed in decoding order. \nIf the DashCue property of a PID equals inband, the PID will be segmented according to the CueStart property of input packets.\nThis feature is typically combined with a list of files as input:\nExample\ngpac -i list.m3u:sigcues -o res/live.mpd\nThis will load the flist filter in cue mode, generating continuous timelines from the sources and injecting a CueStart property at each new file. \nIf the cues option equals none, the DashCue property of input PIDs will be ignored. \nManifest Generation only mode¶\nThe segmenter can be used to generate manifests from already fragmented ISOBMFF inputs using sigfrag.\nIn this case, segment boundaries are attached to each packet starting a segment and used to drive the segmentation.\nThis can be used with single-track ISOBMFF sources, either single file or multi file.\nFor single file source: \nif onDemand profile is requested, sources have to be formatted as a DASH self-initializing media segment with the proper sidx. \ntemplates are disabled. \nsseg is forced for all profiles except onDemand ones. \nFor multi files source: \ninput shall be a playlist containing the initial file followed by the ordered list of segments. \nif no template is provided, the full or main profile will be used \nif -template is provided, it shall be correct: the filter will not try to guess one from the input file names and will not validate it either. \nThe manifest generation-only mode supports both MPD and HLS generation. \nExample\ngpac -i ondemand_src.mp4 -o dash.mpd:sigfrag:profile=onDemand\nThis will generate a DASH manifest for onDemand Profile based on the input file.\nExample\ngpac -i ondemand_src.mp4 -o dash.m3u8:sigfrag\nThis will generate a HLS manifest based on the input file.\nExample\ngpac -i seglist.txt -o dash.mpd:sigfrag\nThis will generate a DASH manifest in Main Profile based on the input files.\nExample\ngpac -i seglist.txt:Template=$XInit=init$$q1/$Number$ -o dash.mpd:sigfrag:profile=live\nThis will generate a DASH manifest in live Profile based on the input files. The input file will contain init.mp4, q1/1.m4s, q1/2.m4s... \nCue Generation only mode¶\nThe segmenter can be used to only generate segment boundaries from a set of inputs using gencues, without generating manifests or output files.\nIn this mode, output PIDs are declared directly rather than redirected to media segment files.\nThe segmentation logic is not changed, and packets are forwarded with the same information and timing as in regular mode. \nOutput PIDs are forwarded with DashCue=inband property, so that any subsequent dasher follows the same segmentation process (see above). \nThe first packet in a segment has: \nproperty FileNumber (and, if multiple files, FileName) set as usual \nproperty CueStart set \nproperty DFPStart=0 set if this is the first packet in a period \nThis mode can be used to pre-segment the streams for later processing that must take place before final dashing.\nExample\ngpac -i source.mp4 dasher:gencues cecrypt:cfile=roll_seg.xml -o live.mpd\nThis will allow the encrypter to locate dash boundaries and roll keys at segment boundaries.\nExample\ngpac -i s1.mp4 -i s2.mp4:#CryptInfo=clear:#Period=3 -i s3.mp4:#Period=3 dasher:gencues cecrypt:cfile=roll_period.xml -o live.mpd\nIf the DRM file uses keyRoll=period, this will generate: \nfirst period crypted with one key \nsecond period clear \nthird period crypted with another key \nForced-Template mode¶\nWhen tpl_force is set, the template string is not analyzed nor modified for missing elements.\nThis is typically used to redirect segments to a given destination regardless of the dash profile.\nExample\ngpac -i SRC -o null:ext=mpd:tpl_force --template=pipe://mypipe\nThis will trash the manifest and open mypipe as destination for the muxer result. \nWarning: Options for segment destination cannot be set through the template, global options must be used. \nBatch Operations¶\nThe segmentation can be performed in multiple calls using a DASH context set with state.\nBetween calls, the PIDs are reassigned by checking that the PID ID match between the calls and: \nthe input file names match between the calls \nor the representation ID (and period ID if specified) match between the calls \nIf a PID is not matched, it will be assigned to a new period. \nThe default behaviour assume that the same inputs are used for segmentation and rebuilds a contiguous timeline at each new file start.\nIf the inputs change but form a continuous timeline, [-keep_ts])() must be used to skip timeline reconstruction. \nThe inputs will be segmented for a duration of subdur if set, otherwise the input media duration.\nWhen inputs are over, they are restarted if loop is set otherwise a new period is created.\nTo avoid this behaviour, the sflush option should be set to end or single, indicating that further sources for the same representations will be added in subsequent calls. When sflush is not off, the (-loop)[] option is ignored. \nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX && gpac -i SRC -o dash.mpd:segdur=2:state=CTX\nThis will generate all dash segments for SRC (last one possibly shorter) and create a new period at end of input.\nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX:loop && gpac -i SRC -o dash.mpd:segdur=2:state=CTX:loop\nThis will generate all dash segments for SRC and restart SRC to fill-up last segment.\nExample\ngpac -i SRC -o dash.mpd:segdur=2:state=CTX:sflush=end && gpac -i SRC -o dash.mpd:segdur=2:state=CTX:sflush=end\nThis will generate all dash segments for SRC without looping/closing the period at end of input. Timestamps in the second call will be rewritten to be contiguous with timestamp at end of first call.\nExample\ngpac -i SRC1 -o dash.mpd:segdur=2:state=CTX:sflush=end:keep_ts && gpac -i SRC2 -o dash.mpd:segdur=2:state=CTX:sflush=end:keep_ts\nThis will generate all dash segments for SRC1 without looping/closing the period at end of input, then for SRC2. Timestamps of the sources will not be rewritten. \nNote: The default behaviour of MP4Box -dash-ctx option is to set the (-loop)[] to true. \nOutput redirecting¶\nWhen loaded implicitly during link resolution, the dasher will only link its outputs to the target sink\nExample\ngpac -i SRC -o URL1:OPTS1 -o URL2:OPTS1\nThis will create one dasher (with options OPTS1) for the URL1 and one dasher (with options OPTS1) for URL2.\nThis allows dashing to multiple outputs with different formats, dash durations, etc. \nIt can be useful to redirect all the filter outputs to several sinks, for example to push through ROUTE and through HTTP the same segments.\nIn order to do this, the filter MUST be explicitly loaded and all options related to dash and MP4 must be set either globally or on the dasher filter.\nExample\ngpac -i SRC dasher:cmfc:segdur=2 -o URL1 -o URL2\nThis will create a single dasher whose outputs (manifests and segments) will be redirected to the given URLs.\nWhen explicitly loading the filter, the dual option will be disabled unless mname is set to the alternate output name. \nMultiplexer development considerations¶\nOutput multiplexers allowing segmented output must obey the following: \ninspect packet properties \nFileNumber: if set, indicate the start of a new DASH segment \nFileName: if set, indicate the file name. If not present, output shall be a single file. This is only set for packet carrying the FileNumber property, and only on one PID (usually the first) for multiplexed outputs \nIDXName: gives the optional index name. If not present, index shall be in the same file as dash segment. Only used for MPEG-2 TS for now \nEODS: property is set on packets with no payload and no timestamp to signal the end of a DASH segment. This is only used when stopping/resuming the segmentation process, in order to flush segments without dispatching an EOS (see subdur ) \nfor each segment done, send a downstream event on the first connected PID signaling the size of the segment and the size of its index if any \nfor multiplexers with init data, send a downstream event signaling the size of the init and the size of the global index if any \nthe following filter options are passed to multiplexers, which should declare them as arguments: \nnoinit: disables output of init segment for the multiplexer (used to handle bitstream switching with single init in DASH) \nfrag: indicates multiplexer shall use fragmented format (used for ISOBMFF mostly) \nsubs_sidx=0: indicates an SIDX shall be generated - only added if not already specified by user \nxps_inband=all|no|both: indicates AVC/HEVC/... parameter sets shall be sent inband, out of band, or both \nnofragdef: indicates fragment defaults should be set in each segment rather than in init segment \nThe segmenter adds the following properties to the output PIDs: \nDashMode: identifies VoD (single file with global index) or regular DASH mode used by segmenter \nDashDur: identifies target DASH segment duration - this can be used to estimate the SIDX size for example \nLLHLS: identifies LLHLS is used; the multiplexer must send fragment size events back to the dasher, and set LLHLSFragNum on the first packet of each fragment \nSegSync: indicates that fragments/segments must be completely flushed before sending back size events \nOptions¶\nsegdur (frac, default: 0/0): target segment duration in seconds. A value less than or equal to 0 defaults to 1.0 second\ntpl (bool, default: true): use template mode (multiple segment, template URLs)\nstl (bool, default: false): use segment timeline (ignored in on_demand mode)\ndmode (enum, default: static, updatable): dash content mode \nstatic: static content \ndynamic: live generation \ndynlast: last call for live, will turn the MPD into static \ndynauto: live generation and move to static manifest upon end of stream \nsseg (bool, default: false): single segment is used\nsfile (bool, default: false): use a single file for all segments (default in on_demand)\nalign (bool, default: true): enable segment time alignment between representations\nsap (bool, default: true): enable splitting segments at SAP boundaries\nmix_codecs (bool, default: false): enable mixing different codecs in an adaptation set\nntp (enum, default: rem): insert/override NTP clock at the beginning of each segment \nrem: removes NTP from all input packets \nyes: inserts NTP at each segment start \nkeep: leaves input packet NTP untouched \nno_sar (bool, default: false): do not check for identical sample aspect ratio for adaptation sets\nbs_switch (enum, default: def): bitstream switching mode (single init segment) \ndef: resolves to off for onDemand and inband for live \noff: disables BS switching \non: enables it if same decoder configuration is possible \ninband: moves decoder config inband if possible \nboth: inband and outband parameter sets \npps: moves PPS and APS inband, keep VPS,SPS and DCI out of band (used for VVC RPR) \nforce: enables it even if only one representation \nmulti: uses multiple stsd entries in ISOBMFF \ntemplate (str): template string to use to generate segment name\nsegext (str): file extension to use for segments\ninitext (str): file extension to use for the init segment\nmuxtype (enum, default: auto): muxtype to use for the segments \nmp4: uses ISOBMFF format \nts: uses MPEG-2 TS format \nmkv: uses Matroska format \nwebm: uses WebM format \nogg: uses OGG format \nraw: uses raw media format (disables multiplexed representations) \nauto: guess format based on extension, default to mp4 if no extension \nrawsub (bool, default: no): use raw subtitle format instead of encapsulating in container\nasto (dbl, default: 0): availabilityStartTimeOffset to use in seconds. A negative value simply increases the AST, a positive value sets the ASToffset to representations\nprofile (enum, default: auto): target DASH profile. This will set default option values to ensure conformance to the desired profile. For MPEG-2 TS, only main and live are used, others default to main \nauto: turns profile to live for dynamic and full for non-dynamic \nlive: DASH live profile, using segment template \nonDemand: MPEG-DASH live profile \nmain: MPEG-DASH main profile, using segment list \nfull: MPEG-DASH full profile \nhbbtv1.5.live: HBBTV 1.5 DASH profile \ndashavc264.live: DASH-IF live profile \ndashavc264.onDemand: DASH-IF onDemand profile \ndashif.ll: DASH IF low-latency profile (set UTC server to time.akamai.com if none set) \nprofX (str): list of profile extensions, as used by DASH-IF and DVB. The string will be colon-concatenated with the profile used. If starting with +, the profile string by default is erased and + is skipped\ncp (enum, default: set): content protection element location \nset: in adaptation set element \nrep: in representation element \nboth: in both adaptation set and representation elements \npssh (enum, default: v): storage mode for PSSH box \nf: stores in movie fragment only \nv: stores in movie only, or movie and fragments if key roll is detected \nm: stores in mpd only \nmf: stores in mpd and movie fragment \nmv: stores in mpd and movie \nn: discard pssh from mpd and segments \nbuf (sint, default: -100): min buffer duration in ms. negative value means percent of segment duration (e.g. -150 = 1.5*seg_dur)\nspd (sint, default: 0): suggested presentation delay in ms\ntimescale (sint, default: 0): set timescale for timeline and segment list/template. A value of 0 picks up the first timescale of the first stream in an adaptation set. A negative value forces using stream timescales for each timed element (multiplication of segment list/template/timelines). A positive value enforces the MPD timescale\ncheck_dur (bool, default: true): check duration of sources in period, trying to have roughly equal duration. Enforced whenever period start times are used\nskip_seg (bool, default: false): increment segment number whenever an empty segment would be produced - NOT DASH COMPLIANT\ntitle (str): MPD title\nsource (str): MPD Source\ninfo (str): MPD info url\ncprt (str): MPD copyright string\nlang (str): language of MPD Info\nlocation (strl): set MPD locations to given URL\nbase (strl): set base URLs of MPD\nrefresh (dbl, default: 0): refresh rate for dynamic manifests, in seconds (a negative value sets the MPD duration, value 0 uses dash duration)\ntsb (dbl, default: 30): time-shift buffer depth in seconds (a negative value means infinity)\nkeep_segs (bool, default: false): do not delete segments no longer in time-shift buffer\nast (str): set start date (as xs:date, e.g. YYYY-MM-DDTHH:MM:SSZ) for live mode. Default is now. !! Do not use with multiple periods, nor when DASH duration is not a multiple of GOP size !!\nstate (str): path to file used to store/reload state info when simulating live. This is stored as a valid MPD with GPAC XML extensions\nkeep_ts (bool, default: false): do not shift timestamp when reloading a context\nloop (bool, default: false): loop sources when dashing with subdur and state. If not set, a new period is created once the sources are over\nsubdur (dbl, default: 0): maximum duration of the input file to be segmented. This does not change the segment duration, segmentation stops once segments produced exceeded the duration\nsplit (bool, default: true): enable cloning samples for text/metadata/scene description streams, marking further clones as redundant\nhlsc (bool, default: false): insert clock reference in variant playlist in live HLS\ncues (str): set cue file\nstrict_cues (bool, default: false): strict mode for cues, complains if splitting is not on SAP type 1/2/3 or if unused cue is found\nstrict_sap (enum, default: off): strict mode for sap \noff: ignore SAP types for PID other than video, enforcing AdaptationSet@startsWithSAP=1 \nsig: same as off but keep AdaptationSet@startsWithSAP to the true SAP value \non: warn if any PID uses SAP 3 or 4 and switch to FULL profile \nintra: ignore SAP types greater than 3 on all media types \nsubs_sidx (sint, default: -1): number of subsegments per sidx. Negative value disables sidx. Only used to inherit sidx option of destination\ncmpd (bool, default: false): skip line feed and spaces in MPD XML for compactness\nstyp (str): indicate the 4CC to use for styp boxes when using ISOBMFF output\ndual (bool): indicate to produce both MPD and M3U files\nsigfrag (bool): use manifest generation only mode\nsbound (enum, default: out): indicate how the theoretical segment start TSS (= segment_number * duration) should be handled \nout: segment split as soon as TSS is exceeded (TSS <= segment_start) \nclosest: segment split at closest SAP to theoretical bound \nin: TSS is always in segment (TSS >= segment_start) \nreschedule (bool, default: false): reschedule sources with no period ID assigned once done (dynamic mode only)\nsreg (bool, default: false): regulate the session \nwhen using subdur and context, only generate segments from the past up to live edge \notherwise in dynamic mode without context, do not generate segments ahead of time \nscope_deps (bool, default: true): scope PID dependencies to be within source. If disabled, PID dependencies will be checked across all input PIDs regardless of their sources\nutcs (str): URL to use as time server / UTCTiming source. Special value inband enables inband UTC (same as publishTime), special prefix xsd@ uses xsDateTime schemeURI rather than ISO\nsflush (enum, default: off): segment flush mode - see filter help: \noff: no specific actions \nsingle: force generating a single segment for each input \nend: skip loop detection and clamp duration adjustment at end of input, used for state mode \nlast_seg_merge (bool, default: false): force merging last segment if less than half the target duration\nmha_compat (enum, default: no): adaptation set generation mode for compatible MPEG-H Audio profile \nno: only generate the adaptation set for the main profile \ncomp: only generate the adaptation sets for all compatible profiles \nall: generate the adaptation set for the main profile and all compatible profiles \nmname (str): output manifest name for ATSC3 multiplexing (using 'm3u8' only toggles HLS generation)\nllhls (enum, default: off): HLS low latency type \noff: do not use LL-HLS \nbr: use LL-HLS with byte-range for segment parts, pointing to full segment (DASH-LL compatible) \nsf: use separate files for segment parts (post-fixed .1, .2 etc.) \nbrsf: generate two sets of manifest, one for byte-range and one for files (_IF added before extension of manifest) \nhlsdrm (str): cryp file info for HLS full segment encryption\nhlsx (strl): list of string to append to master HLS header before variants with ['#foo','#bar=val'] added as #foo \\n #bar=val\nhlsiv (bool, default: true): inject IV in variant HLS playlist` \n<a id=\"ll_preload_hint\">__ll_preload_hint__</a> (bool, default: _true_): inject preload hint for LL-HLS \n<a id=\"ll_rend_rep\">__ll_rend_rep__</a> (bool, default: _true_): inject rendition reports for LL-HLS \n<a id=\"ll_part_hb\">__ll_part_hb__</a> (dbl, default: _-1_): user-defined part hold-back for LLHLS, negative value means 3 times max part duration in session \n<a id=\"ckurl\">__ckurl__</a> (str): set the ClearKey URL common to all encrypted streams (overriden byCKUrl` pid property)\nhls_absu (enum, default: no): use absolute url in HLS generation using first URL in base \nno: do not use absolute URL \nvar: use absolute URL only in variant playlists \nmas: use absolute URL only in master playlist \nboth: use absolute URL everywhere \nhls_ap (bool, default: false): use audio as primary media instead of video when generating playlists\nseg_sync (enum, default: auto): control how waiting on last packet P of fragment/segment to be written impacts segment injection in manifest \nno: do not wait for P \nyes: wait for P \nauto: wait for P if HLS is used \ncmaf (enum, default: no): use cmaf guidelines \nno: CMAF not enforced \ncmfc: use CMAF cmfc guidelines \ncmf2: use CMAF cmf2 guidelines \npswitch (enum, default: single): period switch control mode \nsingle: change period if PID configuration changes \nforce: force period switch at each PID reconfiguration instead of absorbing PID reconfiguration (for splicing or add insertion not using periodID) \nstsd: change period if PID configuration changes unless new configuration was advertised in initial config \nchain (str): URL of next MPD for regular chaining\nchain_fbk (str): URL of fallback MPD\ngencues (bool, default: false): only insert segment boundaries and do not generate manifests\nforce_init (bool, default: false): force init segment creation in bitstream switching mode\nkeep_src (bool, default: false): keep source URLs in manifest generation mode\ngxns (bool, default: false): insert some gpac extensions in manifest (for now, only tfdt of first segment)\ndkid (enum, default: auto): control injection of default KID in MPD \noff: default KID not injected \non: default KID always injected \nauto: default KID only injected if no key roll is detected (as per DASH-IF guidelines) \ntpl_force (bool, default: false): use template string as is without trying to add extension or solve conflicts in names\nttml_agg (bool, default: false): force aggregation of TTML samples of a DASH segment into a single sample \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/dasher/"},{"date_scraped_timestamp":1720188004648,"host":"wiki.gpac.io","page_title":"dtout - GPAC wiki","text":"\n \n \n \n \n \n \nDekTec SDIOut¶\nRegister name used to load filter: dtout\nThis filter may be automatically loaded during graph resolution. \nThis filter provides SDI output to be used with DTA 2174 or DTA 2154 cards. \nOptions¶\nbus (sint, default: -1): PCI bus number. If not set, device discovery is used\nslot (sint, default: -1): PCI bus number. If not set, device discovery is used\nfps (frac, default: 30/1): default FPS to use if input stream fps cannot be detected\nclip (bool, default: false): clip YUV data to valid SDI range, slower\nport (uint, default: 1): set sdi output port of card\nstart (dbl, default: 0.0): set playback start offset, [-1, 0] means percent of media dur, e.g. -1 == dur \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/dtout/"},{"date_scraped_timestamp":1720188004648,"error":"Aborted due to cross-transaction contention. This occurs when multiple transactions attempt to access the same data, requiring Firestore to abort at least one in order to enforce serializability.","host":"wiki.gpac.io","page_title":"dtout - GPAC wiki","text":"\n \n \n \n \n \n \nDekTec SDIOut¶\nRegister name used to load filter: dtout\nThis filter may be automatically loaded during graph resolution. \nThis filter provides SDI output to be used with DTA 2174 or DTA 2154 cards. \nOptions¶\nbus (sint, default: -1): PCI bus number. If not set, device discovery is used\nslot (sint, default: -1): PCI bus number. If not set, device discovery is used\nfps (frac, default: 30/1): default FPS to use if input stream fps cannot be detected\nclip (bool, default: false): clip YUV data to valid SDI range, slower\nport (uint, default: 1): set sdi output port of card\nstart (dbl, default: 0.0): set playback start offset, [-1, 0] means percent of media dur, e.g. -1 == dur \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/dtout/"},{"date_scraped_timestamp":1720187941900,"host":"wiki.gpac.io","page_title":"evgs - GPAC wiki","text":"\n \n \n \n \n \n \nEVG video rescaler¶\nRegister name used to load filter: evgs\nThis filter may be automatically loaded during graph resolution.\nFilters of this class can connect to each-other. \nThis filter rescales raw video data using GPAC's EVG library to the specified size and pixel format. \nOutput size assignment¶\nIf osize is {0,0}, the output dimensions will be set to the input size, and input aspect ratio will be ignored. \nIf osize is {0,H} (resp. {W,0}), the output width (resp. height) will be set to respect input aspect ratio. If keepar=nosrc, input sample aspect ratio is ignored. \nAspect Ratio and Sample Aspect Ratio¶\nWhen output sample aspect ratio is set, the output dimensions are divided by the output sample aspect ratio.\nExample\nevgs:osize=288x240:osar=3/2\nThe output dimensions will be 192x240. \nWhen aspect ratio is not kept (keepar=off): \nsource is resampled to desired dimensions \nif output aspect ratio is not set, output will use source sample aspect ratio \nWhen aspect ratio is partially kept (keepar=nosrc): \nresampling is done on the input data without taking input sample aspect ratio into account \nif output sample aspect ratio is not set (osar=0/N), source aspect ratio is forwarded to output. \nWhen aspect ratio is fully kept (keepar=full), output aspect ratio is force to 1/1 if not set. \nWhen sample aspect ratio is kept, the filter will: \ncenter the rescaled input frame on the output frame \nfill extra pixels with padclr \nOptions¶\nosize (v2di): osize of output video\nofmt (pfmt, default: none): pixel format for output video. When not set, input format is used\nofr (bool, default: false): force output full range\nkeepar (enum, default: off): keep aspect ratio \noff: ignore aspect ratio \nfull: respect aspect ratio, applying input sample aspect ratio info \nnosrc: respect aspect ratio but ignore input sample aspect ratio \npadclr (str, default: black): clear color when aspect ration preservation is used\nosar (frac, default: 0/1): force output pixel aspect ratio\nnbth (sint, default: -1): number of threads to use, -1 means all cores\nhq (bool, default: false): use bilinear interpolation instead of closest pixel \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/evgs/"},{"date_scraped_timestamp":1720188050052,"host":"wiki.gpac.io","page_title":"ffenc - GPAC wiki","text":"\n \n \n \n \n \n \nFFmpeg encoder¶\nRegister name used to load filter: ffenc\nThis filter may be automatically loaded during graph resolution. \nThis filter encodes audio and video streams using FFmpeg.\nSee FFmpeg documentation (https://ffmpeg.org/documentation.html) for more details.\nTo list all supported encoders for your GPAC build, use gpac -h ffenc:*. \nThe filter will try to resolve the codec name in c against a libavcodec codec name (e.g. libx264) and use it if found.\nIf not found, it will consider the name to be a GPAC codec name and find a codec for it. In that case, if no pixel format is given, codecs will be enumerated to find a matching pixel format. \nOptions can be passed from prompt using --OPT=VAL (global options) or appending ::OPT=VAL to the desired encoder filter. \nThe filter will look for property TargetRate on input PID to set the desired bitrate per PID. \nThe filter will force a closed gop boundary: \nat each packet with a FileNumber property set or a CueStart property set to true. \nif fintra and rc is set. \nWhen forcing a closed GOP boundary, the filter will flush, destroy and recreate the encoder to make sure a clean context is used, as currently many encoders in libavcodec do not support clean reset when forcing picture types.\nIf fintra is not set and the output of the encoder is a DASH session in live profile without segment timeline, fintra will be set to the target segment duration and rc will be set. \nThe filter will look for property logpass on input PID to set 2-pass log filename, otherwise defaults to ffenc2pass-PID.log. \nArguments may be updated at runtime. If rld is set, the encoder will be flushed then reloaded with new options.\nIf codec is video and fintra is set, reload will happen at next forced intra; otherwise, reload happens at next encode.\nThe rld option is usually needed for dynamic updates of rate control parameters, since most encoders in ffmpeg do not support it. \nOptions¶\nc (str): codec identifier. Can be any supported GPAC codec name or ffmpeg codec name - updated to ffmpeg codec name after initialization\npfmt (pfmt, default: none): pixel format for input video. When not set, input format is used\nfintra (frac, default: -1/1): force intra / IDR frames at the given period in sec, e.g. fintra=2 will force an intra every 2 seconds and fintra=1001/1000 will force an intra every 30 frames on 30000/1001=29.97 fps video; ignored for audio\nall_intra (bool, default: false, updatable): only produce intra frames\nls (bool, default: false): log stats\nrc (bool, default: false): reset encoder when forcing intra frame (some encoders might not support intra frame forcing)\nrld (bool, default: false, updatable): force reloading of encoder when arguments are updated\n* (str): any possible options defined for AVCodecContext and sub-classes. see gpac -hx ffenc and gpac -hx ffenc:* \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ffenc/"},{"date_scraped_timestamp":1720188079053,"host":"wiki.gpac.io","page_title":"ffenc - GPAC wiki","text":"\n \n \n \n \n \n \nFFmpeg encoder¶\nRegister name used to load filter: ffenc\nThis filter may be automatically loaded during graph resolution. \nThis filter encodes audio and video streams using FFmpeg.\nSee FFmpeg documentation (https://ffmpeg.org/documentation.html) for more details.\nTo list all supported encoders for your GPAC build, use gpac -h ffenc:*. \nThe filter will try to resolve the codec name in c against a libavcodec codec name (e.g. libx264) and use it if found.\nIf not found, it will consider the name to be a GPAC codec name and find a codec for it. In that case, if no pixel format is given, codecs will be enumerated to find a matching pixel format. \nOptions can be passed from prompt using --OPT=VAL (global options) or appending ::OPT=VAL to the desired encoder filter. \nThe filter will look for property TargetRate on input PID to set the desired bitrate per PID. \nThe filter will force a closed gop boundary: \nat each packet with a FileNumber property set or a CueStart property set to true. \nif fintra and rc is set. \nWhen forcing a closed GOP boundary, the filter will flush, destroy and recreate the encoder to make sure a clean context is used, as currently many encoders in libavcodec do not support clean reset when forcing picture types.\nIf fintra is not set and the output of the encoder is a DASH session in live profile without segment timeline, fintra will be set to the target segment duration and rc will be set. \nThe filter will look for property logpass on input PID to set 2-pass log filename, otherwise defaults to ffenc2pass-PID.log. \nArguments may be updated at runtime. If rld is set, the encoder will be flushed then reloaded with new options.\nIf codec is video and fintra is set, reload will happen at next forced intra; otherwise, reload happens at next encode.\nThe rld option is usually needed for dynamic updates of rate control parameters, since most encoders in ffmpeg do not support it. \nOptions¶\nc (str): codec identifier. Can be any supported GPAC codec name or ffmpeg codec name - updated to ffmpeg codec name after initialization\npfmt (pfmt, default: none): pixel format for input video. When not set, input format is used\nfintra (frac, default: -1/1): force intra / IDR frames at the given period in sec, e.g. fintra=2 will force an intra every 2 seconds and fintra=1001/1000 will force an intra every 30 frames on 30000/1001=29.97 fps video; ignored for audio\nall_intra (bool, default: false, updatable): only produce intra frames\nls (bool, default: false): log stats\nrc (bool, default: false): reset encoder when forcing intra frame (some encoders might not support intra frame forcing)\nrld (bool, default: false, updatable): force reloading of encoder when arguments are updated\n* (str): any possible options defined for AVCodecContext and sub-classes. see gpac -hx ffenc and gpac -hx ffenc:* \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ffenc/?q="},{"date_scraped_timestamp":1720187919126,"host":"wiki.gpac.io","page_title":"ffmx - GPAC wiki","text":"\n \n \n \n \n \n \nFFmpeg multiplexer¶\nRegister name used to load filter: ffmx\nThis filter may be automatically loaded during graph resolution. \nMultiplexes files and open output protocols using FFmpeg.\nSee FFmpeg documentation (https://ffmpeg.org/documentation.html) for more details.\nTo list all supported multiplexers for your GPAC build, use gpac -h ffmx:*.This will list both supported output formats and protocols.\nOutput protocols are listed with Description: Output protocol, and the subclass name identifies the protocol scheme.\nFor example, if ffmx:rtmp is listed as output protocol, this means rtmp:// destination URLs are supported. \nSome URL formats may not be sufficient to derive the multiplexing format, you must then use ffmt to specify the desired format. \nUnlike other multiplexing filters in GPAC, this filter is a sink filter and does not produce any PID to be redirected in the graph.\nThe filter can however use template names for its output, using the first input PID to resolve the final name.\nThe filter watches the property FileNumber on incoming packets to create new files. \nAll PID properties prefixed with meta: will be added as metadata. \nOptions¶\ndst (cstr): location of destination file or remote URL\nstart (dbl, default: 0.0): set playback start offset. A negative value means percent of media duration with -1 equal to duration\nspeed (dbl, default: 1.0): set playback speed. If negative and start is 0, start is set to -1\nileave (frac, default: 1): interleave window duration in second, a value of 0 disable interleaving\nnodisc (bool, default: false): ignore stream configuration changes while multiplexing, may result in broken streams\nmime (cstr): set mime type for graph resolution\nffiles (bool, default: false): force complete files to be created for each segment in DASH modes\nffmt (str): force ffmpeg output format for the given URL\nblock_size (uint, default: 4096): block size used to read file when using avio context\nkeepts (bool, default: true): do not shift input timeline back to 0\n* (str): any possible options defined for AVFormatContext and sub-classes (see gpac -hx ffmx and gpac -hx ffmx:*) \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ffmx/"},{"date_scraped_timestamp":1720188073547,"host":"wiki.gpac.io","page_title":"ffsws - GPAC wiki","text":"\n \n \n \n \n \n \nFFmpeg video rescaler¶\nRegister name used to load filter: ffsws\nThis filter may be automatically loaded during graph resolution.\nFilters of this class can connect to each-other. \nThis filter rescales raw video data using FFmpeg to the specified size and pixel format. \nOutput size assignment¶\nIf osize is {0,0}, the output dimensions will be set to the input size, and input aspect ratio will be ignored. \nIf osize is {0,H} (resp. {W,0}), the output width (resp. height) will be set to respect input aspect ratio. If keepar=nosrc, input sample aspect ratio is ignored. \nAspect Ratio and Sample Aspect Ratio¶\nWhen output sample aspect ratio is set, the output dimensions are divided by the output sample aspect ratio.\nExample\nffsws:osize=288x240:osar=3/2\nThe output dimensions will be 192x240. \nWhen aspect ratio is not kept (keepar=off): \nsource is resampled to desired dimensions \nif output aspect ratio is not set, output will use source sample aspect ratio \nWhen aspect ratio is partially kept (keepar=nosrc): \nresampling is done on the input data without taking input sample aspect ratio into account \nif output sample aspect ratio is not set (osar=0/N), source aspect ratio is forwarded to output. \nWhen aspect ratio is fully kept (keepar=full), output aspect ratio is force to 1/1 if not set. \nWhen sample aspect ratio is kept, the filter will: \ncenter the rescaled input frame on the output frame \nfill extra pixels with padclr \nAlgorithms options¶\nfor bicubic, to tune the shape of the basis function, p1 tunes f(1) and p2 f´(1) \nfor gauss p1 tunes the exponent and thus cutoff frequency \nfor lanczos p1 tunes the width of the window function \nSee FFmpeg documentation (https://ffmpeg.org/documentation.html) for more details \nOptions¶\nosize (v2di): osize of output video\nofmt (pfmt, default: none, Enum: none|yuv420|yvu420|yuv420_10|yuv422|yuv422_10|yuv444|yuv444_10|uyvy|vyuy|yuyv|yvyu|uyvl|vyul|yuyl|yvyl|nv12|nv21|nv1l|nv2l|yuva|yuvd|yuv444a|yuv444p|v308|yuv444ap|v408|v410|v210|grey|algr|gral|rgb4|rgb5|rgb6|rgba|argb|bgra|abgr|rgb|bgr|xrgb|rgbx|xbgr|bgrx|rgbd|rgbds|uncv): pixel format for output video. When not set, input format is used \nscale (enum, default: bicubic): scaling mode (see filter help) (fastbilinear|bilinear|bicubic|X|point|area|bicublin|gauss|sinc|lanzcos|spline) \np1 (dbl, default: +I): scaling algo param1\np2 (dbl, default: +I): scaling algo param2\nofr (bool, default: false): force output full range\nbrightness (bool, default: 0): 16.16 fixed point brightness correction, 0 means use default\ncontrast (uint, default: 0): 16.16 fixed point brightness correction, 0 means use default\nsaturation (uint, default: 0): 16.16 fixed point brightness correction, 0 means use default\notable (sintl): the yuv2rgb coefficients describing the output yuv space, normally ff_yuv2rgb_coeffs[x], use default if not set\nitable (sintl): the yuv2rgb coefficients describing the input yuv space, normally ff_yuv2rgb_coeffs[x], use default if not set\nkeepar (enum, default: off): keep aspect ratio \noff: ignore aspect ratio \nfull: respect aspect ratio, applying input sample aspect ratio info \nnosrc: respect aspect ratio but ignore input sample aspect ratio \npadclr (str, default: black): clear color when aspect ration preservation is used\nosar (frac, default: 0/1): force output pixel aspect ratio \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ffsws/"},{"date_scraped_timestamp":1720187974796,"host":"wiki.gpac.io","page_title":"glpush - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nGPU texture uploader¶\nRegister name used to load filter: glpush\nThis is a JavaScript filter. It is not checked during graph resolution and needs explicit loading.\nAuthor: GPAC team \nThis filter pushes input video streams to GPU as OpenGL textures. It can be used to simulate hardware decoders dispatching OpenGL textures \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/glpush/"},{"date_scraped_timestamp":1720187986135,"host":"wiki.gpac.io","page_title":"glpush - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nGPU texture uploader¶\nRegister name used to load filter: glpush\nThis is a JavaScript filter. It is not checked during graph resolution and needs explicit loading.\nAuthor: GPAC team \nThis filter pushes input video streams to GPU as OpenGL textures. It can be used to simulate hardware decoders dispatching OpenGL textures \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/glpush/?q="},{"date_scraped_timestamp":1720188047945,"host":"wiki.gpac.io","page_title":"gsfmx - GPAC wiki","text":"\n \n \n \n \n \n \nGSF Multiplexer¶\nRegister name used to load filter: gsfmx\nThis filter may be automatically loaded during graph resolution. \nThis filter provides GSF (GPAC Serialized Format) multiplexing.\nIt serializes the stream states (config/reconfig/info update/remove/eos) and packets of input PIDs. This allows either saving to file a session, or forwarding the state/data of streams to another instance of GPAC using either pipes or sockets. Upstream events are not serialized. \nThe default behavior does not insert sequence numbers. When running over general protocols not ensuring packet order, this should be inserted.\nThe serializer sends tune-in packets (global and per PID) at the requested carousel rate - if 0, no carousel. These packets are marked as redundant so that they can be discarded by output filters if needed. \nEncryption¶\nThe stream format can be encrypted in AES 128 CBC mode. For all packets, the packet header (header, size, frame size/block offset and optional seq num) are in the clear and the following bytes until the last byte of the last multiple of block size (16) fitting in the payload are encrypted.\nFor data packets, each fragment is encrypted individually to avoid error propagation in case of losses.\nFor other packets, the entire packet is encrypted before fragmentation (fragments cannot be processed individually).\nFor header/tunein packets, the first 25 bytes after the header are in the clear (signature,version,IV and pattern).\nThe IV is constant to avoid packet overhead, randomly generated if not set and sent in the initial stream header. Pattern mode can be used (cf CENC cbcs) to encrypt K block and leave N blocks in the clear. \nFiltering properties¶\nThe header/tunein packet may get quite big when all PID properties are kept. In order to help reduce its size, the minp option can be used: this will remove all built-in properties marked as droppable (cf property help) as well as all non built-in properties.\nThe skp option may also be used to specify which property to drop:\nExample\nThis will remove properties of type 4CC1 and properties (built-in or not) of name Name2. \nFile mode¶\nBy default the filter only accepts framed media streams as input PID, not files. This can be changed by explicitly loading the filter with ext or dst set.\nExample\ngpac -i source.mp4 gsfmx:dst=manifest.mpd -o dump.gsf\nThis will DASH the source and store every files produced as PIDs in the GSF mux.\nIn order to demultiplex such a file, the gsfdmxfilter will likely need to be explicitly loaded:\nExample\ngpac -i mux.gsf gsfdmx -o dump/$File$:dynext\nThis will extract all files from the GSF mux. \nBy default when working in file mode, the filter only accepts PIDs of type file as input.\nTo allow a mix of files and streams, use mixed:\nExample\ngpac -i source.mp4 gsfmx:dst=manifest.mpd:mixed -o dump.gsf\nThis will DASH the source, store the manifest file and the media streams with their packet properties in the GSF mux. \nOptions¶\nsigsn (bool, default: false): signal packet sequence number after header field and before size field. Sequence number is per PID, encoded on 16 bits. Header packet does not have a SN\nsigdur (bool, default: true): signal duration\nsigbo (bool, default: false): signal byte offset\nsigdts (bool, default: true): signal decoding timestamp\ndbg (enum, default: no): set debug mode \nno: disable debug \nnodata: force packet size to 0 \nnopck: skip packet \nkey (mem): encrypt packets using given key\nIV (mem): set IV for encryption - a constant IV is used to keep packet overhead small (cbcs-like)\npattern (frac, default: 1/0): set nb_crypt / nb_skip block pattern. default is all encrypted\nmpck (uint, default: 0): set max packet size. 0 means no fragmentation (each AU is sent in one packet)\nmagic (str): magic string to append in setup packet\nskp (str): comma separated list of PID property names to skip\nminp (bool, default: false): include only the minimum set of properties required for stream processing\ncrate (dbl, default: 0): carousel period for tune-in info in seconds\next (str): file extension for file mode\ndst (str): target URL in file mode\nmixed (bool, default: false): allow GSF to contain both files and media streams \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/gsfmx/"},{"date_scraped_timestamp":1720187925516,"host":"wiki.gpac.io","page_title":"httpin - GPAC wiki","text":"\n \n \n \n \n \n \nHTTP input¶\nRegister name used to load filter: httpin\nThis filter may be automatically loaded during graph resolution. \nThis filter dispatch raw blocks from a remote HTTP resource into a filter chain.\nBlock size can be adjusted using block_size, and disk caching policies can be adjusted.\nContent format can be forced through mime and file extension can be changed through ext. \nThe filter supports both http and https schemes, and will attempt reconnecting as TLS if TCP connection fails. \nNote: Unless disabled at session level (see -no-probe ), file extensions are usually ignored and format probing is done on the first data block. \nOptions¶\nsrc (cstr): URL of source content\nblock_size (uint, default: 100000): block size used to read file\ncache (enum, default: none): set cache mode \nauto: cache to disk if content length is known, no cache otherwise \ndisk: cache to disk, discard once session is no longer used \nkeep: cache to disk and keep \nmem: stores to memory, discard once session is no longer used \nmem_keep: stores to memory, keep after session is reassigned but move to mem after first download \nnone: no cache \nnone_keep: stores to memory, keep after session is reassigned but move to none after first download \nrange (lfrac, default: 0-0): set byte range, as fraction\next (cstr): override file extension\nmime (cstr): set file mime type\nblockio (bool, default: false): use blocking IO \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/httpin/"},{"date_scraped_timestamp":1720188029259,"host":"wiki.gpac.io","page_title":"httpin - GPAC wiki","text":"\n \n \n \n \n \n \nHTTP input¶\nRegister name used to load filter: httpin\nThis filter may be automatically loaded during graph resolution. \nThis filter dispatch raw blocks from a remote HTTP resource into a filter chain.\nBlock size can be adjusted using block_size, and disk caching policies can be adjusted.\nContent format can be forced through mime and file extension can be changed through ext. \nThe filter supports both http and https schemes, and will attempt reconnecting as TLS if TCP connection fails. \nNote: Unless disabled at session level (see -no-probe ), file extensions are usually ignored and format probing is done on the first data block. \nOptions¶\nsrc (cstr): URL of source content\nblock_size (uint, default: 100000): block size used to read file\ncache (enum, default: none): set cache mode \nauto: cache to disk if content length is known, no cache otherwise \ndisk: cache to disk, discard once session is no longer used \nkeep: cache to disk and keep \nmem: stores to memory, discard once session is no longer used \nmem_keep: stores to memory, keep after session is reassigned but move to mem after first download \nnone: no cache \nnone_keep: stores to memory, keep after session is reassigned but move to none after first download \nrange (lfrac, default: 0-0): set byte range, as fraction\next (cstr): override file extension\nmime (cstr): set file mime type\nblockio (bool, default: false): use blocking IO \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/httpin/?q="},{"date_scraped_timestamp":1720187970808,"host":"wiki.gpac.io","page_title":"inspect - GPAC wiki","text":"\n \n \n \n \n \n \nInspect packets¶\nRegister name used to load filter: inspect\nThis filter is not checked during graph resolution and needs explicit loading. \nThe inspect filter can be used to dump PID and packets. It may also be used to check parts of payload of the packets. \nThe default options inspect only PID changes.\nIf full is not set, mode=frame is forced and PID properties are formatted in human-readable form, one PID per line.\nOtherwise, all properties are dumped.\nNote: specifying xml, analyze, fmt or using -for-test will force full to true. \nCustom property dumping¶\nThe packet inspector can be configured to dump specific properties of packets using fmt.\nWhen the option is not present, all properties are dumped. Otherwise, only properties identified by $TOKEN$ are printed. You may use '$', '@' or '%' for TOKEN separator. TOKEN can be: \npn: packet (frame in framed mode) number \ndts: decoding time stamp in stream timescale, N/A if not available \nddts: difference between current and previous packets decoding time stamp in stream timescale, N/A if not available \ncts: composition time stamp in stream timescale, N/A if not available \ndcts: difference between current and previous packets composition time stamp in stream timescale, N/A if not available \nctso: difference between composition time stamp and decoding time stamp in stream timescale, N/A if not available \ndur: duration in stream timescale \nframe: framing status \ninterface: complete AU, interface object (no size info). Typically a GL texture \nframe_full: complete AU \nframe_start: beginning of frame \nframe_end: end of frame \nframe_cont: frame continuation (not beginning, not end) \nsap or rap: SAP type of the frame \nilace: interlacing flag (0: progressive, 1: top field, 2: bottom field) \ncorr: corrupted packet flag \nseek: seek flag \nbo: byte offset in source, N/A if not available \nroll: roll info \ncrypt: crypt flag \nvers: carousel version number \nsize: size of packet \ncsize: total size of packets received so far \ncrc: 32 bit CRC of packet \nlf or n: insert new line \nt: insert tab \ndata: hex dump of packet (big output!) or as string if legal UTF-8 \nlp: leading picture flag \ndepo: depends on other packet flag \ndepf: is depended on other packet flag \nred: redundant coding flag \nstart: packet composition time as HH:MM:SS.ms \nstartc: packet composition time as HH:MM:SS,ms \nend: packet end time as HH:MM:SS.ms \nendc: packet end time as HH:MM:SS,ms \nck: clock type used for PCR discontinuities \npcr: MPEG-2 TS last PCR, n/a if not available \npcrd: difference between last PCR and decoding time, n/a if no PCR available \npcrc: difference between last PCR and composition time, n/a if no PCR available \nP4CC: 4CC of packet property \nPropName: Name of packet property \npid.P4CC: 4CC of PID property \npid.PropName: Name of PID property \nfn: Filter name \nExample\nfmt=\"PID $pid.ID$ packet $pn$ DTS $dts$ CTS $cts$ $lf$\"\nThis dumps packet number, cts and dts as follows: PID 1 packet 10 DTS 100 CTS 108 \\n \nAn unrecognized keyword or missing property will resolve to an empty string. \nNote: when dumping in interleaved mode, there is no guarantee that the packets will be dumped in their original sequence order since the inspector fetches one packet at a time on each PID. \nNote on playback¶\nBuffering can be enabled to check the input filter chain behaviour, e.g. check HAS adaptation logic.\nThe various buffering options control when packets are consumed. Buffering events are logged using media@info for state changes and media@debug for media filling events.\nThe speed option is only used to configure the filter chain but is ignored by the filter when consuming packets.\nIf real-time consumption is required, a reframer filter must be setup before the inspect filter.\nExample\ngpac -i SRC reframer:rt=on inspect:buffer=10000:rbuffer=1000:mbuffer=30000:speed=2\nThis will play the session at 2x speed, using 30s of maximum buffering, consuming packets after 10s of media are ready and rebuffering if less than 1s of media. \nOptions¶\nlog (str, default: stdout, Enum: _any|stderr|stdout|GLOG|TL|null): set probe log filename \n_any: target file path and name \nstderr: dump to stderr \nstdout: dump to stdout \nGLOG: use GPAC logs app@info \nTL: use GPAC log tool TL at level info \nnull: silent mode \nmode (enum, default: pck): dump mode \npck: dump full packet \nblk: dump packets before reconstruction \nframe: force reframer \nraw: dump source packets without demultiplexing \ninterleave (bool, default: true): dump packets as they are received on each PID. If false, logs are reported for each PID at end of session\ndeep (bool, default: false, updatable): dump packets along with PID state change, implied when fmt is set\nprops (bool, default: true, updatable): dump packet properties, ignored when fmt is set\ndump_data (bool, default: false, updatable): enable full data dump (very large output), ignored when fmt is set\nfmt (str, updatable): set packet dump format\nhdr (bool, default: true): print a header corresponding to fmt string without '$' or \"pid\"\nallp (bool, default: false): analyse for the entire duration, rather than stopping when all PIDs are found\ninfo (bool, default: false, updatable): monitor PID info changes\nfull (bool, default: false, updatable): full dump of PID properties (always on if XML)\npcr (bool, default: false, updatable): dump M2TS PCR info\nspeed (dbl, default: 1.0): set playback command speed. If negative and start is 0, start is set to -1\nstart (dbl, default: 0.0): set playback start offset. A negative value means percent of media duration with -1 equal to duration\ndur (frac, default: 0/0): set inspect duration\nanalyze (enum, default: off, updatable): analyze sample content (NALU, OBU), similar to -bsdbg option of reframer filters \noff: no analyzing \non: simple analyzing \nbs: log bitstream syntax (all elements read from bitstream) \nfull: log bitstream syntax and bit sizes signaled as (N) after field value, except 1-bit fields (omitted) \nxml (bool, default: false, updatable): use xml formatting (implied if (-analyze]() is set) and disable fmt\ncrc (bool, default: false, updatable): dump crc of samples of subsamples (NALU or OBU) when analyzing\nfftmcd (bool, default: false, updatable): consider timecodes use ffmpeg-compatible signaling rather than QT compliant one\ndtype (bool, default: false, updatable): dump property type\nbuffer (uint, default: 0): set playback buffer in ms\nmbuffer (uint, default: 0): set max buffer occupancy in ms. If less than buffer, use buffer\nrbuffer (uint, default: 0, updatable): rebuffer trigger in ms. If 0 or more than buffer, disable rebuffering\nstats (bool, default: false): compute statistics for PIDs\ntest (enum, default: no, updatable): skip predefined set of properties, used for test mode \nno: no properties skipped \nnoprop: all properties/info changes on PID are skipped, only packets are dumped \nnetwork: URL/path dump, cache state, file size properties skipped (used for hashing network results) \nnetx: same as network but skip track duration and templates (used for hashing progressive load of fmp4) \nencode: same as network plus skip decoder config (used for hashing encoding results) \nencx: same as encode and skip bitrates, media data size and co \nnocrc: disable packet CRC dump \nnobr: skip bitrate \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/inspect/"},{"date_scraped_timestamp":1720188041825,"host":"wiki.gpac.io","page_title":"jpgenc - GPAC wiki","text":"\n \n \n \n \n \n \nJPG encoder¶\nRegister name used to load filter: jpgenc\nThis filter may be automatically loaded during graph resolution. \nThis filter encodes a single uncompressed video PID to JPEG using libjpeg. \nOptions¶\ndctmode (enum, default: fast): type of DCT used \nslow: precise but slow integer DCT \nfast: less precise but faster integer DCT \nfloat: float DCT \nquality (uint, default: 100, minmax: 0-100, updatable): compression quality \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/jpgenc/"},{"date_scraped_timestamp":1720188060532,"host":"wiki.gpac.io","page_title":"jpgenc - GPAC wiki","text":"\n \n \n \n \n \n \nJPG encoder¶\nRegister name used to load filter: jpgenc\nThis filter may be automatically loaded during graph resolution. \nThis filter encodes a single uncompressed video PID to JPEG using libjpeg. \nOptions¶\ndctmode (enum, default: fast): type of DCT used \nslow: precise but slow integer DCT \nfast: less precise but faster integer DCT \nfloat: float DCT \nquality (uint, default: 100, minmax: 0-100, updatable): compression quality \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/jpgenc/?q="},{"date_scraped_timestamp":1720188050184,"host":"wiki.gpac.io","page_title":"m2psdmx - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMPEG PS demultiplexer¶\nRegister name used to load filter: m2psdmx\nThis filter may be automatically loaded during graph resolution. \nThis filter demultiplexes MPEG-2 program streams to produce media PIDs and frames. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/m2psdmx/"},{"date_scraped_timestamp":1720188079855,"host":"wiki.gpac.io","page_title":"m2psdmx - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMPEG PS demultiplexer¶\nRegister name used to load filter: m2psdmx\nThis filter may be automatically loaded during graph resolution. \nThis filter demultiplexes MPEG-2 program streams to produce media PIDs and frames. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/m2psdmx/?q="},{"date_scraped_timestamp":1720188000585,"host":"wiki.gpac.io","page_title":"m2tsmx - GPAC wiki","text":"\n \n \n \n \n \n \nMPEG-2 TS multiplexer¶\nRegister name used to load filter: m2tsmx\nThis filter may be automatically loaded during graph resolution. \nThis filter multiplexes one or more input PIDs into a MPEG-2 Transport Stream multiplex. \nPID selection¶\nThe MPEG-2 TS multiplexer assigns M2TS PID for media streams using the PID of the PMT plus the stream index.\nFor example, the default config creates the first program with a PMT PID 100, the first stream will have a PID of 101.\nStreams are grouped in programs based on input PID property ServiceID if present. If absent, stream will go in the program with service ID as indicated by sid option. \nname option is overridden by input PID property ServiceName. \nprovider option is overridden by input PID property ServiceProvider. \npcr_offset option is overridden by input PID property \"tsmux:pcr_offset\" \nfirst_pts option is overridden by input PID property \"tsmux:force_pts\" \ntemi option is overridden by input PID property \"tsmux:temi\" \nThe temi option allows specifying a list of URLs or timeline IDs to insert in streams of a program.\nOne or more TEMI timeline can be specified per PID.\nThe syntax is a comma-separated list of one or more TEMI description.\nEach TEMI description is formatted as ID_OR_URL or #OPT1[#OPT2]#ID_OR_URL. Options are: \nSN: indicate the target service with ID N \nTN: set timescale to use (default: PID timescale) \nDN: set delay in ms between two TEMI url descriptors (default 1000) \nON: set offset (max 64 bits) to add to TEMI timecodes (default 0). If timescale is not specified, offset value is in ms, otherwise in timescale units. \nIN: set initial value (max 64 bits) of TEMI timecodes. If not set, initial value will match first packet CTS. If timescale is not specified, value is in PID timescale units, otherwise in specified timescale units. \nPN: indicate target PID in program. Possible values are \nV: only insert for video streams. \nA: only insert for audio streams. \nT: only insert for text streams. \nN: only insert for stream with index N (0-based) in the program. \nLC: set 64bit timecode signaling. Possible values for C are: \nA: automatic switch between 32 and 64 bit depending on timecode value (default if not specified). \nY: use 64 bit signaling only. \nN: use 32 bit signaling only and wrap around timecode value. \nN: insert NTP timestamp in TEMI timeline descriptor \nID_OR_URL: If number, indicate the TEMI ID to use for external timeline. Otherwise, give the URL to insert \nExample\nInserts a TEMI URL+timecode in the each stream of each program.\nExample\nInserts a TEMI URL+timecode in the first stream of all programs and an external TEMI with ID 4 in the second stream of all programs.\nExample\ntemi=\"#P0#2,#P0#url,#P1#4\"\nInserts a TEMI with ID 2 and a TEMI URL+timecode in the first stream of all programs, and an external TEMI with ID 4 in the second stream of all programs.\nExample\nInserts an external TEMI with ID 4 in the each stream of program with ServiceID 20 and a TEMI URL in each stream of program with ServiceID 10.\nExample\ntemi=\"#N#D500#PV#T30000#4\"\nInserts an external TEMI with ID 4 and timescale 30000, NTP injection and carousel of 500 ms in the video stream of all programs. \nWarning: multipliers (k,m,g) are not supported in TEMI options. \nAdaptive Streaming¶\nIn DASH and HLS mode: \nthe PCR is always initialized at 0, and flush_rap is automatically set. \nunless nb_pack is specified, 200 TS packets will be used as pack output in DASH mode. \npes_pack=none is forced since some demultiplexers have issues with non-aligned ADTS PES. \nThe filter watches the property FileNumber on incoming packets to create new files, or new segments in DASH mode.\nThe filter will look for property M2TSRA set on the input stream.\nThe value can either be a 4CC or a string, indicating the MP2G-2 TS Registration tag for unknown media types. \nNotes¶\nIn LATM mux mode, the decoder configuration is inserted at the given repeat_rate or CarouselRate PID property if defined. \nOptions¶\nbreq (uint, default: 100): buffer requirements in ms for input PIDs\npmt_id (uint, default: 100): define the ID of the first PMT to use in the mux\nrate (uint, default: 0): target rate in bps of the multiplex. If not set, variable rate is used\npmt_rate (uint, default: 200): interval between PMT in ms\npat_rate (uint, default: 200): interval between PAT in ms\nfirst_pts (luint, default: 0): force PTS value of first packet, in 90kHz\npcr_offset (luint, default: -1): offset all timestamps from PCR by V, in 90kHz (default value is computed based on input media)\nmpeg4 (enum, default: none): force usage of MPEG-4 signaling (IOD and SL Config) \nnone: disables 4on2 \nfull: sends AUs as SL packets over section for OD, section/pes for scene (cf bifs_pes) \nscene: sends only scene streams as 4on2 but uses regular PES without SL for audio and video \npmt_version (uint, default: 200): set version number of the PMT\ndisc (bool, default: false): set the discontinuity marker for the first packet of each stream\nrepeat_rate (uint, default: 0): interval in ms between two carousel send for MPEG-4 systems (overridden by CarouselRate PID property if defined)\nrepeat_img (uint, default: 0): interval in ms between re-sending (as PES) of single-image streams (if 0, image data is sent once only)\nmax_pcr (uint, default: 100): set max interval in ms between 2 PCR\nnb_pack (uint, default: 4): pack N TS packets in output packets\npes_pack (enum, default: audio): set AU to PES packing mode \naudio: will pack only multiple audio AUs in a PES \nnone: make exactly one AU per PES \nall: will pack multiple AUs per PES for all streams \nrealtime (bool, default: false): use real-time output\nbifs_pes (enum, default: off): select BIFS streams packetization (PES vs sections) \non: uses BIFS PES \noff: uses BIFS sections \ncopy: uses BIFS PES but removes timestamps in BIFS SL and only carries PES timestamps \nflush_rap (bool, default: false): force flushing mux program when RAP is found on video, and injects PAT and PMT before the next video PES begin\npcr_only (bool, default: false): enable PCR-only TS packets\npcr_init (lsint, default: -1): set initial PCR value for the programs. A negative value means random value is picked\nsid (uint, default: 0): set service ID for the program\nname (str): set service name for the program\nprovider (str): set service provider name for the program\nsdt_rate (uint, default: 0): interval in ms between two DVB SDT tables (if 0, SDT is disabled)\ntemi (str): insert TEMI time codes in adaptation field\nlog_freq (uint, default: 500): delay between logs for realtime mux\nlatm (bool, default: false): use LATM AAC encapsulation instead of regular ADTS\nsubs_sidx (sint, default: -1): number of subsegments per sidx (negative value disables sidx)\nkeepts (bool, default: false): keep cts/dts untouched and adjust PCR accordingly, used to keep TS unmodified when dashing \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/m2tsmx/"},{"date_scraped_timestamp":1720188000585,"error":"Aborted due to cross-transaction contention. This occurs when multiple transactions attempt to access the same data, requiring Firestore to abort at least one in order to enforce serializability.","host":"wiki.gpac.io","page_title":"m2tsmx - GPAC wiki","text":"\n \n \n \n \n \n \nMPEG-2 TS multiplexer¶\nRegister name used to load filter: m2tsmx\nThis filter may be automatically loaded during graph resolution. \nThis filter multiplexes one or more input PIDs into a MPEG-2 Transport Stream multiplex. \nPID selection¶\nThe MPEG-2 TS multiplexer assigns M2TS PID for media streams using the PID of the PMT plus the stream index.\nFor example, the default config creates the first program with a PMT PID 100, the first stream will have a PID of 101.\nStreams are grouped in programs based on input PID property ServiceID if present. If absent, stream will go in the program with service ID as indicated by sid option. \nname option is overridden by input PID property ServiceName. \nprovider option is overridden by input PID property ServiceProvider. \npcr_offset option is overridden by input PID property \"tsmux:pcr_offset\" \nfirst_pts option is overridden by input PID property \"tsmux:force_pts\" \ntemi option is overridden by input PID property \"tsmux:temi\" \nThe temi option allows specifying a list of URLs or timeline IDs to insert in streams of a program.\nOne or more TEMI timeline can be specified per PID.\nThe syntax is a comma-separated list of one or more TEMI description.\nEach TEMI description is formatted as ID_OR_URL or #OPT1[#OPT2]#ID_OR_URL. Options are: \nSN: indicate the target service with ID N \nTN: set timescale to use (default: PID timescale) \nDN: set delay in ms between two TEMI url descriptors (default 1000) \nON: set offset (max 64 bits) to add to TEMI timecodes (default 0). If timescale is not specified, offset value is in ms, otherwise in timescale units. \nIN: set initial value (max 64 bits) of TEMI timecodes. If not set, initial value will match first packet CTS. If timescale is not specified, value is in PID timescale units, otherwise in specified timescale units. \nPN: indicate target PID in program. Possible values are \nV: only insert for video streams. \nA: only insert for audio streams. \nT: only insert for text streams. \nN: only insert for stream with index N (0-based) in the program. \nLC: set 64bit timecode signaling. Possible values for C are: \nA: automatic switch between 32 and 64 bit depending on timecode value (default if not specified). \nY: use 64 bit signaling only. \nN: use 32 bit signaling only and wrap around timecode value. \nN: insert NTP timestamp in TEMI timeline descriptor \nID_OR_URL: If number, indicate the TEMI ID to use for external timeline. Otherwise, give the URL to insert \nExample\nInserts a TEMI URL+timecode in the each stream of each program.\nExample\nInserts a TEMI URL+timecode in the first stream of all programs and an external TEMI with ID 4 in the second stream of all programs.\nExample\ntemi=\"#P0#2,#P0#url,#P1#4\"\nInserts a TEMI with ID 2 and a TEMI URL+timecode in the first stream of all programs, and an external TEMI with ID 4 in the second stream of all programs.\nExample\nInserts an external TEMI with ID 4 in the each stream of program with ServiceID 20 and a TEMI URL in each stream of program with ServiceID 10.\nExample\ntemi=\"#N#D500#PV#T30000#4\"\nInserts an external TEMI with ID 4 and timescale 30000, NTP injection and carousel of 500 ms in the video stream of all programs. \nWarning: multipliers (k,m,g) are not supported in TEMI options. \nAdaptive Streaming¶\nIn DASH and HLS mode: \nthe PCR is always initialized at 0, and flush_rap is automatically set. \nunless nb_pack is specified, 200 TS packets will be used as pack output in DASH mode. \npes_pack=none is forced since some demultiplexers have issues with non-aligned ADTS PES. \nThe filter watches the property FileNumber on incoming packets to create new files, or new segments in DASH mode.\nThe filter will look for property M2TSRA set on the input stream.\nThe value can either be a 4CC or a string, indicating the MP2G-2 TS Registration tag for unknown media types. \nNotes¶\nIn LATM mux mode, the decoder configuration is inserted at the given repeat_rate or CarouselRate PID property if defined. \nOptions¶\nbreq (uint, default: 100): buffer requirements in ms for input PIDs\npmt_id (uint, default: 100): define the ID of the first PMT to use in the mux\nrate (uint, default: 0): target rate in bps of the multiplex. If not set, variable rate is used\npmt_rate (uint, default: 200): interval between PMT in ms\npat_rate (uint, default: 200): interval between PAT in ms\nfirst_pts (luint, default: 0): force PTS value of first packet, in 90kHz\npcr_offset (luint, default: -1): offset all timestamps from PCR by V, in 90kHz (default value is computed based on input media)\nmpeg4 (enum, default: none): force usage of MPEG-4 signaling (IOD and SL Config) \nnone: disables 4on2 \nfull: sends AUs as SL packets over section for OD, section/pes for scene (cf bifs_pes) \nscene: sends only scene streams as 4on2 but uses regular PES without SL for audio and video \npmt_version (uint, default: 200): set version number of the PMT\ndisc (bool, default: false): set the discontinuity marker for the first packet of each stream\nrepeat_rate (uint, default: 0): interval in ms between two carousel send for MPEG-4 systems (overridden by CarouselRate PID property if defined)\nrepeat_img (uint, default: 0): interval in ms between re-sending (as PES) of single-image streams (if 0, image data is sent once only)\nmax_pcr (uint, default: 100): set max interval in ms between 2 PCR\nnb_pack (uint, default: 4): pack N TS packets in output packets\npes_pack (enum, default: audio): set AU to PES packing mode \naudio: will pack only multiple audio AUs in a PES \nnone: make exactly one AU per PES \nall: will pack multiple AUs per PES for all streams \nrealtime (bool, default: false): use real-time output\nbifs_pes (enum, default: off): select BIFS streams packetization (PES vs sections) \non: uses BIFS PES \noff: uses BIFS sections \ncopy: uses BIFS PES but removes timestamps in BIFS SL and only carries PES timestamps \nflush_rap (bool, default: false): force flushing mux program when RAP is found on video, and injects PAT and PMT before the next video PES begin\npcr_only (bool, default: false): enable PCR-only TS packets\npcr_init (lsint, default: -1): set initial PCR value for the programs. A negative value means random value is picked\nsid (uint, default: 0): set service ID for the program\nname (str): set service name for the program\nprovider (str): set service provider name for the program\nsdt_rate (uint, default: 0): interval in ms between two DVB SDT tables (if 0, SDT is disabled)\ntemi (str): insert TEMI time codes in adaptation field\nlog_freq (uint, default: 500): delay between logs for realtime mux\nlatm (bool, default: false): use LATM AAC encapsulation instead of regular ADTS\nsubs_sidx (sint, default: -1): number of subsegments per sidx (negative value disables sidx)\nkeepts (bool, default: false): keep cts/dts untouched and adjust PCR accordingly, used to keep TS unmodified when dashing \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/m2tsmx/"},{"date_scraped_timestamp":1720187937200,"host":"wiki.gpac.io","page_title":"nhntw - GPAC wiki","text":"\n \n \n \n \n \n \nNHNT writer¶\nRegister name used to load filter: nhntw\nThis filter may be automatically loaded during graph resolution. \nThis filter converts a single stream to an NHNT output file.\nNHNT documentation is available at https://wiki.gpac.io/xmlformats/NHNT-Format \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results\nlarge (bool, default: false): use large file mode \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/nhntw/"},{"date_scraped_timestamp":1720188013872,"host":"wiki.gpac.io","page_title":"ohevcdec - GPAC wiki","text":"\n \n \n \n \n \n \nOpenHEVC decoder¶\nRegister name used to load filter: ohevcdec\nThis filter may be automatically loaded during graph resolution. \nThis filter decodes HEVC and LHVC (HEVC scalable extensions) from one or more PIDs through the OpenHEVC library \nOptions¶\nthreading (enum, default: frame): set threading mode \nframeslice: parallel decoding of both frames and slices \nframe: parallel decoding of frames \nslice: parallel decoding of slices \nnb_threads (uint, default: 0): set number of threads (if 0, uses number of cores minus one)\nno_copy (bool, default: false): directly dispatch internal decoded frame without copy\npack_hfr (bool, default: false): pack 4 consecutive frames in a single output\nseek_reset (bool, default: false): reset decoder when seeking\nforce_stereo (bool, default: true): use stereo output for multiview (top-bottom only)\nreset_switch (bool, default: false): reset decoder at config change \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ohevcdec/"},{"date_scraped_timestamp":1720188125434,"host":"wiki.gpac.io","page_title":"pout - GPAC wiki","text":"\n \n \n \n \n \n \npipe output¶\nRegister name used to load filter: pout\nThis filter may be automatically loaded during graph resolution. \nThis filter handles generic output pipes (mono-directional) in blocking mode only. \nWarning: Output pipes do not currently support non blocking mode. \nThe associated protocol scheme is pipe:// when loaded as a generic output (e.g. -o pipe://URL where URL is a relative or absolute pipe name).\nData format of the pipe shall be specified using extension (either in filename or through ext option) or MIME type through mime\nThe pipe name indicated in dst can use template mechanisms from gpac, e.g. dst=pipe_$ServiceID$ \nOn Windows hosts, the default pipe prefix is \\\\.\\pipe\\gpac\\ if no prefix is set \ndst=mypipe resolves in \\\\.\\pipe\\gpac\\mypipe\ndst=\\\\.\\pipe\\myapp\\mypipe resolves in \\\\.\\pipe\\myapp\\mypipe\nAny destination name starting with \\\\ is used as is, with \\ translated in / \nThe pipe input can create the pipe if not found using mkp. On windows hosts, this will create a pipe server.\nOn non windows hosts, the created pipe will delete the pipe file upon filter destruction.\nThe pipe can be kept alive after a broken pipe is detected using ka. This is typically used when clients crash/exits and resumes.\nWhen a keep-alive pipe is broken, input data is discarded and the filter will keep trashing data as fast as possible.\nIt is therefore recommended to use this mode with real-time inputs (use a reframer if needed).\nIf marker is set, the string GPACPIF (8 bytes including 0-terminator) will be written to the pipe at each detected pipeline flush.\nPipeline flushing is currently triggered by DASH segment end or ISOBMF fragment end. \nOptions¶\ndst (cstr): name of destination pipe\next (str): indicate file extension of pipe data\nmime (str): indicate mime type of pipe data\ndynext (bool, default: false): indicate the file extension is set by filter chain, not dst\nstart (dbl, default: 0.0): set playback start offset. A negative value means percent of media duration with -1 equal to duration\nspeed (dbl, default: 1.0): set playback speed. If negative and start is 0, start is set to -1\nmkp (bool, default: false): create pipe if not found\nblock_size (uint, default: 5000): buffer size used to write to pipe, Windows only\nka (bool, default: false): keep pipe alive when broken pipe is detected\nmarker (bool, default: false): inject marker upon pipeline flush events \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/pout/?q="},{"date_scraped_timestamp":1720188106776,"host":"wiki.gpac.io","page_title":"pout - GPAC wiki","text":"\n \n \n \n \n \n \npipe output¶\nRegister name used to load filter: pout\nThis filter may be automatically loaded during graph resolution. \nThis filter handles generic output pipes (mono-directional) in blocking mode only. \nWarning: Output pipes do not currently support non blocking mode. \nThe associated protocol scheme is pipe:// when loaded as a generic output (e.g. -o pipe://URL where URL is a relative or absolute pipe name).\nData format of the pipe shall be specified using extension (either in filename or through ext option) or MIME type through mime\nThe pipe name indicated in dst can use template mechanisms from gpac, e.g. dst=pipe_$ServiceID$ \nOn Windows hosts, the default pipe prefix is \\\\.\\pipe\\gpac\\ if no prefix is set \ndst=mypipe resolves in \\\\.\\pipe\\gpac\\mypipe\ndst=\\\\.\\pipe\\myapp\\mypipe resolves in \\\\.\\pipe\\myapp\\mypipe\nAny destination name starting with \\\\ is used as is, with \\ translated in / \nThe pipe input can create the pipe if not found using mkp. On windows hosts, this will create a pipe server.\nOn non windows hosts, the created pipe will delete the pipe file upon filter destruction.\nThe pipe can be kept alive after a broken pipe is detected using ka. This is typically used when clients crash/exits and resumes.\nWhen a keep-alive pipe is broken, input data is discarded and the filter will keep trashing data as fast as possible.\nIt is therefore recommended to use this mode with real-time inputs (use a reframer if needed).\nIf marker is set, the string GPACPIF (8 bytes including 0-terminator) will be written to the pipe at each detected pipeline flush.\nPipeline flushing is currently triggered by DASH segment end or ISOBMF fragment end. \nOptions¶\ndst (cstr): name of destination pipe\next (str): indicate file extension of pipe data\nmime (str): indicate mime type of pipe data\ndynext (bool, default: false): indicate the file extension is set by filter chain, not dst\nstart (dbl, default: 0.0): set playback start offset. A negative value means percent of media duration with -1 equal to duration\nspeed (dbl, default: 1.0): set playback speed. If negative and start is 0, start is set to -1\nmkp (bool, default: false): create pipe if not found\nblock_size (uint, default: 5000): buffer size used to write to pipe, Windows only\nka (bool, default: false): keep pipe alive when broken pipe is detected\nmarker (bool, default: false): inject marker upon pipeline flush events \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/pout/"},{"date_scraped_timestamp":1720188067807,"host":"wiki.gpac.io","page_title":"probe - GPAC wiki","text":"\n \n \n \n \n \n \nProbe source¶\nRegister name used to load filter: probe\nThis filter is not checked during graph resolution and needs explicit loading. \nThe Probe filter is used by applications (typically MP4Box) to query demultiplexed PIDs (audio, video, ...) available in a source chain. \nThe filter outputs the number of input PIDs in the file specified by log.\nIt is up to the app developer to query input PIDs of the prober and take appropriated decisions. \nOptions¶\nlog (str, default: stdout, Enum: _any|stderr|stdout|GLOG|null): set probe log filename to print number of streams \n_any: target file path and name \nstderr: dump to stderr \nstdout: dump to stdout \nGLOG: use GPAC logs app@info \nnull: silent mode \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/probe/"},{"date_scraped_timestamp":1720188102417,"host":"wiki.gpac.io","page_title":"probe - GPAC wiki","text":"\n \n \n \n \n \n \nProbe source¶\nRegister name used to load filter: probe\nThis filter is not checked during graph resolution and needs explicit loading. \nThe Probe filter is used by applications (typically MP4Box) to query demultiplexed PIDs (audio, video, ...) available in a source chain. \nThe filter outputs the number of input PIDs in the file specified by log.\nIt is up to the app developer to query input PIDs of the prober and take appropriated decisions. \nOptions¶\nlog (str, default: stdout, Enum: _any|stderr|stdout|GLOG|null): set probe log filename to print number of streams \n_any: target file path and name \nstderr: dump to stderr \nstdout: dump to stdout \nGLOG: use GPAC logs app@info \nnull: silent mode \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/probe/?q="},{"date_scraped_timestamp":1720188095792,"host":"wiki.gpac.io","page_title":"reframer - GPAC wiki","text":"\n \n \n \n \n \n \nRegister name used to load filter: reframer\nThis filter is not checked during graph resolution and needs explicit loading.\nFilters of this class can connect to each-other. \nThis filter provides various tools on inputs: \nensure reframing (1 packet = 1 Access Unit) \noptionally force decoding \nreal-time regulation \npacket filtering based on SAP types or frame numbers \ntime-range extraction and splitting \nThis filter forces input PIDs to be properly framed (1 packet = 1 Access Unit).\nIt is typically needed to force remultiplexing in file to file operations when source and destination files use the same format. \nSAP filtering¶\nThe filter can remove packets based on their SAP types using saps option.\nFor example, this can be used to extract only the key frame (SAP 1,2,3) of a video to create a trick mode version. \nFrame filtering¶\nThis filter can keep only specific Access Units of the source using frames option.\nFor example, this can be used to extract only specific key pictures of a video to create a HEIF collection. \nFrame decoding¶\nThis filter can force input media streams to be decoded using the raw option.\nExample\ngpac -i m.mp4 reframer:raw=av [dst]\nReal-time Regulation¶\nThe filter can perform real-time regulation of input packets, based on their timescale and timestamps.\nFor example to simulate a live DASH:\nExample\ngpac -i m.mp4 reframer:rt=on -o live.mpd:dynamic\nThe filter can perform time range extraction of the source using xs and xe options.\nThe formats allowed for times specifiers are: \n'T'H:M:S, 'T'M:S: specify time in hours, minutes, seconds \n'T'H:M:S.MS, 'T'M:S.MS, 'T'S.MS: specify time in hours, minutes, seconds and milliseconds \nINT, FLOAT, NUM/DEN: specify time in seconds (number or fraction) \n'D'INT, 'D'FLOAT, 'D'NUM/DEN: specify end time as offset to start time in seconds (number or fraction) - only valid for xe \n'F'NUM: specify time as frame number, 1 being first \nXML DateTime: specify absolute UTC time \nIn this mode, the timestamps are rewritten to form a continuous timeline, unless xots is set.\nWhen multiple ranges are given, the filter will try to seek if needed and supported by source. \nExample\ngpac -i m.mp4 reframer:xs=T00:00:10,T00:01:10,T00:02:00:xe=T00:00:20,T00:01:20 [dst]\nThis will extract the time ranges [10s,20s], [1m10s,1m20s] and all media starting from 2m \nIf no end range is found for a given start range: \nif a following start range is set, the end range is set to this next start \notherwise, the end range is open \nExample\ngpac -i m.mp4 reframer:xs=0,10,25:xe=5,20 [dst]\nThis will extract the time ranges [0s,5s], [10s,20s] and all media starting from 25s\nExample\ngpac -i m.mp4 reframer:xs=0,10,25 [dst]\nThis will extract the time ranges [0s,10s], [10s,25s] and all media starting from 25s \nIt is possible to signal range boundaries in output packets using splitrange.\nThis will expose on the first packet of each range in each PID the following properties: \nFileNumber: starting at 1 for the first range, to be used as replacement for $num$ in templates \nFileSuffix: corresponding to StartRange_EndRange or StartRange for open ranges, to be used as replacement for $FS$ in templates \nExample\ngpac -i m.mp4 reframer:xs=T00:00:10,T00:01:10:xe=T00:00:20:splitrange -o dump_$FS$.264 [dst]\nThis will create two output files dump_T00.00.10_T00.02.00.264 and dump_T00.01.10.264.\nNote: The : and / characters are replaced by . in FileSuffix property. \nIt is possible to modify PID properties per range using props. Each set of property must be specified using the active separator set. \nWarning: The option must be escaped using double separators in order to be parsed properly. \nExample\ngpac -i m.mp4 reframer:xs=0,30::props=#Period=P1,#Period=P2:#foo=bar [dst]\nThis will assign to output PIDs \nduring the range [0,30]: property Period to P1 \nduring the range [30, end]: properties Period to P2 and property foo to bar \nFor uncompressed audio PIDs, input frame will be split to closest audio sample number. \nWhen xround is set to seek, the following applies: \na single range shall be specified \nthe first I-frame preceding or matching the range start is used as split point \nall packets before range start are marked as seek points \npackets overlapping range start are forwarded with a SkipBegin property set to the amount of media to skip \npackets overlapping range end are forwarded with an adjusted duration to match the range end \nThis mode is typically used to extract a range in a frame/sample accurate way, rather than a GOP-aligned way. \nWhen xround is not set to seek, compressed audio streams will still use seek mode.\nConsequently, these streams will have modified edit lists in ISOBMFF which might not be properly handled by players.\nThis can be avoided using no_audio_seek, but this will introduce audio delay. \nThe filter can perform range extraction based on UTC time rather than media time. In this mode, the end time must be: \na UTC date: range extraction will stop after this date \na time in second: range extraction will stop after the specified duration \nThe UTC reference is specified using utc_ref.\nIf UTC signal from media source is used, the filter will probe for utc_probe before considering the source has no UTC signal. \nThe properties SenderNTP and, if absent, UTC of source packets are checked for establishing the UTC reference. \nOther split actions¶\nThe filter can perform splitting of the source using xs option.\nThe additional formats allowed for xs option are: \nSAP: split source at each SAP/RAP \nDVAL: split source by chunks of VAL seconds \nDNUM/DEN: split source by chunks of NUM/DEN seconds \nSVAL: split source by chunks of estimated size VAL bytes (can use property multipliers, e.g. m) \nNote: In these modes, splitrange and xadjust are implicitly set. \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results\nrt (enum, default: off, updatable): real-time regulation mode of input \noff: disables real-time regulation \non: enables real-time regulation, one clock per PID \nsync: enables real-time regulation one clock for all PIDs \nsaps (uintl, Enum: 0|1|2|3|4, updatable): list of SAP types (0,1,2,3,4) to forward, other packets are dropped (forwarding only sap 0 will break the decoding) \nrefs (bool, default: false, updatable): forward only frames used as reference frames, if indicated in the input stream\nspeed (dbl, default: 0.0, updatable): speed for real-time regulation mode, a value of 0 uses speed from play commands\nraw (enum, default: no): force input AV streams to be in raw format \nno: do not force decoding of inputs \nav: force decoding of audio and video inputs \na: force decoding of audio inputs \nv: force decoding of video inputs \nframes (sintl, updatable): drop all except listed frames (first being 1). A negative value -V keeps only first frame every V frames\nxs (strl): extraction start time(s)\nxe (strl): extraction end time(s). If less values than start times, the last time interval extracted is an open range\nxround (enum, default: before): adjust start time of extraction range to I-frame \nbefore: use first I-frame preceding or matching range start \nseek: see filter help \nafter: use first I-frame (if any) following or matching range start \nclosest: use I-frame closest to range start \nxadjust (bool, default: false): adjust end time of extraction range to be before next I-frame\nxots (bool, default: false): keep original timestamps after extraction\nnosap (bool, default: false): do not cut at SAP when extracting range (may result in broken streams)\nsplitrange (bool, default: false): signal file boundary at each extraction first packet for template-base file generation\nseeksafe (dbl, default: 10.0): rewind play requests by given seconds (to make sure the I-frame preceding start is catched)\ntcmdrw (bool, default: true): rewrite TCMD samples when splitting\nprops (strl): extra output PID properties per extraction range\nno_audio_seek (bool, default: false): disable seek mode on audio streams (no change of priming duration)\nprobe_ref (bool, default: false): allow extracted range to be longer in case of B-frames with reference frames presented outside of range\nutc_ref (enum, default: any): set reference mode for UTC range extraction \nlocal: use UTC of local host \nany: use UTC of media, or UTC of local host if not found in media after probing time \nmedia: use UTC of media (abort if none found) \nutc_probe (uint, default: 5000): timeout in milliseconds to try to acquire UTC reference from media\ncopy (bool, default: false, updatable): try copying frame interface into packets\ncues (enum, default: no, updatable): cue filtering mode \nno: do no filter frames based on cue info \nsegs: only forward frames marked as segment start \nfrags: only forward frames marked as fragment start \nrmseek (bool, default: false, updatable): remove seek flag of all sent packets \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/reframer/?q="},{"date_scraped_timestamp":1720188073450,"host":"wiki.gpac.io","page_title":"reframer - GPAC wiki","text":"\n \n \n \n \n \n \nRegister name used to load filter: reframer\nThis filter is not checked during graph resolution and needs explicit loading.\nFilters of this class can connect to each-other. \nThis filter provides various tools on inputs: \nensure reframing (1 packet = 1 Access Unit) \noptionally force decoding \nreal-time regulation \npacket filtering based on SAP types or frame numbers \ntime-range extraction and splitting \nThis filter forces input PIDs to be properly framed (1 packet = 1 Access Unit).\nIt is typically needed to force remultiplexing in file to file operations when source and destination files use the same format. \nSAP filtering¶\nThe filter can remove packets based on their SAP types using saps option.\nFor example, this can be used to extract only the key frame (SAP 1,2,3) of a video to create a trick mode version. \nFrame filtering¶\nThis filter can keep only specific Access Units of the source using frames option.\nFor example, this can be used to extract only specific key pictures of a video to create a HEIF collection. \nFrame decoding¶\nThis filter can force input media streams to be decoded using the raw option.\nExample\ngpac -i m.mp4 reframer:raw=av [dst]\nReal-time Regulation¶\nThe filter can perform real-time regulation of input packets, based on their timescale and timestamps.\nFor example to simulate a live DASH:\nExample\ngpac -i m.mp4 reframer:rt=on -o live.mpd:dynamic\nThe filter can perform time range extraction of the source using xs and xe options.\nThe formats allowed for times specifiers are: \n'T'H:M:S, 'T'M:S: specify time in hours, minutes, seconds \n'T'H:M:S.MS, 'T'M:S.MS, 'T'S.MS: specify time in hours, minutes, seconds and milliseconds \nINT, FLOAT, NUM/DEN: specify time in seconds (number or fraction) \n'D'INT, 'D'FLOAT, 'D'NUM/DEN: specify end time as offset to start time in seconds (number or fraction) - only valid for xe \n'F'NUM: specify time as frame number, 1 being first \nXML DateTime: specify absolute UTC time \nIn this mode, the timestamps are rewritten to form a continuous timeline, unless xots is set.\nWhen multiple ranges are given, the filter will try to seek if needed and supported by source. \nExample\ngpac -i m.mp4 reframer:xs=T00:00:10,T00:01:10,T00:02:00:xe=T00:00:20,T00:01:20 [dst]\nThis will extract the time ranges [10s,20s], [1m10s,1m20s] and all media starting from 2m \nIf no end range is found for a given start range: \nif a following start range is set, the end range is set to this next start \notherwise, the end range is open \nExample\ngpac -i m.mp4 reframer:xs=0,10,25:xe=5,20 [dst]\nThis will extract the time ranges [0s,5s], [10s,20s] and all media starting from 25s\nExample\ngpac -i m.mp4 reframer:xs=0,10,25 [dst]\nThis will extract the time ranges [0s,10s], [10s,25s] and all media starting from 25s \nIt is possible to signal range boundaries in output packets using splitrange.\nThis will expose on the first packet of each range in each PID the following properties: \nFileNumber: starting at 1 for the first range, to be used as replacement for $num$ in templates \nFileSuffix: corresponding to StartRange_EndRange or StartRange for open ranges, to be used as replacement for $FS$ in templates \nExample\ngpac -i m.mp4 reframer:xs=T00:00:10,T00:01:10:xe=T00:00:20:splitrange -o dump_$FS$.264 [dst]\nThis will create two output files dump_T00.00.10_T00.02.00.264 and dump_T00.01.10.264.\nNote: The : and / characters are replaced by . in FileSuffix property. \nIt is possible to modify PID properties per range using props. Each set of property must be specified using the active separator set. \nWarning: The option must be escaped using double separators in order to be parsed properly. \nExample\ngpac -i m.mp4 reframer:xs=0,30::props=#Period=P1,#Period=P2:#foo=bar [dst]\nThis will assign to output PIDs \nduring the range [0,30]: property Period to P1 \nduring the range [30, end]: properties Period to P2 and property foo to bar \nFor uncompressed audio PIDs, input frame will be split to closest audio sample number. \nWhen xround is set to seek, the following applies: \na single range shall be specified \nthe first I-frame preceding or matching the range start is used as split point \nall packets before range start are marked as seek points \npackets overlapping range start are forwarded with a SkipBegin property set to the amount of media to skip \npackets overlapping range end are forwarded with an adjusted duration to match the range end \nThis mode is typically used to extract a range in a frame/sample accurate way, rather than a GOP-aligned way. \nWhen xround is not set to seek, compressed audio streams will still use seek mode.\nConsequently, these streams will have modified edit lists in ISOBMFF which might not be properly handled by players.\nThis can be avoided using no_audio_seek, but this will introduce audio delay. \nThe filter can perform range extraction based on UTC time rather than media time. In this mode, the end time must be: \na UTC date: range extraction will stop after this date \na time in second: range extraction will stop after the specified duration \nThe UTC reference is specified using utc_ref.\nIf UTC signal from media source is used, the filter will probe for utc_probe before considering the source has no UTC signal. \nThe properties SenderNTP and, if absent, UTC of source packets are checked for establishing the UTC reference. \nOther split actions¶\nThe filter can perform splitting of the source using xs option.\nThe additional formats allowed for xs option are: \nSAP: split source at each SAP/RAP \nDVAL: split source by chunks of VAL seconds \nDNUM/DEN: split source by chunks of NUM/DEN seconds \nSVAL: split source by chunks of estimated size VAL bytes (can use property multipliers, e.g. m) \nNote: In these modes, splitrange and xadjust are implicitly set. \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results\nrt (enum, default: off, updatable): real-time regulation mode of input \noff: disables real-time regulation \non: enables real-time regulation, one clock per PID \nsync: enables real-time regulation one clock for all PIDs \nsaps (uintl, Enum: 0|1|2|3|4, updatable): list of SAP types (0,1,2,3,4) to forward, other packets are dropped (forwarding only sap 0 will break the decoding) \nrefs (bool, default: false, updatable): forward only frames used as reference frames, if indicated in the input stream\nspeed (dbl, default: 0.0, updatable): speed for real-time regulation mode, a value of 0 uses speed from play commands\nraw (enum, default: no): force input AV streams to be in raw format \nno: do not force decoding of inputs \nav: force decoding of audio and video inputs \na: force decoding of audio inputs \nv: force decoding of video inputs \nframes (sintl, updatable): drop all except listed frames (first being 1). A negative value -V keeps only first frame every V frames\nxs (strl): extraction start time(s)\nxe (strl): extraction end time(s). If less values than start times, the last time interval extracted is an open range\nxround (enum, default: before): adjust start time of extraction range to I-frame \nbefore: use first I-frame preceding or matching range start \nseek: see filter help \nafter: use first I-frame (if any) following or matching range start \nclosest: use I-frame closest to range start \nxadjust (bool, default: false): adjust end time of extraction range to be before next I-frame\nxots (bool, default: false): keep original timestamps after extraction\nnosap (bool, default: false): do not cut at SAP when extracting range (may result in broken streams)\nsplitrange (bool, default: false): signal file boundary at each extraction first packet for template-base file generation\nseeksafe (dbl, default: 10.0): rewind play requests by given seconds (to make sure the I-frame preceding start is catched)\ntcmdrw (bool, default: true): rewrite TCMD samples when splitting\nprops (strl): extra output PID properties per extraction range\nno_audio_seek (bool, default: false): disable seek mode on audio streams (no change of priming duration)\nprobe_ref (bool, default: false): allow extracted range to be longer in case of B-frames with reference frames presented outside of range\nutc_ref (enum, default: any): set reference mode for UTC range extraction \nlocal: use UTC of local host \nany: use UTC of media, or UTC of local host if not found in media after probing time \nmedia: use UTC of media (abort if none found) \nutc_probe (uint, default: 5000): timeout in milliseconds to try to acquire UTC reference from media\ncopy (bool, default: false, updatable): try copying frame interface into packets\ncues (enum, default: no, updatable): cue filtering mode \nno: do no filter frames based on cue info \nsegs: only forward frames marked as segment start \nfrags: only forward frames marked as fragment start \nrmseek (bool, default: false, updatable): remove seek flag of all sent packets \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/reframer/"},{"date_scraped_timestamp":1720188119745,"host":"wiki.gpac.io","page_title":"resample - GPAC wiki","text":"\n \n \n \n \n \n \nAudio resampler¶\nRegister name used to load filter: resample\nThis filter may be automatically loaded during graph resolution.\nFilters of this class can connect to each-other. \nThis filter resamples raw audio to a target sample rate, number of channels or audio format. \nOptions¶\noch (uint, default: 0): desired number of output audio channels (0 for auto)\nosr (uint, default: 0): desired sample rate of output audio (0 for auto)\nosfmt (afmt, default: none): desired sample format of output audio (none for auto)\nolayout (alay, Enum: mono|stereo|3/0.0|3/1.0|3/2.0|3/2.1|5/2.1|1+1|2/1.0|2/2.0|3/3.1|3/4.1|11/11.2|5/2.1|5/5.2|5/4.1|6/5.1|6/7.1|5/6.1|7/6.1): desired CICP layout of output audio (null for auto) \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/resample/"},{"date_scraped_timestamp":1720187952205,"host":"wiki.gpac.io","page_title":"restamp - GPAC wiki","text":"\n \n \n \n \n \n \nPacket timestamp rewriter¶\nRegister name used to load filter: restamp\nThis filter is not checked during graph resolution and needs explicit loading.\nFilters of this class can connect to each-other. \nThis filter rewrites timing (offsets and rate) of packets. \nThe delays (global or per stream class) can be either positive (stream presented later) or negative (stream presented sooner). \nThe specified fps can be either 0, positive or negative. \nif 0 or if the stream is audio, stream rate is not modified. \notherwise if negative, stream rate is multiplied by -fps.num/fps.den. \notherwise if positive and the stream is not video, stream rate is not modified. \notherwise (video PID), constant frame rate is assumed and: \nif rawv=no, video frame rate is changed to the specified rate (speed-up or slow-down). \nif rawv=force, input video stream is decoded and video frames are dropped/copied to match the new rate. \nif rawv=dyn, input video stream is decoded if not all-intra and video frames are dropped/copied to match the new rate. \nNote: frames are simply copied or dropped with no motion compensation. \nWhen align is not 0, if the difference between two consecutive timestamps is greater than the specified threshold, the new timestamp \nis set to the last computed timestamp plus the minimum packet duration for the stream. \nOptions¶\nfps (frac, default: 0/1): target fps\ndelay (frac, default: 0/1, updatable): delay to add to all streams\ndelay_v (frac, default: 0/1, updatable): delay to add to video streams\ndelay_a (frac, default: 0/1, updatable): delay to add to audio streams\ndelay_t (frac, default: 0/1, updatable): delay to add to text streams\ndelay_o (frac, default: 0/1, updatable): delay to add to other streams\nrawv (enum, default: no): copy video frames \nno: no raw frame copy/drop \nforce: force decoding all video streams \ndyn: decoding video streams if not all intra \ntsinit (lfrac, default: -1/1): initial timestamp to resync to, negative values disables resync\nalign (uint, default: 0): timestamp alignment threshold (0 disables alignment) - see filter help\nreorder (bool, default: false): reorder input packets by CTS (resulting PID may fail decoding) \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/restamp/"},{"date_scraped_timestamp":1720188047485,"host":"wiki.gpac.io","page_title":"rfadts - GPAC wiki","text":"\n \n \n \n \n \n \nADTS reframer¶\nRegister name used to load filter: rfadts\nThis filter may be automatically loaded during graph resolution. \nThis filter parses AAC files/data and outputs corresponding audio PID and frames. \nOptions¶\nframe_size (uint, default: 1024): size of AAC frame in audio samples\nindex (dbl, default: 1.0): indexing window length\novsbr (bool, default: false): force oversampling SBR (does not multiply timescales by 2)\nsbr (enum, default: no): set SBR signaling \nno: no SBR signaling at all \nimp: backward-compatible SBR signaling (audio signaled as AAC-LC) \nexp: explicit SBR signaling (audio signaled as AAC-SBR) \nps (enum, default: no): set PS signaling \nno: no PS signaling at all \nimp: backward-compatible PS signaling (audio signaled as AAC-LC) \nexp: explicit PS signaling (audio signaled as AAC-PS) \nexpart (bool, default: false): expose pictures as a dedicated video PID\naacchcfg (sint, default: 0): set AAC channel configuration to this value if missing from ADTS header, use negative value to always override \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfadts/"},{"date_scraped_timestamp":1720188058775,"host":"wiki.gpac.io","page_title":"rfadts - GPAC wiki","text":"\n \n \n \n \n \n \nADTS reframer¶\nRegister name used to load filter: rfadts\nThis filter may be automatically loaded during graph resolution. \nThis filter parses AAC files/data and outputs corresponding audio PID and frames. \nOptions¶\nframe_size (uint, default: 1024): size of AAC frame in audio samples\nindex (dbl, default: 1.0): indexing window length\novsbr (bool, default: false): force oversampling SBR (does not multiply timescales by 2)\nsbr (enum, default: no): set SBR signaling \nno: no SBR signaling at all \nimp: backward-compatible SBR signaling (audio signaled as AAC-LC) \nexp: explicit SBR signaling (audio signaled as AAC-SBR) \nps (enum, default: no): set PS signaling \nno: no PS signaling at all \nimp: backward-compatible PS signaling (audio signaled as AAC-LC) \nexp: explicit PS signaling (audio signaled as AAC-PS) \nexpart (bool, default: false): expose pictures as a dedicated video PID\naacchcfg (sint, default: 0): set AAC channel configuration to this value if missing from ADTS header, use negative value to always override \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfadts/?q="},{"date_scraped_timestamp":1720187953516,"host":"wiki.gpac.io","page_title":"rfimg - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJPG/J2K/PNG/BMP reframer¶\nRegister name used to load filter: rfimg\nThis filter may be automatically loaded during graph resolution. \nThis filter parses JPG/J2K/PNG/BMP files/data and outputs corresponding visual PID and frames. \nThe following extensions for PNG change the pixel format for RGBA images: \npngd: use RGB+depth map pixel format \npngds: use RGB+depth(7bits)+shape(MSB of alpha channel) pixel format \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfimg/?q="},{"date_scraped_timestamp":1720187927566,"host":"wiki.gpac.io","page_title":"rfimg - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nJPG/J2K/PNG/BMP reframer¶\nRegister name used to load filter: rfimg\nThis filter may be automatically loaded during graph resolution. \nThis filter parses JPG/J2K/PNG/BMP files/data and outputs corresponding visual PID and frames. \nThe following extensions for PNG change the pixel format for RGBA images: \npngd: use RGB+depth map pixel format \npngds: use RGB+depth(7bits)+shape(MSB of alpha channel) pixel format \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfimg/"},{"date_scraped_timestamp":1720188044170,"host":"wiki.gpac.io","page_title":"rfmhas - GPAC wiki","text":"\n \n \n \n \n \n \nMPEH-H Audio Stream reframer¶\nRegister name used to load filter: rfmhas\nThis filter may be automatically loaded during graph resolution. \nThis filter parses MHAS files/data and outputs corresponding audio PID and frames.\nBy default, the filter expects a MHAS stream with SYNC packets set, otherwise tune-in will fail. Using nosync=false can help parsing bitstreams with no SYNC packets.\nThe default behavior is to dispatch a framed MHAS bitstream. To demultiplex into a raw MPEG-H Audio, use mpha. \nOptions¶\nindex (dbl, default: 1.0): indexing window length\nmpha (bool, default: false): demultiplex MHAS and only forward audio frames\npcksync (uint, default: 4): number of unknown packets to tolerate before considering sync is lost\nnosync (bool, default: true): initial sync state \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfmhas/?q="},{"date_scraped_timestamp":1720187922275,"host":"wiki.gpac.io","page_title":"rfmhas - GPAC wiki","text":"\n \n \n \n \n \n \nMPEH-H Audio Stream reframer¶\nRegister name used to load filter: rfmhas\nThis filter may be automatically loaded during graph resolution. \nThis filter parses MHAS files/data and outputs corresponding audio PID and frames.\nBy default, the filter expects a MHAS stream with SYNC packets set, otherwise tune-in will fail. Using nosync=false can help parsing bitstreams with no SYNC packets.\nThe default behavior is to dispatch a framed MHAS bitstream. To demultiplex into a raw MPEG-H Audio, use mpha. \nOptions¶\nindex (dbl, default: 1.0): indexing window length\nmpha (bool, default: false): demultiplex MHAS and only forward audio frames\npcksync (uint, default: 4): number of unknown packets to tolerate before considering sync is lost\nnosync (bool, default: true): initial sync state \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfmhas/"},{"date_scraped_timestamp":1720187966583,"host":"wiki.gpac.io","page_title":"rfmpgvid - GPAC wiki","text":"\n \n \n \n \n \n \nM1V/M2V/M4V reframer¶\nRegister name used to load filter: rfmpgvid\nThis filter may be automatically loaded during graph resolution. \nThis filter parses MPEG-1/2 and MPEG-4 part 2 video files/data and outputs corresponding video PID and frames.\nNote: The filter uses negative CTS offsets: CTS is correct, but some frames may have DTS greater than CTS. \nOptions¶\nfps (frac, default: 0/1000): import frame rate (0 default to FPS from bitstream or 25 Hz)\nindex (dbl, default: -1.0): indexing window length. If 0, bitstream is not probed for duration. A negative value skips the indexing if the source file is larger than 20M (slows down importers) unless a play with start range > 0 is issued\nvfr (bool, default: false): set variable frame rate import\nimporter (bool, default: false): compatibility with old importer, displays import results\nnotime (bool, default: false): ignore input timestamps, rebuild from 0 \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfmpgvid/"},{"date_scraped_timestamp":1720188000542,"host":"wiki.gpac.io","page_title":"rfnalu - GPAC wiki","text":"\n \n \n \n \n \n \nAVC/HEVC reframer¶\nRegister name used to load filter: rfnalu\nThis filter may be automatically loaded during graph resolution. \nThis filter parses AVC|H264 and HEVC files/data and outputs corresponding video PID and frames.\nThis filter produces ISOBMFF-compatible output: start codes are removed, NALU length field added and avcC/hvcC config created.\nNote: The filter uses negative CTS offsets: CTS is correct, but some frames may have DTS greater than CTS. \nOptions¶\nfps (frac, default: 0/1000): import frame rate (0 default to FPS from bitstream or 25 Hz)\nindex (dbl, default: -1.0): indexing window length. If 0, bitstream is not probed for duration. A negative value skips the indexing if the source file is larger than 20M (slows down importers) unless a play with start range > 0 is issued\nexplicit (bool, default: false): use explicit layered (SVC/LHVC) import\nstrict_poc (enum, default: off): delay frame output of an entire GOP to ensure CTS info is correct when POC suddenly changes \noff: disable GOP buffering \non: enable GOP buffering, assuming no error in POC \nerror: enable GOP buffering and try to detect lost frames \nnosei (bool, default: false): remove all sei messages\nnosvc (bool, default: false): remove all SVC/MVC/LHVC data\nnovpsext (bool, default: false): remove all VPS extensions\nimporter (bool, default: false): compatibility with old importer, displays import results\nnal_length (uint, default: 4): set number of bytes used to code length field: 1, 2 or 4\nsubsamples (bool, default: false): import subsamples information\ndeps (bool, default: false): import sample dependency information\nseirw (bool, default: true): rewrite AVC sei messages for ISOBMFF constraints\naudelim (bool, default: false): keep Access Unit delimiter in payload\nnotime (bool, default: false): ignore input timestamps, rebuild from 0\ndv_mode (enum, default: auto): signaling for DolbyVision \nnone: never signal DV profile \nauto: signal DV profile if RPU or EL are found \nclean: do not signal and remove RPU and EL NAL units \nsingle: signal DV profile if RPU are found and remove EL NAL units \ndv_profile (uint, default: 0): profile for DolbyVision (currently defined profiles are 4, 5, 7, 8, 9), 0 for auto-detect\ndv_compatid (enum, default: auto): cross-compatibility ID for DolbyVision \nauto: auto-detect \nnone: no cross-compatibility \nhdr10: CTA HDR10, as specified by EBU TR 03 \nbt709: SDR BT.709 \nhlg709: HLG BT.709 gamut in ITU-R BT.2020 \nhlg2100: HLG BT.2100 gamut in ITU-R BT.2020 \nbt2020: SDR BT.2020 \nbrd: Ultra HD Blu-ray Disc HDR \nbsdbg (enum, default: off): debug NAL parsing in media@debug logs \noff: not enabled \non: enabled \nfull: enable with number of bits dumped \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfnalu/"},{"date_scraped_timestamp":1720188022441,"host":"wiki.gpac.io","page_title":"rfnalu - GPAC wiki","text":"\n \n \n \n \n \n \nAVC/HEVC reframer¶\nRegister name used to load filter: rfnalu\nThis filter may be automatically loaded during graph resolution. \nThis filter parses AVC|H264 and HEVC files/data and outputs corresponding video PID and frames.\nThis filter produces ISOBMFF-compatible output: start codes are removed, NALU length field added and avcC/hvcC config created.\nNote: The filter uses negative CTS offsets: CTS is correct, but some frames may have DTS greater than CTS. \nOptions¶\nfps (frac, default: 0/1000): import frame rate (0 default to FPS from bitstream or 25 Hz)\nindex (dbl, default: -1.0): indexing window length. If 0, bitstream is not probed for duration. A negative value skips the indexing if the source file is larger than 20M (slows down importers) unless a play with start range > 0 is issued\nexplicit (bool, default: false): use explicit layered (SVC/LHVC) import\nstrict_poc (enum, default: off): delay frame output of an entire GOP to ensure CTS info is correct when POC suddenly changes \noff: disable GOP buffering \non: enable GOP buffering, assuming no error in POC \nerror: enable GOP buffering and try to detect lost frames \nnosei (bool, default: false): remove all sei messages\nnosvc (bool, default: false): remove all SVC/MVC/LHVC data\nnovpsext (bool, default: false): remove all VPS extensions\nimporter (bool, default: false): compatibility with old importer, displays import results\nnal_length (uint, default: 4): set number of bytes used to code length field: 1, 2 or 4\nsubsamples (bool, default: false): import subsamples information\ndeps (bool, default: false): import sample dependency information\nseirw (bool, default: true): rewrite AVC sei messages for ISOBMFF constraints\naudelim (bool, default: false): keep Access Unit delimiter in payload\nnotime (bool, default: false): ignore input timestamps, rebuild from 0\ndv_mode (enum, default: auto): signaling for DolbyVision \nnone: never signal DV profile \nauto: signal DV profile if RPU or EL are found \nclean: do not signal and remove RPU and EL NAL units \nsingle: signal DV profile if RPU are found and remove EL NAL units \ndv_profile (uint, default: 0): profile for DolbyVision (currently defined profiles are 4, 5, 7, 8, 9), 0 for auto-detect\ndv_compatid (enum, default: auto): cross-compatibility ID for DolbyVision \nauto: auto-detect \nnone: no cross-compatibility \nhdr10: CTA HDR10, as specified by EBU TR 03 \nbt709: SDR BT.709 \nhlg709: HLG BT.709 gamut in ITU-R BT.2020 \nhlg2100: HLG BT.2100 gamut in ITU-R BT.2020 \nbt2020: SDR BT.2020 \nbrd: Ultra HD Blu-ray Disc HDR \nbsdbg (enum, default: off): debug NAL parsing in media@debug logs \noff: not enabled \non: enabled \nfull: enable with number of bits dumped \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rfnalu/?q="},{"date_scraped_timestamp":1720187990669,"host":"wiki.gpac.io","page_title":"rftruehd - GPAC wiki","text":"\n \n \n \n \n \n \nTrueHD reframer¶\nRegister name used to load filter: rftruehd\nThis filter may be automatically loaded during graph resolution. \nThis filter parses Dolby TrueHD files/data and outputs corresponding audio PID and frames. \nOptions¶\nindex (dbl, default: 1.0): indexing window length\nauxac3 (bool, default: false): expose auxiliary AC-3 stream if present \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rftruehd/"},{"date_scraped_timestamp":1720187990669,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"rftruehd - GPAC wiki","text":"\n \n \n \n \n \n \nTrueHD reframer¶\nRegister name used to load filter: rftruehd\nThis filter may be automatically loaded during graph resolution. \nThis filter parses Dolby TrueHD files/data and outputs corresponding audio PID and frames. \nOptions¶\nindex (dbl, default: 1.0): indexing window length\nauxac3 (bool, default: false): expose auxiliary AC-3 stream if present \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rftruehd/"},{"date_scraped_timestamp":1720188110895,"host":"wiki.gpac.io","page_title":"rftruehd - GPAC wiki","text":"\n \n \n \n \n \n \nTrueHD reframer¶\nRegister name used to load filter: rftruehd\nThis filter may be automatically loaded during graph resolution. \nThis filter parses Dolby TrueHD files/data and outputs corresponding audio PID and frames. \nOptions¶\nindex (dbl, default: 1.0): indexing window length\nauxac3 (bool, default: false): expose auxiliary AC-3 stream if present \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rftruehd/?q="},{"date_scraped_timestamp":1720187964525,"host":"wiki.gpac.io","page_title":"routein - GPAC wiki","text":"\n \n \n \n \n \n \nROUTE input¶\nRegister name used to load filter: routein\nThis filter may be automatically loaded during graph resolution. \nThis filter is a receiver for file delivery over multicast. It currently supports ATSC 3.0, generic ROUTE and DVB-MABR flute. \nATSC 3.0 mode is identified by the URL atsc://. \nGeneric ROUTE mode is identified by the URL route://IP:PORT. \nDVB-MABR mode is identified by the URL mabr://IP:PORT pointing to the bootstrap FLUTE channel carrying the multicast gateway configuration. \nThe filter can work in cached mode, source mode or standalone mode. \nCached mode¶\nThe cached mode is the default filter behavior. It populates GPAC HTTP Cache with the received files, using http://gmcast/serviceN/ as service root, N being the multicast service ID.\nIn cached mode, repeated files are always pushed to cache.\nThe maximum number of media segment objects in cache per service is defined by nbcached; this is a safety used to force object removal in case DASH client timing is wrong and some files are never requested at cache level. \nThe cached MPD is assigned the following headers: \nx-mcast: boolean value, if yes indicates the file comes from a multicast. \nx-mcast-first-seg: string value, indicates the name of the first segment (completely or currently being) retrieved from the broadcast. \nx-mcast-ll: boolean value, if yes indicates that the indicated first segment is currently being received (low latency signaling). \nx-mcast-loop: boolean value, if yes indicates a loop (e.g. pcap replay) in the service has been detected - only checked if cloop is set. \nThe cached files are assigned the following headers: \nx-mcast: boolean value, if yes indicates the file comes from a multicast. \nIf max_segs is set, file deletion event will be triggered in the filter chain. \nSource mode¶\nIn source mode, the filter outputs files on a single output PID of type file. The files are dispatched once fully received, the output PID carries a sequence of complete files. Repeated files are not sent unless requested.\nIf needed, one PID per TSI can be used rather than a single PID. This avoids mixing files of different mime types on the same PID (e.g. HAS manifest and ISOBMFF).\nExample\ngpac -i atsc://gcache=false -o $ServiceID$/$File$:dynext\nThis will grab the files and forward them as output PIDs, consumed by the fout filter. \nIf max_segs is set, file deletion event will be triggered in the filter chain. \nStandalone mode¶\nIn standalone mode, the filter does not produce any output PID and writes received files to the odir directory.\nExample\ngpac -i atsc://:odir=output\nThis will grab the files and write them to output directory. \nIf max_segs is set, old files will be deleted. \nFile Repair¶\nIn case of losses or incomplete segment reception (during tune-in), the files are patched as follows: \nMPEG-2 TS: all lost ranges are adjusted to 188-bytes boundaries, and transformed into NULL TS packets. \nISOBMFF: all top-level boxes are scanned, and incomplete boxes are transformed in free boxes, except mdat kept as is if repair is set to simple. \nIf kc option is set, corrupted files will be kept. If fullseg is not set and files are only partially received, they will be kept. \nInterface setup¶\nOn some systems (OSX), when using VM packet replay, you may need to force multicast routing on your local interface.\nFor ATSC, you will have to do this for the base signaling multicast (224.0.23.60):\nExample\nroute add -net 224.0.23.60/32 -interface vboxnet0\nThen for each multicast service in the multicast:\nExample\nroute add -net 239.255.1.4/32 -interface vboxnet0\nOptions¶\nsrc (cstr): URL of source content\nifce (str): default interface to use for multicast. If NULL, the default system interface will be used\ngcache (bool, default: true): indicate the files should populate GPAC HTTP cache\ntunein (sint, default: -2): service ID to bootstrap on for ATSC 3.0 mode (0 means tune to no service, -1 tune all services -2 means tune on first service found)\nbuffer (uint, default: 0x80000): receive buffer size to use in bytes\ntimeout (uint, default: 5000): timeout in ms after which tunein fails\nnbcached (uint, default: 8): number of segments to keep in cache per service\nkc (bool, default: false): keep corrupted file\nskipr (bool, default: true): skip repeated files (ignored in cache mode)\nstsi (bool, default: false): define one output PID per tsi/serviceID (ignored in cache mode)\nstats (uint, default: 1000): log statistics at the given rate in ms (0 disables stats)\ntsidbg (uint, default: 0): gather only objects with given TSI (debug)\nmax_segs (uint, default: 0): maximum number of segments to keep on disk\nodir (str): output directory for standalone mode\nreorder (bool, default: true): consider packets are not always in order - if false, this will evaluate an LCT object as done when TOI changes\ncloop (bool, default: false): check for loops based on TOI (used for capture replay)\nrtimeout (uint, default: 1000): default timeout in us to wait when gathering out-of-order packets\nfullseg (bool, default: false): only dispatch full segments in cache mode (always true for other modes)\nrepair (enum, default: simple): repair mode for corrupted files \nno: no repair is performed \nsimple: simple repair is performed (incomplete mdat boxes will be kept) \nstrict: incomplete mdat boxes will be lost as well as preceding moof boxes \nfull: HTTP-based repair of all lost packets \nrepair_url (cstr): repair url\nmax_sess (uint, default: 1): max number of concurrent HTTP repair sessions\nllmode (bool, default: true): enable low-latency access\ndynsel (bool, default: true): dynamically enable and disable multicast groups based on their selection state \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/routein/"},{"date_scraped_timestamp":1720188106059,"host":"wiki.gpac.io","page_title":"routein - GPAC wiki","text":"\n \n \n \n \n \n \nROUTE input¶\nRegister name used to load filter: routein\nThis filter may be automatically loaded during graph resolution. \nThis filter is a receiver for file delivery over multicast. It currently supports ATSC 3.0, generic ROUTE and DVB-MABR flute. \nATSC 3.0 mode is identified by the URL atsc://. \nGeneric ROUTE mode is identified by the URL route://IP:PORT. \nDVB-MABR mode is identified by the URL mabr://IP:PORT pointing to the bootstrap FLUTE channel carrying the multicast gateway configuration. \nThe filter can work in cached mode, source mode or standalone mode. \nCached mode¶\nThe cached mode is the default filter behavior. It populates GPAC HTTP Cache with the received files, using http://gmcast/serviceN/ as service root, N being the multicast service ID.\nIn cached mode, repeated files are always pushed to cache.\nThe maximum number of media segment objects in cache per service is defined by nbcached; this is a safety used to force object removal in case DASH client timing is wrong and some files are never requested at cache level. \nThe cached MPD is assigned the following headers: \nx-mcast: boolean value, if yes indicates the file comes from a multicast. \nx-mcast-first-seg: string value, indicates the name of the first segment (completely or currently being) retrieved from the broadcast. \nx-mcast-ll: boolean value, if yes indicates that the indicated first segment is currently being received (low latency signaling). \nx-mcast-loop: boolean value, if yes indicates a loop (e.g. pcap replay) in the service has been detected - only checked if cloop is set. \nThe cached files are assigned the following headers: \nx-mcast: boolean value, if yes indicates the file comes from a multicast. \nIf max_segs is set, file deletion event will be triggered in the filter chain. \nSource mode¶\nIn source mode, the filter outputs files on a single output PID of type file. The files are dispatched once fully received, the output PID carries a sequence of complete files. Repeated files are not sent unless requested.\nIf needed, one PID per TSI can be used rather than a single PID. This avoids mixing files of different mime types on the same PID (e.g. HAS manifest and ISOBMFF).\nExample\ngpac -i atsc://gcache=false -o $ServiceID$/$File$:dynext\nThis will grab the files and forward them as output PIDs, consumed by the fout filter. \nIf max_segs is set, file deletion event will be triggered in the filter chain. \nStandalone mode¶\nIn standalone mode, the filter does not produce any output PID and writes received files to the odir directory.\nExample\ngpac -i atsc://:odir=output\nThis will grab the files and write them to output directory. \nIf max_segs is set, old files will be deleted. \nFile Repair¶\nIn case of losses or incomplete segment reception (during tune-in), the files are patched as follows: \nMPEG-2 TS: all lost ranges are adjusted to 188-bytes boundaries, and transformed into NULL TS packets. \nISOBMFF: all top-level boxes are scanned, and incomplete boxes are transformed in free boxes, except mdat kept as is if repair is set to simple. \nIf kc option is set, corrupted files will be kept. If fullseg is not set and files are only partially received, they will be kept. \nInterface setup¶\nOn some systems (OSX), when using VM packet replay, you may need to force multicast routing on your local interface.\nFor ATSC, you will have to do this for the base signaling multicast (224.0.23.60):\nExample\nroute add -net 224.0.23.60/32 -interface vboxnet0\nThen for each multicast service in the multicast:\nExample\nroute add -net 239.255.1.4/32 -interface vboxnet0\nOptions¶\nsrc (cstr): URL of source content\nifce (str): default interface to use for multicast. If NULL, the default system interface will be used\ngcache (bool, default: true): indicate the files should populate GPAC HTTP cache\ntunein (sint, default: -2): service ID to bootstrap on for ATSC 3.0 mode (0 means tune to no service, -1 tune all services -2 means tune on first service found)\nbuffer (uint, default: 0x80000): receive buffer size to use in bytes\ntimeout (uint, default: 5000): timeout in ms after which tunein fails\nnbcached (uint, default: 8): number of segments to keep in cache per service\nkc (bool, default: false): keep corrupted file\nskipr (bool, default: true): skip repeated files (ignored in cache mode)\nstsi (bool, default: false): define one output PID per tsi/serviceID (ignored in cache mode)\nstats (uint, default: 1000): log statistics at the given rate in ms (0 disables stats)\ntsidbg (uint, default: 0): gather only objects with given TSI (debug)\nmax_segs (uint, default: 0): maximum number of segments to keep on disk\nodir (str): output directory for standalone mode\nreorder (bool, default: true): consider packets are not always in order - if false, this will evaluate an LCT object as done when TOI changes\ncloop (bool, default: false): check for loops based on TOI (used for capture replay)\nrtimeout (uint, default: 1000): default timeout in us to wait when gathering out-of-order packets\nfullseg (bool, default: false): only dispatch full segments in cache mode (always true for other modes)\nrepair (enum, default: simple): repair mode for corrupted files \nno: no repair is performed \nsimple: simple repair is performed (incomplete mdat boxes will be kept) \nstrict: incomplete mdat boxes will be lost as well as preceding moof boxes \nfull: HTTP-based repair of all lost packets \nrepair_url (cstr): repair url\nmax_sess (uint, default: 1): max number of concurrent HTTP repair sessions\nllmode (bool, default: true): enable low-latency access\ndynsel (bool, default: true): dynamically enable and disable multicast groups based on their selection state \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/routein/?q="},{"date_scraped_timestamp":1720187981978,"host":"wiki.gpac.io","page_title":"rtpin - GPAC wiki","text":"\n \n \n \n \n \n \nRTP/RTSP/SDP input¶\nRegister name used to load filter: rtpin\nThis filter may be automatically loaded during graph resolution. \nThis filter handles SDP/RTSP/RTP input reading. It supports: \nSDP file reading \nRTP direct url through rtp:// protocol scheme \nRTSP session processing through rtsp:// and satip:// protocol schemes \nThe filter produces either PIDs with media frames, or file PIDs with multiplexed data (e.g. MPEG-2 TS). \nThe filter will use: \nRTSP over HTTP tunnel if server port is 80 or 8080 or if protocol scheme is rtsph://. \nRTSP over TLS if server port is 322 or if protocol scheme is rtsps://. \nRTSP over HTTPS tunnel if server port is 443 and if protocol scheme is rtsph://. \nThe filter will attempt reconnecting in TLS mode after two consecutive initial connection failures. \nOptions¶\nsrc (cstr): location of source content (SDP, RTP or RTSP URL)\nfirstport (uint, default: 0): default first port number to use (0 lets the filter decide)\nifce (str): default interface IP to use for multicast. If NULL, the default system interface will be used\nttl (uint, default: 127, minmax: 0-127): multicast TTL\nreorder_len (uint, default: 1000): reorder length in packets\nreorder_delay (uint, default: 50): max delay in RTP re-orderer, packets will be dispatched after that\nblock_size (uint, default: 0x100000): buffer size for RTP/UDP or RTSP when interleaved\ndisable_rtcp (bool, default: false): disable RTCP reporting\nnat_keepalive (uint, default: 0): delay in ms of NAT keepalive, disabled by default (except for SatIP, set to 30s by default)\nforce_mcast (str): force multicast on indicated IP in RTSP setup\nuse_client_ports (bool, default: false): force using client ports (hack for some RTSP servers overriding client ports)\nbandwidth (uint, default: 0): set bandwidth param for RTSP requests\ndefault_port (uint, default: 554, minmax: 0-65535): set default RTSP port\nsatip_port (uint, default: 1400, minmax: 0-65535): set default port for SATIP\ntransport (enum, default: auto): set RTP over RTSP \nauto: set interleave on if HTTP tunnel is used, off otherwise and retry in interleaved mode if UDP timeout \ntcp: enable RTP over RTSP \nudp: disable RTP over RTSP \nudp_timeout (uint, default: 10000): default timeout before considering UDP is down\nrtcp_timeout (uint, default: 5000): default timeout for RTCP traffic in ms. After this timeout, playback will start out of sync. If 0 always wait for RTCP\nfirst_packet_drop (uint, default: 0, updatable): set number of first RTP packet to drop (0 if no drop)\nfrequency_drop (uint, default: 0, updatable): drop 1 out of N packet (0 disable dropping)\nloss_rate (sint, default: -1, updatable): loss rate to signal in RTCP, -1 means real loss rate, otherwise a per-thousand of packet lost\nuser_agent (str, default: $GUA): user agent string, by default solved from GPAC preferences\nlanguages (str, default: $GLANG): user languages, by default solved from GPAC preferences\nstats (uint, default: 500): update statistics to the user every given MS (0 disables reporting)\nmax_sleep (sint, default: 1000): set max sleep in milliseconds: \na negative value -N means to always sleep for N ms \na positive value N means to sleep at most N ms but will sleep less if frame duration is shorter \nrtcpsync (bool, default: true): use RTCP to adjust synchronization\nforceagg (bool, default: false): force RTSP control aggregation (patch for buggy servers)\nssm (strl): list of IP to include for source-specific multicast\nssmx (strl): list of IP to exclude for source-specific multicast \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rtpin/"},{"date_scraped_timestamp":1720187997335,"host":"wiki.gpac.io","page_title":"rtpin - GPAC wiki","text":"\n \n \n \n \n \n \nRTP/RTSP/SDP input¶\nRegister name used to load filter: rtpin\nThis filter may be automatically loaded during graph resolution. \nThis filter handles SDP/RTSP/RTP input reading. It supports: \nSDP file reading \nRTP direct url through rtp:// protocol scheme \nRTSP session processing through rtsp:// and satip:// protocol schemes \nThe filter produces either PIDs with media frames, or file PIDs with multiplexed data (e.g. MPEG-2 TS). \nThe filter will use: \nRTSP over HTTP tunnel if server port is 80 or 8080 or if protocol scheme is rtsph://. \nRTSP over TLS if server port is 322 or if protocol scheme is rtsps://. \nRTSP over HTTPS tunnel if server port is 443 and if protocol scheme is rtsph://. \nThe filter will attempt reconnecting in TLS mode after two consecutive initial connection failures. \nOptions¶\nsrc (cstr): location of source content (SDP, RTP or RTSP URL)\nfirstport (uint, default: 0): default first port number to use (0 lets the filter decide)\nifce (str): default interface IP to use for multicast. If NULL, the default system interface will be used\nttl (uint, default: 127, minmax: 0-127): multicast TTL\nreorder_len (uint, default: 1000): reorder length in packets\nreorder_delay (uint, default: 50): max delay in RTP re-orderer, packets will be dispatched after that\nblock_size (uint, default: 0x100000): buffer size for RTP/UDP or RTSP when interleaved\ndisable_rtcp (bool, default: false): disable RTCP reporting\nnat_keepalive (uint, default: 0): delay in ms of NAT keepalive, disabled by default (except for SatIP, set to 30s by default)\nforce_mcast (str): force multicast on indicated IP in RTSP setup\nuse_client_ports (bool, default: false): force using client ports (hack for some RTSP servers overriding client ports)\nbandwidth (uint, default: 0): set bandwidth param for RTSP requests\ndefault_port (uint, default: 554, minmax: 0-65535): set default RTSP port\nsatip_port (uint, default: 1400, minmax: 0-65535): set default port for SATIP\ntransport (enum, default: auto): set RTP over RTSP \nauto: set interleave on if HTTP tunnel is used, off otherwise and retry in interleaved mode if UDP timeout \ntcp: enable RTP over RTSP \nudp: disable RTP over RTSP \nudp_timeout (uint, default: 10000): default timeout before considering UDP is down\nrtcp_timeout (uint, default: 5000): default timeout for RTCP traffic in ms. After this timeout, playback will start out of sync. If 0 always wait for RTCP\nfirst_packet_drop (uint, default: 0, updatable): set number of first RTP packet to drop (0 if no drop)\nfrequency_drop (uint, default: 0, updatable): drop 1 out of N packet (0 disable dropping)\nloss_rate (sint, default: -1, updatable): loss rate to signal in RTCP, -1 means real loss rate, otherwise a per-thousand of packet lost\nuser_agent (str, default: $GUA): user agent string, by default solved from GPAC preferences\nlanguages (str, default: $GLANG): user languages, by default solved from GPAC preferences\nstats (uint, default: 500): update statistics to the user every given MS (0 disables reporting)\nmax_sleep (sint, default: 1000): set max sleep in milliseconds: \na negative value -N means to always sleep for N ms \na positive value N means to sleep at most N ms but will sleep less if frame duration is shorter \nrtcpsync (bool, default: true): use RTCP to adjust synchronization\nforceagg (bool, default: false): force RTSP control aggregation (patch for buggy servers)\nssm (strl): list of IP to include for source-specific multicast\nssmx (strl): list of IP to exclude for source-specific multicast \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rtpin/?q="},{"date_scraped_timestamp":1720187997335,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"rtpin - GPAC wiki","text":"\n \n \n \n \n \n \nRTP/RTSP/SDP input¶\nRegister name used to load filter: rtpin\nThis filter may be automatically loaded during graph resolution. \nThis filter handles SDP/RTSP/RTP input reading. It supports: \nSDP file reading \nRTP direct url through rtp:// protocol scheme \nRTSP session processing through rtsp:// and satip:// protocol schemes \nThe filter produces either PIDs with media frames, or file PIDs with multiplexed data (e.g. MPEG-2 TS). \nThe filter will use: \nRTSP over HTTP tunnel if server port is 80 or 8080 or if protocol scheme is rtsph://. \nRTSP over TLS if server port is 322 or if protocol scheme is rtsps://. \nRTSP over HTTPS tunnel if server port is 443 and if protocol scheme is rtsph://. \nThe filter will attempt reconnecting in TLS mode after two consecutive initial connection failures. \nOptions¶\nsrc (cstr): location of source content (SDP, RTP or RTSP URL)\nfirstport (uint, default: 0): default first port number to use (0 lets the filter decide)\nifce (str): default interface IP to use for multicast. If NULL, the default system interface will be used\nttl (uint, default: 127, minmax: 0-127): multicast TTL\nreorder_len (uint, default: 1000): reorder length in packets\nreorder_delay (uint, default: 50): max delay in RTP re-orderer, packets will be dispatched after that\nblock_size (uint, default: 0x100000): buffer size for RTP/UDP or RTSP when interleaved\ndisable_rtcp (bool, default: false): disable RTCP reporting\nnat_keepalive (uint, default: 0): delay in ms of NAT keepalive, disabled by default (except for SatIP, set to 30s by default)\nforce_mcast (str): force multicast on indicated IP in RTSP setup\nuse_client_ports (bool, default: false): force using client ports (hack for some RTSP servers overriding client ports)\nbandwidth (uint, default: 0): set bandwidth param for RTSP requests\ndefault_port (uint, default: 554, minmax: 0-65535): set default RTSP port\nsatip_port (uint, default: 1400, minmax: 0-65535): set default port for SATIP\ntransport (enum, default: auto): set RTP over RTSP \nauto: set interleave on if HTTP tunnel is used, off otherwise and retry in interleaved mode if UDP timeout \ntcp: enable RTP over RTSP \nudp: disable RTP over RTSP \nudp_timeout (uint, default: 10000): default timeout before considering UDP is down\nrtcp_timeout (uint, default: 5000): default timeout for RTCP traffic in ms. After this timeout, playback will start out of sync. If 0 always wait for RTCP\nfirst_packet_drop (uint, default: 0, updatable): set number of first RTP packet to drop (0 if no drop)\nfrequency_drop (uint, default: 0, updatable): drop 1 out of N packet (0 disable dropping)\nloss_rate (sint, default: -1, updatable): loss rate to signal in RTCP, -1 means real loss rate, otherwise a per-thousand of packet lost\nuser_agent (str, default: $GUA): user agent string, by default solved from GPAC preferences\nlanguages (str, default: $GLANG): user languages, by default solved from GPAC preferences\nstats (uint, default: 500): update statistics to the user every given MS (0 disables reporting)\nmax_sleep (sint, default: 1000): set max sleep in milliseconds: \na negative value -N means to always sleep for N ms \na positive value N means to sleep at most N ms but will sleep less if frame duration is shorter \nrtcpsync (bool, default: true): use RTCP to adjust synchronization\nforceagg (bool, default: false): force RTSP control aggregation (patch for buggy servers)\nssm (strl): list of IP to include for source-specific multicast\nssmx (strl): list of IP to exclude for source-specific multicast \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rtpin/?q="},{"date_scraped_timestamp":1720187962886,"host":"wiki.gpac.io","page_title":"rtpout - GPAC wiki","text":"\n \n \n \n \n \n \nRTP Streamer¶\nRegister name used to load filter: rtpout\nThis filter may be automatically loaded during graph resolution. \nThe RTP streamer handles SDP/RTP output streaming. \nSDP mode¶\nWhen the destination URL is an SDP, the filter outputs an SDP on a file PID and streams RTP packets over UDP, starting from the indicated port. \nDirect RTP mode¶\nWhen the destination URL uses the protocol scheme rtp://IP:PORT, the filter does not output any SDP and streams a single input over RTP, using PORT indicated in the destination URL, or the first port configured.\nIn this mode, it is usually needed to specify the desired format using ext or mime.\nExample\ngpac -i src -o rtp://localhost:1234/:ext=ts\nThis will indicate that the RTP streamer expects a MPEG-2 TS mux as an input. \nRTP Packets¶\nThe RTP packets produced have a maximum payload set by the mtu option (IP packet will be MTU + 40 bytes of IP+UDP+RTP headers).\nThe real-time scheduling algorithm works as follows: \nfirst initialize the clock by: \ncomputing the smallest timestamp for all input PIDs \nmapping this media time to the system clock \ndetermine the earliest packet to send next on each input PID, adding delay if any \nfinally compare the packet mapped timestamp TS to the system clock SC. When TS - SC is less than tt, the RTP packets for the source packet are sent \nThe filter does not check for RTCP timeout and will run until all input PIDs reach end of stream. \nOptions¶\nip (str): destination IP address (NULL is 127.0.0.1)\nport (uint, default: 7000): port for first stream in session\nloop (bool, default: true): loop all streams in session (not always possible depending on source type)\nmpeg4 (bool, default: false): send all streams using MPEG-4 generic payload format if possible\nmtu (uint, default: 1460): size of RTP MTU in bytes\nttl (uint, default: 2): time-to-live for multicast packets\nifce (str): default network interface to use\npayt (uint, default: 96, minmax: 96-127): payload type to use for dynamic decoder configurations\ndelay (sint, default: 0): send delay for packet (negative means send earlier)\ntt (uint, default: 1000): time tolerance in microseconds. Whenever schedule time minus realtime is below this value, the packet is sent right away\nrunfor (sint, default: -1): run for the given time in ms. Negative value means run for ever (if loop) or source duration, 0 only outputs the sdp\ntso (sint, default: -1): set timestamp offset in microseconds. Negative value means random initial timestamp\nxps (bool, default: false): force parameter set injection at each SAP. If not set, only inject if different from SDP ones\nlatm (bool, default: false): use latm for AAC payload format\ndst (cstr): URL for direct RTP mode\next (str): file extension for direct RTP mode\nmime (cstr): set mime type for direct RTP mode \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rtpout/"},{"date_scraped_timestamp":1720188075239,"host":"wiki.gpac.io","page_title":"rtpout - GPAC wiki","text":"\n \n \n \n \n \n \nRTP Streamer¶\nRegister name used to load filter: rtpout\nThis filter may be automatically loaded during graph resolution. \nThe RTP streamer handles SDP/RTP output streaming. \nSDP mode¶\nWhen the destination URL is an SDP, the filter outputs an SDP on a file PID and streams RTP packets over UDP, starting from the indicated port. \nDirect RTP mode¶\nWhen the destination URL uses the protocol scheme rtp://IP:PORT, the filter does not output any SDP and streams a single input over RTP, using PORT indicated in the destination URL, or the first port configured.\nIn this mode, it is usually needed to specify the desired format using ext or mime.\nExample\ngpac -i src -o rtp://localhost:1234/:ext=ts\nThis will indicate that the RTP streamer expects a MPEG-2 TS mux as an input. \nRTP Packets¶\nThe RTP packets produced have a maximum payload set by the mtu option (IP packet will be MTU + 40 bytes of IP+UDP+RTP headers).\nThe real-time scheduling algorithm works as follows: \nfirst initialize the clock by: \ncomputing the smallest timestamp for all input PIDs \nmapping this media time to the system clock \ndetermine the earliest packet to send next on each input PID, adding delay if any \nfinally compare the packet mapped timestamp TS to the system clock SC. When TS - SC is less than tt, the RTP packets for the source packet are sent \nThe filter does not check for RTCP timeout and will run until all input PIDs reach end of stream. \nOptions¶\nip (str): destination IP address (NULL is 127.0.0.1)\nport (uint, default: 7000): port for first stream in session\nloop (bool, default: true): loop all streams in session (not always possible depending on source type)\nmpeg4 (bool, default: false): send all streams using MPEG-4 generic payload format if possible\nmtu (uint, default: 1460): size of RTP MTU in bytes\nttl (uint, default: 2): time-to-live for multicast packets\nifce (str): default network interface to use\npayt (uint, default: 96, minmax: 96-127): payload type to use for dynamic decoder configurations\ndelay (sint, default: 0): send delay for packet (negative means send earlier)\ntt (uint, default: 1000): time tolerance in microseconds. Whenever schedule time minus realtime is below this value, the packet is sent right away\nrunfor (sint, default: -1): run for the given time in ms. Negative value means run for ever (if loop) or source duration, 0 only outputs the sdp\ntso (sint, default: -1): set timestamp offset in microseconds. Negative value means random initial timestamp\nxps (bool, default: false): force parameter set injection at each SAP. If not set, only inject if different from SDP ones\nlatm (bool, default: false): use latm for AAC payload format\ndst (cstr): URL for direct RTP mode\next (str): file extension for direct RTP mode\nmime (cstr): set mime type for direct RTP mode \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/rtpout/?q="},{"date_scraped_timestamp":1720187944000,"host":"wiki.gpac.io","page_title":"scte35dec - GPAC wiki","text":"\n \n \n \n \n \n \nSCTE35 decoder¶\nRegister name used to load filter: scte35dec\nThis filter is not checked during graph resolution and needs explicit loading. \nThis filter extracts SCTE-35 markers attached as properties to audio and video\npackets as 23001-18 'emib' boxes. It also creates empty 'emeb' box in between\nfollowing segmentation as hinted by the graph. \nOptions¶\nmode (enum, default: 23001-18): mode to operate in \n23001-18: extract SCTE-35 markers as emib/emeb boxes for Event Tracks \npassthrough: pass-through mode adding cue start property on splice points \nsegdur (frac, default: 1/1): segmentation duration in seconds. 0/0 flushes immediately for each input packet (beware of the bitrate overhead) \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/scte35dec/"},{"date_scraped_timestamp":1720188127593,"host":"wiki.gpac.io","page_title":"sockout - GPAC wiki","text":"\n \n \n \n \n \n \nUDP/TCP output¶\nRegister name used to load filter: sockout\nThis filter may be automatically loaded during graph resolution. \nThis filter handles generic output sockets (mono-directional) in blocking mode only.\nThe filter can work in server mode, waiting for source connections, or in client mode, directly connecting to a server.\nIn server mode, the filter can be instructed to keep running at the end of the stream.\nIn server mode, the default behavior is to keep input packets when no more clients are connected; this can be adjusted though the kp option, however there is no realtime regulation of how fast packets are dropped.\nIf your sources are not real time, consider adding a real-time scheduler in the chain (cf reframer filter), or set the send rate option. \nUDP sockets are used for destinations URLs formatted as udp://NAME \nTCP sockets are used for destinations URLs formatted as tcp://NAME \nUDP unix domain sockets are used for destinations URLs formatted as udpu://NAME \nTCP unix domain sockets are used for destinations URLs formatted as tcpu://NAME \nWhen ports are specified in the URL and the default option separators are used (see gpac -h doc), the URL must either: \nhave a trailing '/', e.g. udp://localhost:1234/[:opts] \nuse gpac escape, e.g. udp://localhost:1234[:gpac:opts] \nThe socket output can be configured to drop or revert packet order for test purposes.\nA window size in packets is specified as the drop/revert fraction denominator, and the index of the packet to drop/revert is given as the numerator/\nIf the numerator is 0, a packet is randomly chosen in that window.\nExample\nThis drops every 4th packet of each 10 packet window.\nExample\nThis reverts the send order of one random packet in each 100 packet window. \nOptions¶\ndst (cstr): URL of destination\nsockbuf (uint, default: 65536): block size used to read file\nport (uint, default: 1234): default port if not specified\nifce (cstr): default multicast interface\next (str): file extension of pipe data\nmime (str): mime type of pipe data\nlisten (bool, default: false): indicate the output socket works in server mode\nmaxc (uint, default: +I): max number of concurrent connections\nka (bool, default: false): keep socket alive if no more connections\nkp (bool, default: true): keep packets in queue if no more clients\nstart (dbl, default: 0.0): set playback start offset. A negative value means percent of media duration with -1 equal to duration\nspeed (dbl, default: 1.0): set playback speed. If negative and start is 0, start is set to -1\nrate (uint, default: 0): set send rate in bps, disabled by default (as fast as possible)\npckr (frac, default: 0/0): reverse packet every N\npckd (frac, default: 0/0): drop packet every N\nttl (uint, default: 0, minmax: 0-127): multicast TTL \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/sockout/"},{"date_scraped_timestamp":1720187985171,"host":"wiki.gpac.io","page_title":"theoradec - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTheora decoder¶\nRegister name used to load filter: theoradec\nThis filter may be automatically loaded during graph resolution. \nThis filter decodes Theora streams through libtheora library. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/theoradec/"},{"date_scraped_timestamp":1720187977195,"host":"wiki.gpac.io","page_title":"thumbs - GPAC wiki","text":"\n \n \n \n \n \n \nThumbnail collection generator¶\nRegister name used to load filter: thumbs\nThis is a JavaScript filter. It is not checked during graph resolution and needs explicit loading.\nAuthor: GPAC team \nThis filter generates screenshots from a video stream. \nThe input video is down-sampled by the scale factor. The output size is configured based on the number of images per line and per column in the grid. \nOnce configured, the output size is no longer modified. \nThe snap option indicates to use one video frame every given seconds. If value is 0, all input frames are used. \nIf the number of rows is 0, it will be computed based on the source duration and desired snap time, and will default to 10 if it cannot be resolved. \nTo output one image per input frame, use :grid=1x1. \nIf a single image per output frame is used, the default value for snap is 0 and for scale is 1.\nOtherwise, the default value for snap is 1 second and for scale is 10. \nA single line of text can be inserted over each frame. Predefined keywords can be used in input text, identified as $KEYWORD$: \nts: replaced by packet timestamp \ntimescale: replaced by PID timescale \ntime: replaced by packet time as HH:MM:SS.ms \ncpu: replaced by current CPU usage of process \nmem: replaced by current memory usage of process \nversion: replaced by GPAC version \nfversion: replaced by GPAC full version \nmae: replaced by Mean Absolute Error with previous frame \nmse: replaced by Mean Square Error with previous frame \nP4CC, PropName: replaced by corresponding PID property \nExample\ngpac -i src reframer:saps=1 thumbs:snap=30:grid=6x30 -o dump/$num$.png\nThis will generate images from key-frames only, inserting one image every 30 seconds. Using key-frame filtering is much faster but may give unexpected results if there are not enough key-frames in the source. \nExample\ngpac -i src thumbs:snap=0:grid=5x5 -o dump/$num$.png\nThis will generate one image containing 25 frames every second at 25 fps. \nIf a single image per output frame is used and the scaling factor is 1, the input packet is reused as input with text and graphics overlaid. \nExample\ngpac -i src thumbs:grid=1x1:txt='Frame $time$' -o dump/$num$.png\nThis will inject text over each frame and keep timing and other packet properties. \nA json output can be specified in input list to let applications retrieve frame position in output image from its timing. \nScene change detection¶\nThe filter can compute the absolute and/or square error metrics between consecutive images and drop image if the computed metric is less than the given threshold.\nIf both mae and mse thresholds are 0, scene detection is not performed (default).\nIf both mae and mse thresholds are not 0, the frame is added if it passes both thresholds. \nFor both metrics, a value of 0 means all pixels are the same, a value of 100 means all pixels have 100% intensity difference (e.g. black versus white). \nThe scene detection is performed after the snap filtering and uses: \nthe previous frame in the stream, whether it was added or not, if scref is not set, \nthe last added frame otherwise. \nTypical thresholds for scene cut detection are 14 to 20 for mae and 5 to 7 for mse. \nSince this is a costly process, it is recommended to use it combined with key-frames selection: \nExample\ngpac -i src reframer:saps=1 thumbs:mae=15 -o dump/$num$.png\nThe maxsnap option can be used to force insertion after the given time if no scene cut is found. \nOptions¶\ngrid (v2di, default: 6x0): number of images per lines and columns\nscale (dbl, default: -1): scale factor for input size\nmae (uint, default: 0, minmax: 0,100): scene diff threshold using Mean Absolute Error\nmse (uint, default: 0, minmax: 0,100): scene diff threshold using Mean Square Error\nlw (dbl, default: 0.0): line width between images in pixels\nlc (str, default: white): line color\nclear (str, default: white): clear color\nsnap (dbl, default: -1): duration between images, 0 for all images\nmaxsnap (dbl, default: -1): maximum duration between two thumbnails when scene change detection is enabled\npfmt (pfmt, default: rgb): output pixel format\ntxt (str, default: ): text to insert per thumbnail\n__tc (str, default: white): text color\ntb (str, default: black): text shadow\nfont (str, default: SANS): font to use\nfs (dbl, default: 10): font size to use in percent of scaled height\ntv (dbl, default: 0): text vertical position in percent of scaled height\nthread (sint, default: -1): number of threads for software rasterizer, -1 for all available cores\nblt (bool, default: true): use blit instead of software rasterizer\nscref (bool, default: false): use last inserted image as reference for scene change detection\ndropfirst (bool, default: false): drop first image\nlist (str, default: null): export json list of frame times and positions to given file\nlxy (bool, default: false): add explicit x and y in json export \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/thumbs/"},{"date_scraped_timestamp":1720188086058,"host":"wiki.gpac.io","page_title":"ttml2vtt - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTTML to WebVTT¶\nRegister name used to load filter: ttml2vtt\nThis filter may be automatically loaded during graph resolution. \nThis filter converts TTML frames to unframed WebVTT. \nConversion is quite limited: only the first div is analyzed and only basic styling is implemented. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ttml2vtt/"},{"date_scraped_timestamp":1720188003585,"host":"wiki.gpac.io","page_title":"tx3g2srt - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTX3G to SRT¶\nRegister name used to load filter: tx3g2srt\nThis filter may be automatically loaded during graph resolution. \nThis filter converts a single ISOBMFF TX3G stream to an SRT unframed stream. \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/tx3g2srt/"},{"date_scraped_timestamp":1720188014902,"host":"wiki.gpac.io","page_title":"tx3g2ttml - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nTX3G to TTML¶\nRegister name used to load filter: tx3g2ttml\nThis filter may be automatically loaded during graph resolution. \nThis filter converts ISOBMFF TX3G stream to a TTML stream. \nEach output TTML frame is a complete TTML document. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/tx3g2ttml/"},{"date_scraped_timestamp":1720188117168,"host":"wiki.gpac.io","page_title":"txtin - GPAC wiki","text":"\n \n \n \n \n \n \nSubtitle loader¶\nRegister name used to load filter: txtin\nThis filter may be automatically loaded during graph resolution. \nThis filter reads subtitle data from input PID to produce subtitle frames on a single PID.\nThe filter supports the following formats: \nSRT: https://en.wikipedia.org/wiki/SubRip \nWebVTT: https://www.w3.org/TR/webvtt1/ \nTTXT: https://wiki.gpac.io/xmlformats/TTXT-Format-Documentation \nQT 3GPP Text XML (TexML): Apple QT6, likely deprecated \nTTML: https://www.w3.org/TR/ttml2/ \nSUB: one subtitle per line formatted as {start_frame}{end_frame}text \nSSA (Substation Alpha): basic parsing support for common files \nInput files must be in UTF-8 or UTF-16 format, with or without BOM. The internal frame format is: \nWebVTT (and srt if desired): ISO/IEC 14496-30 VTT cues \nTTML: ISO/IEC 14496-30 XML subtitles \nstxt and sbtt: ISO/IEC 14496-30 text stream and text subtitles \nOthers: 3GPP/QT Timed Text \nTTML Support¶\nIf ttml_split option is set, the TTML document is split in independent time segments by inspecting all overlapping subtitles in the body.\nEmpty periods in TTML will result in empty TTML documents or will be skipped if no_empty option is set. \nThe first sample has a CTS assigned as indicated by ttml_cts: \na numerator of -2 indicates the first CTS is 0 \na numerator of -1 indicates the first CTS is the first active time in document \na numerator >= 0 indicates the CTS to use for first sample \nWhen TTML splitting is disabled, the duration of the TTML sample is given by ttml_dur if not 0, or set to the document duration \nBy default, media resources are kept as declared in TTML2 documents. \nttml_embed can be used to embed inside the TTML sample the resources in <head> or <body>: \nfor <source>, <image>, <audio>, <font>, local URIs indicated in src will be loaded and src rewritten. \nfor <data> with base64 coding, the data will be decoded, <data> element removed and parent <source> rewritten with src attribute inserted. \nThe embedded data is added as a subsample to the TTML frame, and the referring elements will use src=urn:mpeg:14496-30:N with N the index of the subsample. \nA subtitle zero may be specified using ttml_zero. This will remove all subtitles before the given time T0, and rewrite each subtitle begin/end T to T-T0 using millisecond accuracy. \nWarning: Original time formatting (tick, frames/subframe ...) will be lost when this option is used, converted to HH:MM:SS.ms. \nThe subtitle zero time must be prefixed with T when the option is not set as a global argument:\nExample\ngpac -i test.ttml:ttml_zero=T10:00:00 [...] \nMP4Box -add test.ttml:sopt:ttml_zero=T10:00:00 [...] \ngpac -i test.ttml --ttml_zero=10:00:00 [...] \ngpac -i test.ttml --ttml_zero=T10:00:00 [...] \nMP4Box -add test.ttml --ttml_zero=10:00:00 [...]\nSimple Text Support¶\nThe text loader can convert input files in simple text streams of a single packet, by forcing the codec type on the input:\nExample\ngpac -i test.txt:#CodecID=stxt [...] \ngpac fin:pck=\"Text Data\":#CodecID=stxt [...]\nThe content of the source file will be the payload of the text sample. The stxtmod option allows specifying WebVTT, TX3G or simple text mode for output format.\nIn this mode, the stxtdur option is used to control the duration of the generated subtitle: \na positive value always forces the duration \na negative value forces the duration if input packet duration is not known \nNotes¶\nWhen reframing simple text streams from demuxers (e.g. subtitles from MKV), the output format of these streams can be selected using stxtmod. \nWhen importing SRT, SUB or SSA files, the output format of the PID can be selected using stxtmod. \nOptions¶\nnodefbox (bool, default: false): skip default text box\nnoflush (bool, default: false): skip final sample flush for srt\nfontname (str): default font\nfontsize (uint, default: 18): default font size\nlang (str): default language\nwidth (uint, default: 0): default width of text area\nheight (uint, default: 0): default height of text area\ntxtx (uint, default: 0): default horizontal offset of text area: -1 (left), 0 (center) or 1 (right)\ntxty (uint, default: 0): default vertical offset of text area: -1 (bottom), 0 (center) or 1 (top)\nzorder (sint, default: 0): default z-order of the PID\ntimescale (uint, default: 1000): default timescale of the PID\nttml_split (bool, default: true): split ttml doc in non-overlapping samples\nttml_cts (lfrac, default: -1/1): first sample cts - see filter help\nttml_dur (frac, default: 0/1): sample duration when not spliting split - see filter help\nttml_embed (bool, default: false): force embedding TTML resources\nttml_zero (str): set subtitle zero time for TTML\nno_empty (bool, default: false): do not send empty samples\nstxtdur (frac, default: 1): duration for simple text\nstxtmod (enum, default: tx3g): text stream mode for simple text streams and SRT inputs \nstxt: output PID formatted as simple text stream \nsbtt: output PID formatted as subtitle text stream \ntx3g: output PID formatted as TX3G/Apple stream \nvtt: output PID formatted as WebVTT stream \nwebvtt: same as vtt (for backward compatiblity \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/txtin/"},{"date_scraped_timestamp":1720188064309,"host":"wiki.gpac.io","page_title":"ufm4v - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nM4V writer¶\nRegister name used to load filter: ufm4v\nThis filter may be automatically loaded during graph resolution. \nThis filter converts MPEG-4 part 2 visual streams into writable format (reinsert decoder config). \nOptions¶\nrcfg (bool, default: true): force repeating decoder config at each I-frame \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ufm4v/"},{"date_scraped_timestamp":1720187977978,"host":"wiki.gpac.io","page_title":"ufmhas - GPAC wiki","text":"\n \n \n \n \n \n \nMHAS writer¶\nRegister name used to load filter: ufmhas\nThis filter may be automatically loaded during graph resolution. \nThis filter converts MPEG-H Audio streams into MHAS encapsulated data. \nOptions¶\nsyncp (bool, default: false): if set, insert sync packet at each frame, otherwise only at SAP \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ufmhas/"},{"date_scraped_timestamp":1720188125412,"host":"wiki.gpac.io","page_title":"ufmhas - GPAC wiki","text":"\n \n \n \n \n \n \nMHAS writer¶\nRegister name used to load filter: ufmhas\nThis filter may be automatically loaded during graph resolution. \nThis filter converts MPEG-H Audio streams into MHAS encapsulated data. \nOptions¶\nsyncp (bool, default: false): if set, insert sync packet at each frame, otherwise only at SAP \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ufmhas/?q="},{"date_scraped_timestamp":1720187977978,"error":"Too much contention on these documents. Please try again.","host":"wiki.gpac.io","page_title":"ufmhas - GPAC wiki","text":"\n \n \n \n \n \n \nMHAS writer¶\nRegister name used to load filter: ufmhas\nThis filter may be automatically loaded during graph resolution. \nThis filter converts MPEG-H Audio streams into MHAS encapsulated data. \nOptions¶\nsyncp (bool, default: false): if set, insert sync packet at each frame, otherwise only at SAP \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/ufmhas/"},{"date_scraped_timestamp":1720187993304,"host":"wiki.gpac.io","page_title":"uncvdec - GPAC wiki","text":"\n \n \n \n \n \n \nUNCV decoder¶\nRegister name used to load filter: uncvdec\nThis filter may be automatically loaded during graph resolution. \nThis filter translates UNCV pixel format to a usable pixel format. \nOptions¶\nforce_pf (bool, default: false): ignore possible mapping to GPAC pixel formats\nno_tile (bool, default: false): ignore tiling info (debug) \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/uncvdec/"},{"date_scraped_timestamp":1720187993304,"error":"Aborted due to cross-transaction contention. This occurs when multiple transactions attempt to access the same data, requiring Firestore to abort at least one in order to enforce serializability.","host":"wiki.gpac.io","page_title":"uncvdec - GPAC wiki","text":"\n \n \n \n \n \n \nUNCV decoder¶\nRegister name used to load filter: uncvdec\nThis filter may be automatically loaded during graph resolution. \nThis filter translates UNCV pixel format to a usable pixel format. \nOptions¶\nforce_pf (bool, default: false): ignore possible mapping to GPAC pixel formats\nno_tile (bool, default: false): ignore tiling info (debug) \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/uncvdec/"},{"date_scraped_timestamp":1720188004687,"error":"Aborted due to cross-transaction contention. This occurs when multiple transactions attempt to access the same data, requiring Firestore to abort at least one in order to enforce serializability.","host":"wiki.gpac.io","page_title":"uncvdec - GPAC wiki","text":"\n \n \n \n \n \n \nUNCV decoder¶\nRegister name used to load filter: uncvdec\nThis filter may be automatically loaded during graph resolution. \nThis filter translates UNCV pixel format to a usable pixel format. \nOptions¶\nforce_pf (bool, default: false): ignore possible mapping to GPAC pixel formats\nno_tile (bool, default: false): ignore tiling info (debug) \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/uncvdec/?q="},{"date_scraped_timestamp":1720188004687,"host":"wiki.gpac.io","page_title":"uncvdec - GPAC wiki","text":"\n \n \n \n \n \n \nUNCV decoder¶\nRegister name used to load filter: uncvdec\nThis filter may be automatically loaded during graph resolution. \nThis filter translates UNCV pixel format to a usable pixel format. \nOptions¶\nforce_pf (bool, default: false): ignore possible mapping to GPAC pixel formats\nno_tile (bool, default: false): ignore tiling info (debug) \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/uncvdec/?q="},{"date_scraped_timestamp":1720187977840,"host":"wiki.gpac.io","page_title":"unframer - GPAC wiki","text":"\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Howtos\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n MP4Box\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n GPAC\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Build\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Developers\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Playback\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n XML formats\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nStream unframer¶\nRegister name used to load filter: unframer\nThis filter is not checked during graph resolution and needs explicit loading. \nThis filter is used to force reframing of input sources using the same internal framing as GPAC (e.g. ISOBMFF) but with broken framing or signaling.\nExample\ngpac -i src.mp4 unframer -o dst.mp4\nThis will: \nforce input PIDs of unframer to be in serialized form (AnnexB, ADTS, ...) \ntrigger reframers to be instanciated after the unframer filter. \nUsing the unframer filter avoids doing a dump to disk then re-import or other complex data piping. \nNo options \n \n \n \n \n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/unframer/"},{"date_scraped_timestamp":1720188063054,"host":"wiki.gpac.io","page_title":"vtbdec - GPAC wiki","text":"\n \n \n \n \n \n \nRegister name used to load filter: vtbdec\nThis filter may be automatically loaded during graph resolution. \nThis filter decodes video streams through OSX/iOS VideoToolBox (MPEG-2, H263, AVC|H264, HEVC, ProRes). It allows GPU frame dispatch or direct frame copy. \nOptions¶\nreorder (uint, default: 6): number of frames to wait for temporal re-ordering\nno_copy (bool, default: true): dispatch decoded frames as OpenGL textures (true) or as copied packets (false) \nofmt (pfmt, default: nv12): set default pixel format for decoded video. If not found, fall back to nv12\ndisable_hw (bool, default: false): disable hardware decoding\nwait_sync (bool, default: false, updatable): wait for sync frame before decoding \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/vtbdec/?q="},{"date_scraped_timestamp":1720188044547,"host":"wiki.gpac.io","page_title":"vtbdec - GPAC wiki","text":"\n \n \n \n \n \n \nRegister name used to load filter: vtbdec\nThis filter may be automatically loaded during graph resolution. \nThis filter decodes video streams through OSX/iOS VideoToolBox (MPEG-2, H263, AVC|H264, HEVC, ProRes). It allows GPU frame dispatch or direct frame copy. \nOptions¶\nreorder (uint, default: 6): number of frames to wait for temporal re-ordering\nno_copy (bool, default: true): dispatch decoded frames as OpenGL textures (true) or as copied packets (false) \nofmt (pfmt, default: nv12): set default pixel format for decoded video. If not found, fall back to nv12\ndisable_hw (bool, default: false): disable hardware decoding\nwait_sync (bool, default: false, updatable): wait for sync frame before decoding \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/vtbdec/"},{"date_scraped_timestamp":1720187988140,"host":"wiki.gpac.io","page_title":"vtt2tx3g - GPAC wiki","text":"\n \n \n \n \n \n \nWebVTT to TX3G¶\nRegister name used to load filter: vtt2tx3g\nThis filter may be automatically loaded during graph resolution. \nThis filter rewrites unframed WebVTT to TX3G / QT Timed Text (binary format) \nUnframed WebVTT packets consist in single cues: \ncue payload as packet payload \nprefix as packet string property vtt_pre \ncue ID as packet string property vtt_cueid \ncue settings as packet string property vtt_settings \npacket timing contains the cue timing (start and duration) \nOptions¶\nfontname (str): default font\nfontsize (uint, default: 18): default font size \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/vtt2tx3g/"},{"date_scraped_timestamp":1720188045397,"host":"wiki.gpac.io","page_title":"writeqcp - GPAC wiki","text":"\n \n \n \n \n \n \nQCP writer¶\nRegister name used to load filter: writeqcp\nThis filter may be automatically loaded during graph resolution. \nThis filter converts a single QCELP, EVRC or MSV stream to a QCP output file. \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/writeqcp/?q="},{"date_scraped_timestamp":1720188024172,"host":"wiki.gpac.io","page_title":"writeqcp - GPAC wiki","text":"\n \n \n \n \n \n \nQCP writer¶\nRegister name used to load filter: writeqcp\nThis filter may be automatically loaded during graph resolution. \nThis filter converts a single QCELP, EVRC or MSV stream to a QCP output file. \nOptions¶\nexporter (bool, default: false): compatibility with old exporter, displays export results \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/writeqcp/"},{"date_scraped_timestamp":1720187938713,"host":"wiki.gpac.io","page_title":"xviddec - GPAC wiki","text":"\n \n \n \n \n \n \nXVid decoder¶\nRegister name used to load filter: xviddec\nThis filter may be automatically loaded during graph resolution. \nThis filter decodes MPEG-4 part 2 (and DivX) through libxvidcore library. \nOptions¶\ndeblock_y (bool, default: false): enable Y deblocking\ndeblock_uv (bool, default: false): enable UV deblocking\nfilm_effect (bool, default: false): enable film effect\ndering_y (bool, default: false): enable Y deblocking\ndering_uv (bool, default: false): enable UV deblocking \n \n \n \n \n Was this page helpful?\n \n \n \n \n \n \n ","url":"https://wiki.gpac.io/Filters/xviddec/"}]